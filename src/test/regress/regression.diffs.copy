diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_replicate_reference_table.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_replicate_reference_table.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_replicate_reference_table.out.modified	2022-11-09 13:37:37.859312602 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_replicate_reference_table.out.modified	2022-11-09 13:37:37.869312602 +0300
@@ -378,45 +378,37 @@
 -- required for create_distributed_table_concurrently
 SELECT 1 FROM citus_set_coordinator_host('localhost', :master_port);
  ?column? 
 ----------
         1
 (1 row)
 
 SET citus.shard_replication_factor TO 1;
 CREATE TABLE distributed_table_cdtc(column1 int primary key);
 SELECT create_distributed_table_concurrently('distributed_table_cdtc', 'column1');
- create_distributed_table_concurrently
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  relation replicate_reference_table.distributed_table_cdtc_1370005 already exists on worker localhost:57638
 RESET citus.shard_replication_factor;
 SELECT citus_remove_node('localhost', :master_port);
- citus_remove_node
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  cannot remove or disable the node localhost:57636 because because it contains the only shard placement for shard 1370005
+DETAIL:  One of the table(s) that prevents the operation complete successfully is replicate_reference_table.distributed_table_cdtc
+HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement_view
 WHERE
     nodeport = :worker_2_port
 ORDER BY shardid, nodeport;
  shardid | shardstate | shardlength | nodename  | nodeport 
 ---------+------------+-------------+-----------+----------
  1370004 |          1 |           0 | localhost |    57638
- 1370005 |          1 |           0 | localhost |    57638
- 1370007 |          1 |           0 | localhost |    57638
-(3 rows)
+(1 row)
 
 DROP TABLE replicate_reference_table_cdtc, distributed_table_cdtc;
 -- test adding new node + upgrading another hash distributed table to reference table + creating new reference table in TRANSACTION
 SELECT master_remove_node('localhost', :worker_2_port);
  master_remove_node 
 --------------------
  
 (1 row)
 
 CREATE TABLE replicate_reference_table_reference_one(column1 int);
@@ -708,25 +700,21 @@
 --------------------
  
 (1 row)
 
 CREATE TABLE ref_table_1(id int primary key, v int);
 CREATE TABLE ref_table_2(id int primary key, v int references ref_table_1(id));
 CREATE TABLE ref_table_3(id int primary key, v int references ref_table_2(id));
 SELECT create_reference_table('ref_table_1'),
        create_reference_table('ref_table_2'),
        create_reference_table('ref_table_3');
- create_reference_table | create_reference_table | create_reference_table
----------------------------------------------------------------------
-                        |                        |
-(1 row)
-
+ERROR:  relation "ref_table_2_1370017" is a shard relation 
 -- status before master_add_node
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement_view
 WHERE
     nodeport = :worker_2_port
 ORDER BY shardid, nodeport;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
@@ -747,21 +735,21 @@
     nodeport = :worker_2_port
 ORDER BY shardid, nodeport;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
 (0 rows)
 
 -- verify constraints have been created on the new node
 SELECT run_command_on_workers('select count(*) from pg_constraint where contype=''f'' AND conname similar to ''ref_table%\d'';');
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,2)
+ (localhost,57637,t,0)
  (localhost,57638,t,0)
 (2 rows)
 
 DROP TABLE ref_table_1, ref_table_2, ref_table_3;
 -- do some tests with inactive node
 SELECT master_remove_node('localhost', :worker_2_port);
  master_remove_node 
 --------------------
  
 (1 row)
@@ -1132,40 +1120,42 @@
 CREATE TABLE test (x int, y int references ref(a));
 select 1 FROM master_add_node('localhost', :worker_2_port);
  ?column? 
 ----------
         1
 (1 row)
 
 BEGIN;
 DROP TABLE test;
 CREATE TABLE test (x int, y int references ref(a));
-SELECT create_distributed_table('test','x');
 ERROR:  canceling the transaction since it was involved in a distributed deadlock
+DETAIL:  When adding a foreign key from a local table to a reference table, Citus applies a conversion to all the local tables in the foreign key graph
+SELECT create_distributed_table('test','x');
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 END;
 -- verify the split fails if we still need to replicate reference tables
 SELECT citus_remove_node('localhost', :worker_2_port);
  citus_remove_node 
 -------------------
  
 (1 row)
 
 SELECT create_distributed_table('test','x');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT citus_add_node('localhost', :worker_2_port);
  citus_add_node 
 ----------------
-        1370022
+        1370021
 (1 row)
 
 SELECT
   citus_split_shard_by_split_points(shardid,
                                     ARRAY[(shardminvalue::int + (shardmaxvalue::int - shardminvalue::int)/2)::text],
                                     ARRAY[nodeid, nodeid],
                                     'force_logical')
 FROM
   pg_dist_shard, pg_dist_node
 WHERE
@@ -1187,17 +1177,17 @@
         BEGIN
 		SELECT master_add_node('invalid-node-name', 9999);
         EXCEPTION WHEN OTHERS THEN
                 IF SQLERRM LIKE 'connection to the remote node%%' THEN
                         errors_received := errors_received + 1;
                 END IF;
         END;
 RAISE '(%/1) failed to add node', errors_received;
 END;
 $$;
-ERROR:  (1/1) failed to add node
+ERROR:  (0/1) failed to add node
 -- drop unnecassary tables
 DROP TABLE initially_not_replicated_reference_table;
 -- reload pg_dist_shard_placement table
 INSERT INTO pg_dist_shard_placement (SELECT * FROM tmp_shard_placement);
 DROP TABLE tmp_shard_placement;
 DROP SCHEMA replicate_reference_table CASCADE;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_remove_node_reference_table.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_remove_node_reference_table.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_remove_node_reference_table.out.modified	2022-11-09 13:37:38.629312599 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_remove_node_reference_table.out.modified	2022-11-09 13:37:38.639312599 +0300
@@ -212,23 +212,25 @@
 WHERE colocationid IN
     (SELECT colocationid
      FROM pg_dist_partition
      WHERE logicalrelid = 'remove_node_reference_table'::regclass);
  shardcount | replicationfactor | distributioncolumntype 
 ------------+-------------------+------------------------
           1 |                -1 |                      0
 (1 row)
 
 SELECT master_remove_node('localhost', :worker_1_port);
-ERROR:  cannot remove or disable the node localhost:xxxxx because because it contains the only shard placement for shard xxxxx
-DETAIL:  One of the table(s) that prevents the operation complete successfully is public.remove_node_reference_table
-HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
+ master_remove_node 
+--------------------
+ 
+(1 row)
+
 \c - - - :worker_1_port
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
      0
 (1 row)
 
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
@@ -238,46 +240,48 @@
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
 (0 rows)
 
 \c - - - :master_port
 -- remove same node twice
 SELECT master_remove_node('localhost', :worker_2_port);
 ERROR:  node at "localhost:57638" does not exist
 -- re-add the node for next tests
 SELECT 1 FROM master_add_node('localhost', :worker_2_port);
+NOTICE:  shards are still on the coordinator after adding the new node
+HINT:  Use SELECT rebalance_table_shards(); to balance shards data between workers and coordinator or SELECT citus_drain_node('localhost',57636); to permanently move shards away from the coordinator.
  ?column? 
 ----------
         1
 (1 row)
 
 -- try to disable the node before removing it (this used to crash)
 SELECT citus_disable_node('localhost', :worker_2_port);
- citus_disable_node
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  disabling the first worker node in the metadata is not allowed
+DETAIL:  Citus uses the first worker node in the metadata for certain internal operations when replicated tables are modified. Synchronous mode ensures that all nodes have the same view of the first worker node, which is used for certain locking operations.
+HINT:  You can force disabling node, SELECT citus_disable_node('localhost', 57638, synchronous:=true);
 SELECT public.wait_until_metadata_sync();
  wait_until_metadata_sync 
 --------------------------
  
 (1 row)
 
 SELECT master_remove_node('localhost', :worker_2_port);
  master_remove_node 
 --------------------
  
 (1 row)
 
 -- re-add the node for the next test
 SELECT 1 FROM master_add_node('localhost', :worker_2_port);
+NOTICE:  shards are still on the coordinator after adding the new node
+HINT:  Use SELECT rebalance_table_shards(); to balance shards data between workers and coordinator or SELECT citus_drain_node('localhost',57636); to permanently move shards away from the coordinator.
  ?column? 
 ----------
         1
 (1 row)
 
 -- remove node in a transaction and ROLLBACK
 -- status before master_remove_node
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
@@ -302,21 +306,21 @@
      WHERE logicalrelid = 'remove_node_reference_table'::regclass);
  shardcount | replicationfactor | distributioncolumntype 
 ------------+-------------------+------------------------
           1 |                -1 |                      0
 (1 row)
 
 \c - - - :worker_1_port
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
-     1
+     0
 (1 row)
 
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
@@ -356,21 +360,21 @@
      WHERE logicalrelid = 'remove_node_reference_table'::regclass);
  shardcount | replicationfactor | distributioncolumntype 
 ------------+-------------------+------------------------
           1 |                -1 |                      0
 (1 row)
 
 \c - - - :worker_1_port
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
-     1
+     0
 (1 row)
 
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
@@ -403,21 +407,21 @@
      WHERE logicalrelid = 'remove_node_reference_table'::regclass);
  shardcount | replicationfactor | distributioncolumntype 
 ------------+-------------------+------------------------
           1 |                -1 |                      0
 (1 row)
 
 \c - - - :worker_1_port
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
-     1
+     0
 (1 row)
 
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
@@ -473,20 +477,22 @@
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
 (0 rows)
 
 \c - - - :master_port
 -- re-add the node for next tests
 SELECT 1 FROM master_add_node('localhost', :worker_2_port);
+NOTICE:  shards are still on the coordinator after adding the new node
+HINT:  Use SELECT rebalance_table_shards(); to balance shards data between workers and coordinator or SELECT citus_drain_node('localhost',57636); to permanently move shards away from the coordinator.
  ?column? 
 ----------
         1
 (1 row)
 
 -- test inserting a value then removing a node in a transaction
 -- status before master_remove_node
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
@@ -511,21 +517,21 @@
      WHERE logicalrelid = 'remove_node_reference_table'::regclass);
  shardcount | replicationfactor | distributioncolumntype 
 ------------+-------------------+------------------------
           1 |                -1 |                      0
 (1 row)
 
 \c - - - :worker_1_port
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
-     1
+     0
 (1 row)
 
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
@@ -595,20 +601,22 @@
 
 SELECT * FROM remove_node_reference_table;
  column1 
 ---------
        1
 (1 row)
 
 \c - - - :master_port
 -- re-add the node for next tests
 SELECT 1 FROM master_add_node('localhost', :worker_2_port);
+NOTICE:  shards are still on the coordinator after adding the new node
+HINT:  Use SELECT rebalance_table_shards(); to balance shards data between workers and coordinator or SELECT citus_drain_node('localhost',57636); to permanently move shards away from the coordinator.
  ?column? 
 ----------
         1
 (1 row)
 
 -- test executing DDL command then removing a node in a transaction
 -- status before master_remove_node
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
@@ -633,21 +641,21 @@
      WHERE logicalrelid = 'remove_node_reference_table'::regclass);
  shardcount | replicationfactor | distributioncolumntype 
 ------------+-------------------+------------------------
           1 |                -1 |                      0
 (1 row)
 
 \c - - - :worker_1_port
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
-     1
+     0
 (1 row)
 
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
@@ -713,20 +721,22 @@
 -- verify table structure is changed
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid='public.remove_node_reference_table'::regclass;
  Column  |  Type   | Modifiers 
 ---------+---------+-----------
  column1 | integer | 
  column2 | integer | 
 (2 rows)
 
 -- re-add the node for next tests
 SELECT 1 FROM master_add_node('localhost', :worker_2_port);
+NOTICE:  shards are still on the coordinator after adding the new node
+HINT:  Use SELECT rebalance_table_shards(); to balance shards data between workers and coordinator or SELECT citus_drain_node('localhost',57636); to permanently move shards away from the coordinator.
  ?column? 
 ----------
         1
 (1 row)
 
 -- test DROP table after removing a node in a transaction
 -- status before master_remove_node
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
@@ -838,36 +848,34 @@
      WHERE logicalrelid = 'remove_node_reference_table_schema.table1'::regclass);
  shardcount | replicationfactor | distributioncolumntype 
 ------------+-------------------+------------------------
           1 |                -1 |                      0
 (1 row)
 
 \c - - - :worker_1_port
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
-     1
+     0
 (1 row)
 
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port
 ORDER BY
 	shardid;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
- 1380001 |          1 |           0 | localhost |    57638
- 1380002 |          1 |           0 | localhost |    57638
-(2 rows)
+(0 rows)
 
 \c - - - :master_port
 SELECT master_remove_node('localhost', :worker_2_port);
  master_remove_node 
 --------------------
  
 (1 row)
 
 -- status after master_remove_node
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
@@ -910,20 +918,22 @@
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
 (0 rows)
 
 \c - - - :master_port
 -- re-add the node for next tests
 SELECT 1 FROM master_add_node('localhost', :worker_2_port);
+NOTICE:  shards are still on the coordinator after adding the new node
+HINT:  Use SELECT rebalance_table_shards(); to balance shards data between workers and coordinator or SELECT citus_drain_node('localhost',57636); to permanently move shards away from the coordinator.
  ?column? 
 ----------
         1
 (1 row)
 
 -- test with citus_disable_node_and_wait
 -- status before citus_disable_node_and_wait
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
@@ -950,21 +960,21 @@
      WHERE logicalrelid = 'remove_node_reference_table'::regclass);
  shardcount | replicationfactor | distributioncolumntype 
 ------------+-------------------+------------------------
           1 |                -1 |                      0
 (1 row)
 
 \c - - - :worker_1_port
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
-     1
+     0
 (1 row)
 
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port
 ORDER BY shardid ASC;
  shardid | shardstate | shardlength | nodename | nodeport 
@@ -972,25 +982,23 @@
 (0 rows)
 
 \c - - - :master_port
 SELECT 1 FROM citus_set_coordinator_host('localhost', :master_port);
  ?column? 
 ----------
         1
 (1 row)
 
 SELECT citus_disable_node('localhost', :worker_2_port);
- citus_disable_node
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  disabling the first worker node in the metadata is not allowed
+DETAIL:  Citus uses the first worker node in the metadata for certain internal operations when replicated tables are modified. Synchronous mode ensures that all nodes have the same view of the first worker node, which is used for certain locking operations.
+HINT:  You can force disabling node, SELECT citus_disable_node('localhost', 57638, synchronous:=true);
 SELECT public.wait_until_metadata_sync();
  wait_until_metadata_sync 
 --------------------------
  
 (1 row)
 
 -- status after citus_disable_node_and_wait
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
@@ -998,25 +1006,23 @@
 (1 row)
 
 -- never mark coordinator metadatasynced = false
 SELECT hasmetadata, metadatasynced FROM pg_dist_node WHERE nodeport = :master_port;
  hasmetadata | metadatasynced 
 -------------+----------------
  t           | t
 (1 row)
 
 SELECT 1 FROM citus_remove_node('localhost', :master_port);
- ?column?
----------------------------------------------------------------------
-        1
-(1 row)
-
+ERROR:  cannot remove or disable the node localhost:57636 because because it contains the only shard placement for shard 1380001
+DETAIL:  One of the table(s) that prevents the operation complete successfully is public.remove_node_reference_table
+HINT:  To proceed, either drop the tables or use undistribute_table() function to convert them to local tables
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
 (0 rows)
 
@@ -1028,21 +1034,21 @@
      WHERE logicalrelid = 'remove_node_reference_table'::regclass);
  shardcount | replicationfactor | distributioncolumntype 
 ------------+-------------------+------------------------
           1 |                -1 |                      0
 (1 row)
 
 \c - - - :worker_1_port
 SELECT COUNT(*) FROM pg_dist_node WHERE nodeport = :worker_2_port;
  count 
 -------
-     1
+     0
 (1 row)
 
 SELECT
     shardid, shardstate, shardlength, nodename, nodeport
 FROM
     pg_dist_shard_placement
 WHERE
     nodeport = :worker_2_port;
  shardid | shardstate | shardlength | nodename | nodeport 
 ---------+------------+-------------+----------+----------
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_create_table.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_create_table.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_create_table.out.modified	2022-11-09 13:37:39.049312597 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_create_table.out.modified	2022-11-09 13:37:39.059312597 +0300
@@ -18,43 +18,36 @@
 	l_returnflag char(1) not null,
 	l_linestatus char(1) not null,
 	l_shipdate date not null,
 	l_commitdate date not null,
 	l_receiptdate date not null,
 	l_shipinstruct char(25) not null,
 	l_shipmode char(10) not null,
 	l_comment varchar(44) not null,
 	PRIMARY KEY(l_orderkey, l_linenumber) );
 SELECT create_distributed_table('lineitem', 'l_orderkey', 'hash', shard_count := 2);
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE INDEX lineitem_time_index ON lineitem (l_shipdate);
 CREATE TABLE orders (
 	o_orderkey bigint not null,
 	o_custkey integer not null,
 	o_orderstatus char(1) not null,
 	o_totalprice decimal(15,2) not null,
 	o_orderdate date not null,
 	o_orderpriority char(15) not null,
 	o_clerk char(15) not null,
 	o_shippriority integer not null,
 	o_comment varchar(79) not null,
 	PRIMARY KEY(o_orderkey) );
 SELECT create_distributed_table('orders', 'o_orderkey', 'hash', colocate_with := 'lineitem');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  relation lineitem is not distributed
 CREATE TABLE orders_reference (
 	o_orderkey bigint not null,
 	o_custkey integer not null,
 	o_orderstatus char(1) not null,
 	o_totalprice decimal(15,2) not null,
 	o_orderdate date not null,
 	o_orderpriority char(15) not null,
 	o_clerk char(15) not null,
 	o_shippriority integer not null,
 	o_comment varchar(79) not null,
@@ -89,25 +82,21 @@
 	c_acctbal decimal(15,2) not null,
 	c_mktsegment char(10) not null,
 	c_comment varchar(117) not null);
 SELECT create_distributed_table('customer_append', 'c_custkey', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('customer_append');
- master_create_empty_shard
----------------------------------------------------------------------
-                    360006
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 CREATE TABLE nation (
 	n_nationkey integer not null,
 	n_name char(25) not null,
 	n_regionkey integer not null,
 	n_comment varchar(152));
 SELECT create_reference_table('nation');
  create_reference_table 
 ------------------------
  
 (1 row)
@@ -138,25 +127,21 @@
 	p_container char(10) not null,
 	p_retailprice decimal(15,2) not null,
 	p_comment varchar(23) not null);
 SELECT create_distributed_table('part_append', 'p_partkey', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('part_append');
- master_create_empty_shard
----------------------------------------------------------------------
-                    360009
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 CREATE TABLE supplier
 (
 	s_suppkey integer not null,
 	s_name char(25) not null,
 	s_address varchar(40) not null,
 	s_nationkey integer,
 	s_phone char(15) not null,
 	s_acctbal decimal(15,2) not null,
 	s_comment varchar(101) not null
 );
@@ -172,51 +157,41 @@
 (
 	s_suppkey integer not null,
  	s_name char(25) not null,
  	s_address varchar(40) not null,
  	s_nationkey integer,
  	s_phone char(15) not null,
   	s_acctbal decimal(15,2) not null,
   	s_comment varchar(101) not null
 );
 SELECT create_distributed_table('supplier_single_shard', 's_suppkey', 'hash', shard_count := 1);
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE mx_table_test (col1 int, col2 text);
 SET citus.next_shard_id TO 360013;
 -- Test initial data loading
 CREATE TABLE data_load_test (col1 int, col2 text, col3 serial);
 INSERT INTO data_load_test VALUES (132, 'hello');
 INSERT INTO data_load_test VALUES (243, 'world');
 -- table must be empty when using append- or range-partitioning
 SELECT create_distributed_table('data_load_test', 'col1', 'append');
 ERROR:  cannot distribute relation "data_load_test"
 DETAIL:  Relation "data_load_test" contains data.
 HINT:  Empty your table before distributing it.
 SELECT create_distributed_table('data_load_test', 'col1', 'range');
 ERROR:  cannot distribute relation "data_load_test"
 DETAIL:  Relation "data_load_test" contains data.
 HINT:  Empty your table before distributing it.
 -- create_distributed_table creates shards and copies data into the distributed table
 SELECT create_distributed_table('data_load_test', 'col1');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.data_load_test$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT * FROM data_load_test ORDER BY col1;
  col1 | col2  | col3 
 ------+-------+------
   132 | hello |    1
   243 | world |    2
 (2 rows)
 
 DROP TABLE data_load_test;
 -- test queries on distributed tables with no shards
 CREATE TABLE no_shard_test (col1 int, col2 text);
@@ -243,113 +218,80 @@
  col1 | col2 
 ------+------
 (0 rows)
 
 DROP TABLE no_shard_test;
 -- ensure writes in the same transaction as create_distributed_table are visible
 BEGIN;
 CREATE TABLE data_load_test (col1 int, col2 text, col3 serial);
 INSERT INTO data_load_test VALUES (132, 'hello');
 SELECT create_distributed_table('data_load_test', 'col1');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.data_load_test$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO data_load_test VALUES (243, 'world');
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 END;
 SELECT * FROM data_load_test ORDER BY col1;
- col1 | col2  | col3
----------------------------------------------------------------------
-  132 | hello |    1
-  243 | world |    2
-(2 rows)
-
+ERROR:  relation "data_load_test" does not exist
 DROP TABLE data_load_test;
+ERROR:  table "data_load_test" does not exist
 -- creating co-located distributed tables in the same transaction works
 BEGIN;
 CREATE TABLE data_load_test1 (col1 int, col2 text, col3 serial);
 INSERT INTO data_load_test1 VALUES (132, 'hello');
 SELECT create_distributed_table('data_load_test1', 'col1');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.data_load_test1$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE data_load_test2 (col1 int, col2 text, col3 serial);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 INSERT INTO data_load_test2 VALUES (132, 'world');
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT create_distributed_table('data_load_test2', 'col1');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.data_load_test2$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT a.col2 ||' '|| b.col2
 FROM data_load_test1 a JOIN data_load_test2 b USING (col1)
 WHERE col1 = 132;
-  ?column?
----------------------------------------------------------------------
- hello world
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 DROP TABLE data_load_test1, data_load_test2;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 END;
 -- distributing catalog tables is not supported
 SELECT create_distributed_table('pg_class', 'relname');
 ERROR:  cannot create a citus table from a catalog table
 SELECT create_reference_table('pg_class');
 ERROR:  cannot create a citus table from a catalog table
 -- test shard_count parameter
 -- first set citus.shard_count so we know the parameter works
 SET citus.shard_count TO 10;
 CREATE TABLE shard_count_table (a INT, b TEXT);
 CREATE TABLE shard_count_table_2 (a INT, b TEXT);
 SELECT create_distributed_table('shard_count_table', 'a', shard_count:=5);
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT shard_count FROM citus_tables WHERE table_name::text = 'shard_count_table';
  shard_count 
 -------------
-           5
-(1 row)
+(0 rows)
 
 SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=0);
 ERROR:  0 is outside the valid range for parameter "shard_count" (1 .. 64000)
 SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=-100);
 ERROR:  -100 is outside the valid range for parameter "shard_count" (1 .. 64000)
 SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=64001);
 ERROR:  64001 is outside the valid range for parameter "shard_count" (1 .. 64000)
 -- shard count with colocate with table should error
 SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=12, colocate_with:='shard_count');
 ERROR:  Cannot use colocate_with with a table and shard_count at the same time
 -- none should not error
 SELECT create_distributed_table('shard_count_table_2', 'a', shard_count:=12, colocate_with:='none');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 DROP TABLE shard_count_table, shard_count_table_2;
 -- test shard splitting doesn't break shard_count parameter
 -- when shard count is given table needs to have exactly that
 -- many shards, regardless of shard splitting on other tables
 -- ensure there is no colocation group with 9 shards
 SELECT count(*) FROM pg_dist_colocation WHERE shardcount = 9;
  count 
 -------
      0
 (1 row)
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_create_table_superuser.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_create_table_superuser.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_create_table_superuser.out.modified	2022-11-09 13:37:39.619312595 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_create_table_superuser.out.modified	2022-11-09 13:37:39.629312595 +0300
@@ -38,420 +38,329 @@
  relname 
 ---------
 (0 rows)
 
 \c - - :master_host :master_port
 -- creating an index after loading data works
 BEGIN;
 CREATE TABLE data_load_test (col1 int, col2 text, col3 serial);
 INSERT INTO data_load_test VALUES (132, 'hello');
 SELECT create_distributed_table('data_load_test', 'col1');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.data_load_test$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE INDEX data_load_test_idx ON data_load_test (col2);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 DROP TABLE data_load_test;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 END;
 -- popping in and out of existence in the same transaction works
 BEGIN;
 CREATE TABLE data_load_test (col1 int, col2 text, col3 serial);
 INSERT INTO data_load_test VALUES (132, 'hello');
 SELECT create_distributed_table('data_load_test', 'col1');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.data_load_test$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 DROP TABLE data_load_test;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 END;
 -- but dropping after a write on the distributed table is currently disallowed
 BEGIN;
 CREATE TABLE data_load_test (col1 int, col2 text, col3 serial);
 INSERT INTO data_load_test VALUES (132, 'hello');
 SELECT create_distributed_table('data_load_test', 'col1');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.data_load_test$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO data_load_test VALUES (243, 'world');
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 DROP TABLE data_load_test;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 END;
 -- Test data loading after dropping a column
 CREATE TABLE data_load_test (col1 int, col2 text, col3 text, "CoL4"")" int);
 INSERT INTO data_load_test VALUES (132, 'hello', 'world');
 INSERT INTO data_load_test VALUES (243, 'world', 'hello');
 ALTER TABLE data_load_test DROP COLUMN col1;
 SELECT create_distributed_table('data_load_test', 'col3');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.data_load_test$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT * FROM data_load_test ORDER BY col2;
  col2  | col3  | CoL4") 
 -------+-------+--------
  hello | world |       
  world | hello |       
 (2 rows)
 
 -- make sure the tuple went to the right shard
 SELECT * FROM data_load_test WHERE col3 = 'world';
  col2  | col3  | CoL4") 
 -------+-------+--------
  hello | world |       
 (1 row)
 
 DROP TABLE data_load_test;
 SET citus.shard_replication_factor TO default;
 SET citus.shard_count to 4;
 CREATE TABLE lineitem_hash_part (like lineitem);
 SELECT create_distributed_table('lineitem_hash_part', 'l_orderkey');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE orders_hash_part (like orders);
 SELECT create_distributed_table('orders_hash_part', 'o_orderkey');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE UNLOGGED TABLE unlogged_table
 (
 	key text,
 	value text
 );
 SELECT create_distributed_table('unlogged_table', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT * FROM master_get_table_ddl_events('unlogged_table');
                           master_get_table_ddl_events                          
 -------------------------------------------------------------------------------
  CREATE UNLOGGED TABLE public.unlogged_table (key text, value text) USING heap
  ALTER TABLE public.unlogged_table OWNER TO postgres
 (2 rows)
 
 \c - - :public_worker_1_host :worker_1_port
 SELECT relpersistence FROM pg_class WHERE relname LIKE 'unlogged_table_%';
  relpersistence 
 ----------------
- u
- u
- u
- u
-(4 rows)
+(0 rows)
 
 \c - - :master_host :master_port
 -- Test rollback of create table
 BEGIN;
 CREATE TABLE rollback_table(id int, name varchar(20));
 SELECT create_distributed_table('rollback_table','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ROLLBACK;
 -- Table should not exist on the worker node
 \c - - :public_worker_1_host :worker_1_port
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid = (SELECT oid FROM pg_class WHERE relname LIKE 'rollback_table%');
  Column | Type | Modifiers 
 --------+------+-----------
 (0 rows)
 
 \c - - :master_host :master_port
 -- Insert 3 rows to make sure that copy after shard creation touches the same
 -- worker node twice.
 BEGIN;
 CREATE TABLE rollback_table(id int, name varchar(20));
 INSERT INTO rollback_table VALUES(1, 'Name_1');
 INSERT INTO rollback_table VALUES(2, 'Name_2');
 INSERT INTO rollback_table VALUES(3, 'Name_3');
 SELECT create_distributed_table('rollback_table','id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.rollback_table$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ROLLBACK;
 -- Table should not exist on the worker node
 \c - - :public_worker_1_host :worker_1_port
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid = (SELECT oid FROM pg_class WHERE relname LIKE 'rollback_table%');
  Column | Type | Modifiers 
 --------+------+-----------
 (0 rows)
 
 \c - - :master_host :master_port
 BEGIN;
 CREATE TABLE rollback_table(id int, name varchar(20));
 SELECT create_distributed_table('rollback_table','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 \copy rollback_table from stdin delimiter ','
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
+1, 'name_1'
+2, 'name_2'
+3, 'name_3'
+\.
+invalid command \.
 CREATE INDEX rollback_index ON rollback_table(id);
+ERROR:  syntax error at or near "1"
 COMMIT;
 -- Check the table is created
 SELECT count(*) FROM rollback_table;
- count
----------------------------------------------------------------------
-     3
-(1 row)
-
+ERROR:  relation "rollback_table" does not exist
 DROP TABLE rollback_table;
+ERROR:  table "rollback_table" does not exist
 BEGIN;
 CREATE TABLE rollback_table(id int, name varchar(20));
 SELECT create_distributed_table('rollback_table','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 \copy rollback_table from stdin delimiter ','
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
+1, 'name_1'
+2, 'name_2'
+3, 'name_3'
+\.
+invalid command \.
 ROLLBACK;
+ERROR:  syntax error at or near "1"
 -- Table should not exist on the worker node
 \c - - :public_worker_1_host :worker_1_port
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid = (SELECT oid FROM pg_class WHERE relname LIKE 'rollback_table%');
  Column | Type | Modifiers 
 --------+------+-----------
 (0 rows)
 
 \c - - :master_host :master_port
 BEGIN;
 CREATE TABLE tt1(id int);
 SELECT create_distributed_table('tt1','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE tt2(id int);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT create_distributed_table('tt2','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 INSERT INTO tt1 VALUES(1);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 INSERT INTO tt2 SELECT * FROM tt1 WHERE id = 1;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 -- Table should exist on the worker node
 \c - - :public_worker_1_host :worker_1_port
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid = 'public.tt1_360069'::regclass;
- Column |  Type   | Modifiers
----------------------------------------------------------------------
- id     | integer |
-(1 row)
-
+ERROR:  relation "public.tt1_360069" does not exist
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid = 'public.tt2_360073'::regclass;
- Column |  Type   | Modifiers
----------------------------------------------------------------------
- id     | integer |
-(1 row)
-
+ERROR:  relation "public.tt2_360073" does not exist
 \c - - :master_host :master_port
 DROP TABLE tt1;
+ERROR:  table "tt1" does not exist
 DROP TABLE tt2;
+ERROR:  table "tt2" does not exist
 -- It is known that creating a table with master_create_empty_shard is not
 -- transactional, so table stay remaining on the worker node after the rollback
 BEGIN;
 CREATE TABLE append_tt1(id int);
 SELECT create_distributed_table('append_tt1','id','append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('append_tt1');
- master_create_empty_shard
----------------------------------------------------------------------
-                    360077
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 ROLLBACK;
 -- Table exists on the worker node.
 \c - - :public_worker_1_host :worker_1_port
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid = 'public.append_tt1_360077'::regclass;
- Column |  Type   | Modifiers
----------------------------------------------------------------------
- id     | integer |
-(1 row)
-
+ERROR:  relation "public.append_tt1_360077" does not exist
 \c - - :master_host :master_port
 -- There should be no table on the worker node
 \c - - :public_worker_1_host :worker_1_port
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid = (SELECT oid from pg_class WHERE relname LIKE 'public.tt1%');
  Column | Type | Modifiers 
 --------+------+-----------
 (0 rows)
 
 \c - - :master_host :master_port
 -- Queries executing with router executor is allowed in the same transaction
 -- with create_distributed_table
 BEGIN;
 CREATE TABLE tt1(id int);
 INSERT INTO tt1 VALUES(1);
 SELECT create_distributed_table('tt1','id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.tt1$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO tt1 VALUES(2);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT * FROM tt1 WHERE id = 1;
- id
----------------------------------------------------------------------
-  1
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 -- Placements should be created on the worker
 \c - - :public_worker_1_host :worker_1_port
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid = 'public.tt1_360078'::regclass;
- Column |  Type   | Modifiers
----------------------------------------------------------------------
- id     | integer |
-(1 row)
-
+ERROR:  relation "public.tt1_360078" does not exist
 \c - - :master_host :master_port
 DROP TABLE tt1;
+ERROR:  table "tt1" does not exist
 BEGIN;
 CREATE TABLE tt1(id int);
 SELECT create_distributed_table('tt1','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 DROP TABLE tt1;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 -- There should be no table on the worker node
 \c - - :public_worker_1_host :worker_1_port
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid = (SELECT oid from pg_class WHERE  relname LIKE 'tt1%');
  Column | Type | Modifiers 
 --------+------+-----------
 (0 rows)
 
 \c - - :master_host :master_port
 -- Tests with create_distributed_table & DDL & DML commands
 -- Test should pass since GetPlacementListConnection can provide connections
 -- in this order of execution
 CREATE TABLE sample_table(id int);
 SELECT create_distributed_table('sample_table','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 BEGIN;
 CREATE TABLE stage_table (LIKE sample_table);
 \COPY stage_table FROM stdin; -- Note that this operation is a local copy
 SELECT create_distributed_table('stage_table', 'id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.stage_table$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO sample_table SELECT * FROM stage_table;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 DROP TABLE stage_table;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT * FROM sample_table WHERE id = 3;
- id
----------------------------------------------------------------------
-  3
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 -- Show that rows of sample_table are updated
 SELECT count(*) FROM sample_table;
  count 
 -------
-     4
+     0
 (1 row)
 
 DROP table sample_table;
 -- Test as create_distributed_table - copy - create_distributed_table - copy
 -- This combination is used by tests written by some ORMs.
 BEGIN;
 CREATE TABLE tt1(id int);
 SELECT create_distributed_table('tt1','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 \COPY tt1 from stdin;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
+1
+2
+3
+\.
+invalid command \.
 CREATE TABLE tt2(like tt1);
+ERROR:  syntax error at or near "1"
 SELECT create_distributed_table('tt2','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 \COPY tt2 from stdin;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
+4
+5
+6
+\.
+invalid command \.
 INSERT INTO tt1 SELECT * FROM tt2;
+ERROR:  syntax error at or near "4"
 SELECT * FROM tt1 WHERE id = 3;
- id
----------------------------------------------------------------------
-  3
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT * FROM tt2 WHERE id = 6;
- id
----------------------------------------------------------------------
-  6
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 END;
 SELECT count(*) FROM tt1;
- count
----------------------------------------------------------------------
-     6
-(1 row)
-
+ERROR:  relation "tt1" does not exist
 -- the goal of the following test is to make sure that
 -- both create_reference_table and create_distributed_table
 -- calls creates the schemas without leading to any deadlocks
 -- first create reference table, then hash distributed table
 BEGIN;
 CREATE SCHEMA sc;
 CREATE TABLE sc.ref(a int);
 insert into sc.ref SELECT s FROM generate_series(0, 100) s;
 SELECT create_reference_table('sc.ref');
 NOTICE:  Copying data from local table...
@@ -459,239 +368,175 @@
 DETAIL:  The local data in the table is no longer visible, but is still on disk.
 HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$sc.ref$$)
  create_reference_table 
 ------------------------
  
 (1 row)
 
 CREATE TABLE sc.hash(a int);
 insert into sc.hash SELECT s FROM generate_series(0, 100) s;
 SELECT create_distributed_table('sc.hash', 'a');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$sc.hash$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 COMMIT;
 -- first create hash distributed table, then reference table
 BEGIN;
 CREATE SCHEMA sc2;
 CREATE TABLE sc2.hash(a int);
 insert into sc2.hash SELECT s FROM generate_series(0, 100) s;
 SELECT create_distributed_table('sc2.hash', 'a');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$sc2.hash$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE sc2.ref(a int);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 insert into sc2.ref SELECT s FROM generate_series(0, 100) s;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT create_reference_table('sc2.ref');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$sc2.ref$$)
- create_reference_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 SET citus.shard_count TO 4;
 BEGIN;
 CREATE SCHEMA sc3;
 CREATE TABLE sc3.alter_replica_table
 (
 	name text,
 	id int PRIMARY KEY
 );
 ALTER TABLE sc3.alter_replica_table REPLICA IDENTITY USING INDEX alter_replica_table_pkey;
 SELECT create_distributed_table('sc3.alter_replica_table', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 COMMIT;
 SELECT run_command_on_workers($$SELECT relreplident FROM pg_class join information_schema.tables AS tables ON (pg_class.relname=tables.table_name) WHERE relname LIKE 'alter_replica_table_%' AND table_schema='sc3' LIMIT 1$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,i)
- (localhost,57638,t,i)
-(2 rows)
+ (localhost,57638,t,"")
+(1 row)
 
 BEGIN;
 CREATE SCHEMA sc4;
 CREATE TABLE sc4.alter_replica_table
 (
 	name text,
 	id int PRIMARY KEY
 );
 INSERT INTO sc4.alter_replica_table(id) SELECT generate_series(1,100);
 SET search_path = 'sc4';
 ALTER TABLE alter_replica_table REPLICA IDENTITY USING INDEX alter_replica_table_pkey;
 SELECT create_distributed_table('alter_replica_table', 'id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$sc4.alter_replica_table$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 COMMIT;
 SELECT run_command_on_workers($$SELECT relreplident FROM pg_class join information_schema.tables AS tables ON (pg_class.relname=tables.table_name) WHERE relname LIKE 'alter_replica_table_%' AND table_schema='sc4' LIMIT 1$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,i)
- (localhost,57638,t,i)
-(2 rows)
+ (localhost,57638,t,"")
+(1 row)
 
 SET search_path = 'public';
 BEGIN;
 CREATE SCHEMA sc5;
 CREATE TABLE sc5.alter_replica_table
 (
 	name text,
 	id int NOT NULL
 );
 INSERT INTO sc5.alter_replica_table(id) SELECT generate_series(1,100);
 ALTER TABLE sc5.alter_replica_table REPLICA IDENTITY FULL;
 SELECT create_distributed_table('sc5.alter_replica_table', 'id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$sc5.alter_replica_table$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 COMMIT;
 SELECT run_command_on_workers($$SELECT relreplident FROM pg_class join information_schema.tables AS tables ON (pg_class.relname=tables.table_name) WHERE relname LIKE 'alter_replica_table_%' AND table_schema='sc5' LIMIT 1$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,f)
- (localhost,57638,t,f)
-(2 rows)
+ (localhost,57638,t,"")
+(1 row)
 
 BEGIN;
 CREATE SCHEMA sc6;
 CREATE TABLE sc6.alter_replica_table
 (
 	name text,
 	id int NOT NULL
 );
 INSERT INTO sc6.alter_replica_table(id) SELECT generate_series(1,100);
 CREATE UNIQUE INDEX unique_idx ON sc6.alter_replica_table(id);
 ALTER TABLE sc6.alter_replica_table REPLICA IDENTITY USING INDEX unique_idx;
 SELECT create_distributed_table('sc6.alter_replica_table', 'id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$sc6.alter_replica_table$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 COMMIT;
 SELECT run_command_on_workers($$SELECT relreplident FROM pg_class join information_schema.tables AS tables ON (pg_class.relname=tables.table_name) WHERE relname LIKE 'alter_replica_table_%' AND table_schema='sc6' LIMIT 1$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,i)
- (localhost,57638,t,i)
-(2 rows)
+ (localhost,57638,t,"")
+(1 row)
 
 BEGIN;
 CREATE TABLE alter_replica_table
 (
 	name text,
 	id int NOT NULL
 );
 INSERT INTO alter_replica_table(id) SELECT generate_series(1,100);
 CREATE UNIQUE INDEX unique_idx ON alter_replica_table(id);
 ALTER TABLE alter_replica_table REPLICA IDENTITY USING INDEX unique_idx;
 SELECT create_distributed_table('alter_replica_table', 'id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.alter_replica_table$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 COMMIT;
 SELECT run_command_on_workers($$SELECT relreplident FROM pg_class join information_schema.tables AS tables ON (pg_class.relname=tables.table_name) WHERE relname LIKE 'alter_replica_table_%' AND table_schema='public' LIMIT 1$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,i)
- (localhost,57638,t,i)
-(2 rows)
+ (localhost,57638,t,"")
+(1 row)
 
 DROP TABLE tt1;
+ERROR:  table "tt1" does not exist
 DROP TABLE tt2;
+ERROR:  table "tt2" does not exist
 DROP TABLE alter_replica_table;
+ERROR:  table "alter_replica_table" does not exist
 DROP SCHEMA sc CASCADE;
-NOTICE:  drop cascades to 2 other objects
-DETAIL:  drop cascades to table sc.ref
-drop cascades to table sc.hash
+ERROR:  schema "sc" does not exist
 DROP SCHEMA sc2 CASCADE;
-NOTICE:  drop cascades to 2 other objects
-DETAIL:  drop cascades to table sc2.hash
-drop cascades to table sc2.ref
+ERROR:  schema "sc2" does not exist
 DROP SCHEMA sc3 CASCADE;
-NOTICE:  drop cascades to table sc3.alter_replica_table
+ERROR:  schema "sc3" does not exist
 DROP SCHEMA sc4 CASCADE;
-NOTICE:  drop cascades to table sc4.alter_replica_table
+ERROR:  schema "sc4" does not exist
 DROP SCHEMA sc5 CASCADE;
-NOTICE:  drop cascades to table sc5.alter_replica_table
+ERROR:  schema "sc5" does not exist
 DROP SCHEMA sc6 CASCADE;
-NOTICE:  drop cascades to table sc6.alter_replica_table
+ERROR:  schema "sc6" does not exist
 CREATE TABLE shard_col_table (a INT, b INT);
 CREATE TABLE shard_col_table_2 (a INT, b INT);
 SELECT create_distributed_table('shard_col_table', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- ensure there are no colocation group with 11 shards
 SELECT count(*) FROM pg_dist_colocation WHERE shardcount = 11;
  count 
 -------
      0
 (1 row)
 
 UPDATE pg_dist_colocation SET shardcount = 11 WHERE colocationid IN
 (
 	SELECT colocation_id FROM citus_tables WHERE table_name = 'shard_col_table'::regclass
 );
 SELECT create_distributed_table('shard_col_table_2', 'a', shard_count:=11);
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- ensure shard_col_table and shard_col_table_2 are not colocated
 SELECT a.colocation_id = b.colocation_id FROM citus_tables a, citus_tables b
 	WHERE a.table_name = 'shard_col_table'::regclass AND b.table_name = 'shard_col_table_2'::regclass;
  ?column? 
 ----------
- f
-(1 row)
+(0 rows)
 
 UPDATE pg_dist_colocation SET shardcount = 12 WHERE colocationid IN
 (
 	SELECT colocation_id FROM citus_tables WHERE table_name = 'shard_col_table'::regclass
 );
 DROP TABLE shard_col_table, shard_col_table_2;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_master_protocol.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_master_protocol.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_master_protocol.out.modified	2022-11-09 13:37:40.529312591 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_master_protocol.out.modified	2022-11-09 13:37:40.539312591 +0300
@@ -15,13 +15,12 @@
 SELECT * FROM master_get_new_shardid();
  master_get_new_shardid 
 ------------------------
                  740000
 (1 row)
 
 SELECT * FROM master_get_active_worker_nodes();
  node_name | node_port 
 -----------+-----------
  localhost |     57638
- localhost |     57637
-(2 rows)
+(1 row)
 
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_load_data.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_load_data.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_load_data.out.modified	2022-11-09 13:37:40.609312591 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_load_data.out.modified	2022-11-09 13:37:40.609312591 +0300
@@ -18,20 +18,22 @@
 \set client_side_copy_command '\\copy orders_reference FROM ' :'orders_2_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
 \set customer_1_data_file :abs_srcdir '/data/customer.1.data'
 \set nation_data_file :abs_srcdir '/data/nation.data'
 \set part_data_file :abs_srcdir '/data/part.data'
 \set supplier_data_file :abs_srcdir '/data/supplier.data'
 \set client_side_copy_command '\\copy customer FROM ' :'customer_1_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
 \set client_side_copy_command '\\copy customer_append FROM ' :'customer_1_data_file' ' with (delimiter '''|''', append_to_shard 360006);'
 :client_side_copy_command
+ERROR:  shard 360006 does not belong to table customer_append
 \set client_side_copy_command '\\copy nation FROM ' :'nation_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
 \set client_side_copy_command '\\copy part FROM ' :'part_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
 \set client_side_copy_command '\\copy part_append FROM ' :'part_data_file' ' with (delimiter '''|''', append_to_shard 360009);'
 :client_side_copy_command
+ERROR:  could not find valid entry for shard 360009
 \set client_side_copy_command '\\copy supplier FROM ' :'supplier_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
 \set client_side_copy_command '\\copy supplier_single_shard FROM ' :'supplier_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_insert_select_0.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_insert_select.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_insert_select_0.out.modified	2022-11-09 13:37:42.759312582 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_insert_select.out.modified	2022-11-09 13:37:42.789312582 +0300
@@ -15,76 +15,60 @@
 SET citus.next_shard_id TO 13300000;
 SET citus.next_placement_id TO 13300000;
 -- create co-located tables
 SET citus.shard_count = 4;
 SET citus.shard_replication_factor = 2;
 -- order of execution might change in parallel executions
 -- and the error details might contain the worker node
 -- so be less verbose with \set VERBOSITY TERSE when necessary
 CREATE TABLE raw_events_first (user_id int, time timestamp, value_1 int, value_2 int, value_3 float, value_4 bigint, UNIQUE(user_id, value_1));
 SELECT create_distributed_table('raw_events_first', 'user_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE raw_events_second (user_id int, time timestamp, value_1 int, value_2 int, value_3 float, value_4 bigint, UNIQUE(user_id, value_1));
 SELECT create_distributed_table('raw_events_second', 'user_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE agg_events (user_id int, value_1_agg int, value_2_agg int, value_3_agg float, value_4_agg bigint, agg_time timestamp, UNIQUE(user_id, value_1_agg));
 SELECT create_distributed_table('agg_events', 'user_id');;
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- create the reference table as well
 CREATE TABLE reference_table (user_id int);
 SELECT create_reference_table('reference_table');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 CREATE TABLE insert_select_varchar_test (key varchar, value int);
 SELECT create_distributed_table('insert_select_varchar_test', 'key', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- set back to the defaults
 SET citus.shard_count = DEFAULT;
 SET citus.shard_replication_factor = DEFAULT;
 INSERT INTO raw_events_first (user_id, time, value_1, value_2, value_3, value_4) VALUES
                          (1, now(), 10, 100, 1000.1, 10000);
 INSERT INTO raw_events_first (user_id, time, value_1, value_2, value_3, value_4) VALUES
                          (2, now(), 20, 200, 2000.1, 20000);
 INSERT INTO raw_events_first (user_id, time, value_1, value_2, value_3, value_4) VALUES
                          (3, now(), 30, 300, 3000.1, 30000);
 INSERT INTO raw_events_first (user_id, time, value_1, value_2, value_3, value_4) VALUES
                          (4, now(), 40, 400, 4000.1, 40000);
 INSERT INTO raw_events_first (user_id, time, value_1, value_2, value_3, value_4) VALUES
                          (5, now(), 50, 500, 5000.1, 50000);
 INSERT INTO raw_events_first (user_id, time, value_1, value_2, value_3, value_4) VALUES
                          (6, now(), 60, 600, 6000.1, 60000);
 SET client_min_messages TO DEBUG2;
 -- raw table to raw table
 INSERT INTO raw_events_second  SELECT * FROM raw_events_first;
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_first_13300000 raw_events_first WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_first_13300001 raw_events_first WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300006 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_first_13300002 raw_events_first WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_first_13300003 raw_events_first WHERE (user_id IS NOT NULL)
 -- see that our first multi shard INSERT...SELECT works expected
 SET client_min_messages TO INFO;
 SELECT
    raw_events_first.user_id
 FROM
    raw_events_first, raw_events_second
 WHERE
    raw_events_first.user_id = raw_events_second.user_id
 ORDER BY
   user_id DESC;
@@ -94,21 +78,21 @@
        5
        4
        3
        2
        1
 (6 rows)
 
 -- see that we get unique vialitons
 \set VERBOSITY TERSE
 INSERT INTO raw_events_second  SELECT * FROM raw_events_first;
-ERROR:  duplicate key value violates unique constraint "raw_events_second_user_id_value_1_key_xxxxxxx"
+ERROR:  duplicate key value violates unique constraint "raw_events_second_user_id_value_1_key"
 \set VERBOSITY DEFAULT
 -- stable functions should be allowed
 INSERT INTO raw_events_second (user_id, time)
 SELECT
   user_id, now()
 FROM
   raw_events_first
 WHERE
   user_id < 0;
 INSERT INTO raw_events_second (user_id)
@@ -129,194 +113,152 @@
   RETURN 0;
 END;
 $function$;
 INSERT INTO raw_events_second (user_id, value_1)
 SELECT
   user_id, evaluate_on_master()
 FROM
   raw_events_first
 WHERE
   user_id < 0;
-NOTICE:  evaluating on master
 -- make sure we don't evaluate stable functions with column arguments
 SET citus.enable_metadata_sync TO OFF;
 CREATE OR REPLACE FUNCTION evaluate_on_master(x int)
 RETURNS int LANGUAGE plpgsql STABLE
 AS $function$
 BEGIN
   RAISE NOTICE 'evaluating on master';
   RETURN x;
 END;
 $function$;
 RESET citus.enable_metadata_sync;
 INSERT INTO raw_events_second (user_id, value_1)
 SELECT
   user_id, evaluate_on_master(value_1)
 FROM
   raw_events_first
 WHERE
   user_id = 0;
-ERROR:  function public.evaluate_on_master(integer) does not exist
 -- add one more row
 INSERT INTO raw_events_first (user_id, time) VALUES
                          (7, now());
 -- try a single shard query
 SET client_min_messages TO DEBUG2;
 INSERT INTO raw_events_second (user_id, time) SELECT user_id, time FROM raw_events_first WHERE user_id = 7;
-DEBUG:  Skipping target shard interval 13300004 since SELECT query for it pruned away
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id, "time") SELECT user_id, "time" FROM public.raw_events_first_13300001 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) 7) AND (user_id IS NOT NULL))
-DEBUG:  Skipping target shard interval 13300006 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300007 since SELECT query for it pruned away
 SET client_min_messages TO INFO;
 -- add one more row
 INSERT INTO raw_events_first (user_id, time, value_1, value_2, value_3, value_4) VALUES
                          (8, now(), 80, 800, 8000, 80000);
 -- reorder columns
 SET client_min_messages TO DEBUG2;
 INSERT INTO raw_events_second (value_2, value_1, value_3, value_4, user_id, time)
 SELECT
    value_2, value_1, value_3, value_4, user_id, time
 FROM
    raw_events_first
 WHERE
    user_id = 8;
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_first_13300000 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) 8) AND (user_id IS NOT NULL))
-DEBUG:  Skipping target shard interval 13300005 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300006 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300007 since SELECT query for it pruned away
 -- a zero shard select
 INSERT INTO raw_events_second (value_2, value_1, value_3, value_4, user_id, time)
 SELECT
    value_2, value_1, value_3, value_4, user_id, time
 FROM
    raw_events_first
 WHERE
    false;
-DEBUG:  Skipping target shard interval 13300004 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300005 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300006 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300007 since SELECT query for it pruned away
 -- another zero shard select
 INSERT INTO raw_events_second (value_2, value_1, value_3, value_4, user_id, time)
 SELECT
    value_2, value_1, value_3, value_4, user_id, time
 FROM
    raw_events_first
 WHERE
    0 != 0;
-DEBUG:  Skipping target shard interval 13300004 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300005 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300006 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300007 since SELECT query for it pruned away
 -- add one more row
 SET client_min_messages TO INFO;
 INSERT INTO raw_events_first (user_id, time, value_1, value_2, value_3, value_4) VALUES
                          (9, now(), 90, 900, 9000, 90000);
 -- show that RETURNING also works
 SET client_min_messages TO DEBUG2;
 INSERT INTO raw_events_second (user_id, value_1, value_3)
 SELECT
    user_id, value_1, value_3
 FROM
    raw_events_first
 WHERE
    value_3 = 9000
 RETURNING *;
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id, value_1, value_3) SELECT user_id, value_1, value_3 FROM public.raw_events_first_13300000 raw_events_first WHERE ((value_3 OPERATOR(pg_catalog.=) (9000)::double precision) AND (user_id IS NOT NULL)) RETURNING citus_table_alias.user_id, citus_table_alias."time", citus_table_alias.value_1, citus_table_alias.value_2, citus_table_alias.value_3, citus_table_alias.value_4
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id, value_1, value_3) SELECT user_id, value_1, value_3 FROM public.raw_events_first_13300001 raw_events_first WHERE ((value_3 OPERATOR(pg_catalog.=) (9000)::double precision) AND (user_id IS NOT NULL)) RETURNING citus_table_alias.user_id, citus_table_alias."time", citus_table_alias.value_1, citus_table_alias.value_2, citus_table_alias.value_3, citus_table_alias.value_4
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300006 AS citus_table_alias (user_id, value_1, value_3) SELECT user_id, value_1, value_3 FROM public.raw_events_first_13300002 raw_events_first WHERE ((value_3 OPERATOR(pg_catalog.=) (9000)::double precision) AND (user_id IS NOT NULL)) RETURNING citus_table_alias.user_id, citus_table_alias."time", citus_table_alias.value_1, citus_table_alias.value_2, citus_table_alias.value_3, citus_table_alias.value_4
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id, value_1, value_3) SELECT user_id, value_1, value_3 FROM public.raw_events_first_13300003 raw_events_first WHERE ((value_3 OPERATOR(pg_catalog.=) (9000)::double precision) AND (user_id IS NOT NULL)) RETURNING citus_table_alias.user_id, citus_table_alias."time", citus_table_alias.value_1, citus_table_alias.value_2, citus_table_alias.value_3, citus_table_alias.value_4
  user_id | time | value_1 | value_2 | value_3 | value_4 
 ---------+------+---------+---------+---------+---------
        9 |      |      90 |         |    9000 |        
 (1 row)
 
 -- hits two shards
 \set VERBOSITY TERSE
 INSERT INTO raw_events_second (user_id, value_1, value_3)
 SELECT
    user_id, value_1, value_3
 FROM
    raw_events_first
 WHERE
    user_id = 9 OR user_id = 16
 RETURNING *;
-DEBUG:  Skipping target shard interval 13300004 since SELECT query for it pruned away
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id, value_1, value_3) SELECT user_id, value_1, value_3 FROM public.raw_events_first_13300001 raw_events_first WHERE (((user_id OPERATOR(pg_catalog.=) 9) OR (user_id OPERATOR(pg_catalog.=) 16)) AND (user_id IS NOT NULL)) RETURNING citus_table_alias.user_id, citus_table_alias."time", citus_table_alias.value_1, citus_table_alias.value_2, citus_table_alias.value_3, citus_table_alias.value_4
-DEBUG:  Skipping target shard interval 13300006 since SELECT query for it pruned away
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id, value_1, value_3) SELECT user_id, value_1, value_3 FROM public.raw_events_first_13300003 raw_events_first WHERE (((user_id OPERATOR(pg_catalog.=) 9) OR (user_id OPERATOR(pg_catalog.=) 16)) AND (user_id IS NOT NULL)) RETURNING citus_table_alias.user_id, citus_table_alias."time", citus_table_alias.value_1, citus_table_alias.value_2, citus_table_alias.value_3, citus_table_alias.value_4
-ERROR:  duplicate key value violates unique constraint "raw_events_second_user_id_value_1_key_xxxxxxx"
+ERROR:  duplicate key value violates unique constraint "raw_events_second_user_id_value_1_key"
 -- now do some aggregations
 INSERT INTO agg_events
 SELECT
    user_id, sum(value_1), avg(value_2), sum(value_3), count(value_4)
 FROM
    raw_events_first
 GROUP BY
    user_id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg, value_2_agg, value_3_agg, value_4_agg) SELECT user_id, sum(value_1) AS sum, avg(value_2) AS avg, sum(value_3) AS sum, count(value_4) AS count FROM public.raw_events_first_13300000 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg, value_2_agg, value_3_agg, value_4_agg) SELECT user_id, sum(value_1) AS sum, avg(value_2) AS avg, sum(value_3) AS sum, count(value_4) AS count FROM public.raw_events_first_13300001 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg, value_2_agg, value_3_agg, value_4_agg) SELECT user_id, sum(value_1) AS sum, avg(value_2) AS avg, sum(value_3) AS sum, count(value_4) AS count FROM public.raw_events_first_13300002 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg, value_2_agg, value_3_agg, value_4_agg) SELECT user_id, sum(value_1) AS sum, avg(value_2) AS avg, sum(value_3) AS sum, count(value_4) AS count FROM public.raw_events_first_13300003 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
 -- group by column not exists on the SELECT target list
 INSERT INTO agg_events (value_3_agg, value_4_agg, value_1_agg, user_id)
 SELECT
    sum(value_3), count(value_4), sum(value_1), user_id
 FROM
    raw_events_first
 GROUP BY
    value_2, user_id
 RETURNING *;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg, value_3_agg, value_4_agg) SELECT user_id, sum(value_1) AS sum, sum(value_3) AS sum, count(value_4) AS count FROM public.raw_events_first_13300000 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY value_2, user_id RETURNING citus_table_alias.user_id, citus_table_alias.value_1_agg, citus_table_alias.value_2_agg, citus_table_alias.value_3_agg, citus_table_alias.value_4_agg, citus_table_alias.agg_time
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg, value_3_agg, value_4_agg) SELECT user_id, sum(value_1) AS sum, sum(value_3) AS sum, count(value_4) AS count FROM public.raw_events_first_13300001 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY value_2, user_id RETURNING citus_table_alias.user_id, citus_table_alias.value_1_agg, citus_table_alias.value_2_agg, citus_table_alias.value_3_agg, citus_table_alias.value_4_agg, citus_table_alias.agg_time
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg, value_3_agg, value_4_agg) SELECT user_id, sum(value_1) AS sum, sum(value_3) AS sum, count(value_4) AS count FROM public.raw_events_first_13300002 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY value_2, user_id RETURNING citus_table_alias.user_id, citus_table_alias.value_1_agg, citus_table_alias.value_2_agg, citus_table_alias.value_3_agg, citus_table_alias.value_4_agg, citus_table_alias.agg_time
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg, value_3_agg, value_4_agg) SELECT user_id, sum(value_1) AS sum, sum(value_3) AS sum, count(value_4) AS count FROM public.raw_events_first_13300003 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY value_2, user_id RETURNING citus_table_alias.user_id, citus_table_alias.value_1_agg, citus_table_alias.value_2_agg, citus_table_alias.value_3_agg, citus_table_alias.value_4_agg, citus_table_alias.agg_time
-ERROR:  duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key_xxxxxxx"
+ERROR:  duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key"
 -- some subquery tests
 INSERT INTO agg_events
             (value_1_agg,
              user_id)
 SELECT SUM(value_1),
        id
 FROM   (SELECT raw_events_second.user_id AS id,
                raw_events_second.value_1
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id) AS foo
 GROUP  BY id
 ORDER  BY id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg) SELECT id, sum(value_1) AS sum FROM (SELECT raw_events_second.user_id AS id, raw_events_second.value_1 FROM public.raw_events_first_13300000 raw_events_first, public.raw_events_second_13300004 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id)) foo WHERE (id IS NOT NULL) GROUP BY id ORDER BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg) SELECT id, sum(value_1) AS sum FROM (SELECT raw_events_second.user_id AS id, raw_events_second.value_1 FROM public.raw_events_first_13300001 raw_events_first, public.raw_events_second_13300005 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id)) foo WHERE (id IS NOT NULL) GROUP BY id ORDER BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg) SELECT id, sum(value_1) AS sum FROM (SELECT raw_events_second.user_id AS id, raw_events_second.value_1 FROM public.raw_events_first_13300002 raw_events_first, public.raw_events_second_13300006 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id)) foo WHERE (id IS NOT NULL) GROUP BY id ORDER BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg) SELECT id, sum(value_1) AS sum FROM (SELECT raw_events_second.user_id AS id, raw_events_second.value_1 FROM public.raw_events_first_13300003 raw_events_first, public.raw_events_second_13300007 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id)) foo WHERE (id IS NOT NULL) GROUP BY id ORDER BY id
-ERROR:  duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key_xxxxxxx"
+ERROR:  duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key"
 -- subquery one more level depth
 INSERT INTO agg_events
             (value_4_agg,
              value_1_agg,
              user_id)
 SELECT v4,
        v1,
        id
 FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
                SUM(raw_events_first.value_1) AS v1,
                raw_events_second.user_id      AS id
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.user_id) AS foo
 ORDER  BY id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg, value_4_agg) SELECT id, v1, v4 FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300000 raw_events_first, public.raw_events_second_13300004 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id) foo WHERE (id IS NOT NULL) ORDER BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg, value_4_agg) SELECT id, v1, v4 FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300001 raw_events_first, public.raw_events_second_13300005 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id) foo WHERE (id IS NOT NULL) ORDER BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg, value_4_agg) SELECT id, v1, v4 FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300002 raw_events_first, public.raw_events_second_13300006 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id) foo WHERE (id IS NOT NULL) ORDER BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg, value_4_agg) SELECT id, v1, v4 FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300003 raw_events_first, public.raw_events_second_13300007 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id) foo WHERE (id IS NOT NULL) ORDER BY id
-ERROR:  duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key_xxxxxxx"
+ERROR:  duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key"
 \set VERBOSITY DEFAULT
 -- join between subqueries
 INSERT INTO agg_events
             (user_id)
 SELECT f2.id FROM
 (SELECT
       id
 FROM   (SELECT reference_table.user_id      AS id
         FROM   raw_events_first,
                reference_table
@@ -327,24 +269,21 @@
        id
 FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
                SUM(raw_events_first.value_1) AS v1,
                raw_events_second.user_id      AS id
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.user_id
         HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id = f2.id);
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id) SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300000 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300000 raw_events_first, public.raw_events_second_13300004 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE (f2.id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id) SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300001 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300001 raw_events_first, public.raw_events_second_13300005 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE (f2.id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id) SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300002 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300002 raw_events_first, public.raw_events_second_13300006 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE (f2.id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id) SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300003 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300003 raw_events_first, public.raw_events_second_13300007 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE (f2.id IS NOT NULL)
+DEBUG:  Creating router plan
 -- add one more level subqueris on top of subquery JOINs
 INSERT INTO agg_events
             (user_id, value_4_agg)
 SELECT
   outer_most.id, max(outer_most.value)
 FROM
 (
   SELECT f2.id as id, f2.v4 as value FROM
     (SELECT
           id
@@ -360,69 +299,50 @@
                SUM(raw_events_first.value_1) AS v1,
                raw_events_second.user_id      AS id
             FROM   raw_events_first,
                     raw_events_second
             WHERE  raw_events_first.user_id = raw_events_second.user_id
             GROUP  BY raw_events_second.user_id
             HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id = f2.id)) as outer_most
 GROUP BY
   outer_most.id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_4_agg) SELECT id, max(value) AS max FROM (SELECT f2.id, f2.v4 AS value FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300000 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300000 raw_events_first, public.raw_events_second_13300004 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id)))) outer_most WHERE (id IS NOT NULL) GROUP BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_4_agg) SELECT id, max(value) AS max FROM (SELECT f2.id, f2.v4 AS value FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300001 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300001 raw_events_first, public.raw_events_second_13300005 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id)))) outer_most WHERE (id IS NOT NULL) GROUP BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_4_agg) SELECT id, max(value) AS max FROM (SELECT f2.id, f2.v4 AS value FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300002 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300002 raw_events_first, public.raw_events_second_13300006 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id)))) outer_most WHERE (id IS NOT NULL) GROUP BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_4_agg) SELECT id, max(value) AS max FROM (SELECT f2.id, f2.v4 AS value FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300003 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300003 raw_events_first, public.raw_events_second_13300007 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id)))) outer_most WHERE (id IS NOT NULL) GROUP BY id
+DEBUG:  Creating router plan
 -- subqueries in WHERE clause
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id IN (SELECT user_id
                    FROM   raw_events_second
                    WHERE  user_id = 2);
-DEBUG:  Skipping target shard interval 13300004 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300005 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300006 since SELECT query for it pruned away
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300003 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300007 raw_events_second WHERE (raw_events_second.user_id OPERATOR(pg_catalog.=) 2))) AND (user_id IS NOT NULL))
 INSERT INTO raw_events_second
              (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id IN (SELECT user_id
                    FROM   raw_events_second
                    WHERE  user_id != 2 AND value_1 = 2000)
 ON conflict (user_id, value_1) DO NOTHING;
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300000 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300004 raw_events_second WHERE ((raw_events_second.user_id OPERATOR(pg_catalog.<>) 2) AND (raw_events_second.value_1 OPERATOR(pg_catalog.=) 2000)))) AND (user_id IS NOT NULL)) ON CONFLICT(user_id, value_1) DO NOTHING
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300001 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300005 raw_events_second WHERE ((raw_events_second.user_id OPERATOR(pg_catalog.<>) 2) AND (raw_events_second.value_1 OPERATOR(pg_catalog.=) 2000)))) AND (user_id IS NOT NULL)) ON CONFLICT(user_id, value_1) DO NOTHING
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300006 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300002 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300006 raw_events_second WHERE ((raw_events_second.user_id OPERATOR(pg_catalog.<>) 2) AND (raw_events_second.value_1 OPERATOR(pg_catalog.=) 2000)))) AND (user_id IS NOT NULL)) ON CONFLICT(user_id, value_1) DO NOTHING
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300003 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300007 raw_events_second WHERE ((raw_events_second.user_id OPERATOR(pg_catalog.<>) 2) AND (raw_events_second.value_1 OPERATOR(pg_catalog.=) 2000)))) AND (user_id IS NOT NULL)) ON CONFLICT(user_id, value_1) DO NOTHING
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id IN (SELECT user_id
                    FROM  raw_events_second WHERE false);
-DEBUG:  Skipping target shard interval 13300004 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300005 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300006 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300007 since SELECT query for it pruned away
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id IN (SELECT user_id
                   FROM   raw_events_second
                    WHERE value_1 = 1000 OR value_1 = 2000 OR value_1 = 3000);
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300000 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300004 raw_events_second WHERE ((raw_events_second.value_1 OPERATOR(pg_catalog.=) 1000) OR (raw_events_second.value_1 OPERATOR(pg_catalog.=) 2000) OR (raw_events_second.value_1 OPERATOR(pg_catalog.=) 3000)))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300001 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300005 raw_events_second WHERE ((raw_events_second.value_1 OPERATOR(pg_catalog.=) 1000) OR (raw_events_second.value_1 OPERATOR(pg_catalog.=) 2000) OR (raw_events_second.value_1 OPERATOR(pg_catalog.=) 3000)))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300006 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300002 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300006 raw_events_second WHERE ((raw_events_second.value_1 OPERATOR(pg_catalog.=) 1000) OR (raw_events_second.value_1 OPERATOR(pg_catalog.=) 2000) OR (raw_events_second.value_1 OPERATOR(pg_catalog.=) 3000)))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300003 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300007 raw_events_second WHERE ((raw_events_second.value_1 OPERATOR(pg_catalog.=) 1000) OR (raw_events_second.value_1 OPERATOR(pg_catalog.=) 2000) OR (raw_events_second.value_1 OPERATOR(pg_catalog.=) 3000)))) AND (user_id IS NOT NULL))
 -- lets mix subqueries in FROM clause and subqueries in WHERE
 INSERT INTO agg_events
             (user_id)
 SELECT f2.id FROM
 (SELECT
       id
 FROM   (SELECT reference_table.user_id      AS id
         FROM   raw_events_first,
                reference_table
         WHERE  raw_events_first.user_id = reference_table.user_id ) AS foo) as f
@@ -434,102 +354,80 @@
                SUM(raw_events_first.value_1) AS v1,
                raw_events_second.user_id      AS id
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.user_id
         HAVING SUM(raw_events_second.value_4) > 1000) AS foo2 ) as f2
 ON (f.id = f2.id)
 WHERE f.id IN (SELECT user_id
                FROM   raw_events_second);
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id) SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300000 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300000 raw_events_first, public.raw_events_second_13300004 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (1000)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE ((f.id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300004 raw_events_second)) AND (f2.id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id) SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300001 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300001 raw_events_first, public.raw_events_second_13300005 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (1000)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE ((f.id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300005 raw_events_second)) AND (f2.id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id) SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300002 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300002 raw_events_first, public.raw_events_second_13300006 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (1000)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE ((f.id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300006 raw_events_second)) AND (f2.id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id) SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300003 raw_events_first, public.reference_table_13300012 reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300003 raw_events_first, public.raw_events_second_13300007 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (1000)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE ((f.id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300007 raw_events_second)) AND (f2.id IS NOT NULL))
+DEBUG:  Creating router plan
 -- some UPSERTS
 INSERT INTO agg_events AS ae
             (
                         user_id,
                         value_1_agg,
                         agg_time
             )
 SELECT user_id,
        value_1,
        time
 FROM   raw_events_first
 ON conflict (user_id, value_1_agg)
 DO UPDATE
    SET    agg_time = EXCLUDED.agg_time
    WHERE  ae.agg_time < EXCLUDED.agg_time;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS ae (user_id, value_1_agg, agg_time) SELECT user_id, value_1, "time" FROM public.raw_events_first_13300000 raw_events_first WHERE (user_id IS NOT NULL) ON CONFLICT(user_id, value_1_agg) DO UPDATE SET agg_time = excluded.agg_time WHERE (ae.agg_time OPERATOR(pg_catalog.<) excluded.agg_time)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS ae (user_id, value_1_agg, agg_time) SELECT user_id, value_1, "time" FROM public.raw_events_first_13300001 raw_events_first WHERE (user_id IS NOT NULL) ON CONFLICT(user_id, value_1_agg) DO UPDATE SET agg_time = excluded.agg_time WHERE (ae.agg_time OPERATOR(pg_catalog.<) excluded.agg_time)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS ae (user_id, value_1_agg, agg_time) SELECT user_id, value_1, "time" FROM public.raw_events_first_13300002 raw_events_first WHERE (user_id IS NOT NULL) ON CONFLICT(user_id, value_1_agg) DO UPDATE SET agg_time = excluded.agg_time WHERE (ae.agg_time OPERATOR(pg_catalog.<) excluded.agg_time)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS ae (user_id, value_1_agg, agg_time) SELECT user_id, value_1, "time" FROM public.raw_events_first_13300003 raw_events_first WHERE (user_id IS NOT NULL) ON CONFLICT(user_id, value_1_agg) DO UPDATE SET agg_time = excluded.agg_time WHERE (ae.agg_time OPERATOR(pg_catalog.<) excluded.agg_time)
 -- upserts with returning
 INSERT INTO agg_events AS ae
             (
                         user_id,
                         value_1_agg,
                         agg_time
             )
 SELECT user_id,
        value_1,
        time
 FROM   raw_events_first
 ON conflict (user_id, value_1_agg)
 DO UPDATE
    SET    agg_time = EXCLUDED.agg_time
    WHERE  ae.agg_time < EXCLUDED.agg_time
 RETURNING user_id, value_1_agg;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS ae (user_id, value_1_agg, agg_time) SELECT user_id, value_1, "time" FROM public.raw_events_first_13300000 raw_events_first WHERE (user_id IS NOT NULL) ON CONFLICT(user_id, value_1_agg) DO UPDATE SET agg_time = excluded.agg_time WHERE (ae.agg_time OPERATOR(pg_catalog.<) excluded.agg_time) RETURNING ae.user_id, ae.value_1_agg
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS ae (user_id, value_1_agg, agg_time) SELECT user_id, value_1, "time" FROM public.raw_events_first_13300001 raw_events_first WHERE (user_id IS NOT NULL) ON CONFLICT(user_id, value_1_agg) DO UPDATE SET agg_time = excluded.agg_time WHERE (ae.agg_time OPERATOR(pg_catalog.<) excluded.agg_time) RETURNING ae.user_id, ae.value_1_agg
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS ae (user_id, value_1_agg, agg_time) SELECT user_id, value_1, "time" FROM public.raw_events_first_13300002 raw_events_first WHERE (user_id IS NOT NULL) ON CONFLICT(user_id, value_1_agg) DO UPDATE SET agg_time = excluded.agg_time WHERE (ae.agg_time OPERATOR(pg_catalog.<) excluded.agg_time) RETURNING ae.user_id, ae.value_1_agg
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS ae (user_id, value_1_agg, agg_time) SELECT user_id, value_1, "time" FROM public.raw_events_first_13300003 raw_events_first WHERE (user_id IS NOT NULL) ON CONFLICT(user_id, value_1_agg) DO UPDATE SET agg_time = excluded.agg_time WHERE (ae.agg_time OPERATOR(pg_catalog.<) excluded.agg_time) RETURNING ae.user_id, ae.value_1_agg
  user_id | value_1_agg 
 ---------+-------------
        7 |            
 (1 row)
 
 INSERT INTO agg_events (user_id, value_1_agg)
 SELECT
    user_id, sum(value_1 + value_2)
 FROM
    raw_events_first GROUP BY user_id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, sum((value_1 OPERATOR(pg_catalog.+) value_2)) AS sum FROM public.raw_events_first_13300000 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, sum((value_1 OPERATOR(pg_catalog.+) value_2)) AS sum FROM public.raw_events_first_13300001 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, sum((value_1 OPERATOR(pg_catalog.+) value_2)) AS sum FROM public.raw_events_first_13300002 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, sum((value_1 OPERATOR(pg_catalog.+) value_2)) AS sum FROM public.raw_events_first_13300003 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
 --  FILTER CLAUSE
 INSERT INTO agg_events (user_id, value_1_agg)
 SELECT
    user_id, sum(value_1 + value_2) FILTER (where value_3 = 15)
 FROM
    raw_events_first GROUP BY user_id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, sum((value_1 OPERATOR(pg_catalog.+) value_2)) FILTER (WHERE (value_3 OPERATOR(pg_catalog.=) (15)::double precision)) AS sum FROM public.raw_events_first_13300000 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, sum((value_1 OPERATOR(pg_catalog.+) value_2)) FILTER (WHERE (value_3 OPERATOR(pg_catalog.=) (15)::double precision)) AS sum FROM public.raw_events_first_13300001 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, sum((value_1 OPERATOR(pg_catalog.+) value_2)) FILTER (WHERE (value_3 OPERATOR(pg_catalog.=) (15)::double precision)) AS sum FROM public.raw_events_first_13300002 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, sum((value_1 OPERATOR(pg_catalog.+) value_2)) FILTER (WHERE (value_3 OPERATOR(pg_catalog.=) (15)::double precision)) AS sum FROM public.raw_events_first_13300003 raw_events_first WHERE (user_id IS NOT NULL) GROUP BY user_id
 -- a test with reference table JOINs
 INSERT INTO
   agg_events (user_id, value_1_agg)
 SELECT
   raw_events_first.user_id, sum(value_1)
 FROM
   reference_table, raw_events_first
 WHERE
   raw_events_first.user_id = reference_table.user_id
 GROUP BY
   raw_events_first.user_id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg) SELECT raw_events_first.user_id, sum(raw_events_first.value_1) AS sum FROM public.reference_table_13300012 reference_table, public.raw_events_first_13300000 raw_events_first WHERE ((raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id) AND (raw_events_first.user_id IS NOT NULL)) GROUP BY raw_events_first.user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg) SELECT raw_events_first.user_id, sum(raw_events_first.value_1) AS sum FROM public.reference_table_13300012 reference_table, public.raw_events_first_13300001 raw_events_first WHERE ((raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id) AND (raw_events_first.user_id IS NOT NULL)) GROUP BY raw_events_first.user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg) SELECT raw_events_first.user_id, sum(raw_events_first.value_1) AS sum FROM public.reference_table_13300012 reference_table, public.raw_events_first_13300002 raw_events_first WHERE ((raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id) AND (raw_events_first.user_id IS NOT NULL)) GROUP BY raw_events_first.user_id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg) SELECT raw_events_first.user_id, sum(raw_events_first.value_1) AS sum FROM public.reference_table_13300012 reference_table, public.raw_events_first_13300003 raw_events_first WHERE ((raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id) AND (raw_events_first.user_id IS NOT NULL)) GROUP BY raw_events_first.user_id
+DEBUG:  Creating router plan
 -- a note on the outer joins is that
 -- we filter out outer join results
 -- where partition column returns
 -- NULL. Thus, we could INSERT less rows
 -- than we expect from subquery result.
 -- see the following tests
 SET client_min_messages TO INFO;
 -- we don't want to see constraint violations, so truncate first
 TRUNCATE agg_events;
 -- add a row to first table to make table contents different
@@ -560,76 +458,65 @@
 
 SET client_min_messages TO DEBUG2;
 -- we insert 10 rows since we filtered out
 -- NULL partition column values
 INSERT INTO agg_events (user_id, value_1_agg)
 SELECT t1.user_id AS col1,
        t2.user_id AS col2
 FROM   raw_events_first t1
        FULL JOIN raw_events_second t2
               ON t1.user_id = t2.user_id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg) SELECT t1.user_id AS col1, t2.user_id AS col2 FROM (public.raw_events_first_13300000 t1 FULL JOIN public.raw_events_second_13300004 t2 ON ((t1.user_id OPERATOR(pg_catalog.=) t2.user_id))) WHERE (t1.user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg) SELECT t1.user_id AS col1, t2.user_id AS col2 FROM (public.raw_events_first_13300001 t1 FULL JOIN public.raw_events_second_13300005 t2 ON ((t1.user_id OPERATOR(pg_catalog.=) t2.user_id))) WHERE (t1.user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg) SELECT t1.user_id AS col1, t2.user_id AS col2 FROM (public.raw_events_first_13300002 t1 FULL JOIN public.raw_events_second_13300006 t2 ON ((t1.user_id OPERATOR(pg_catalog.=) t2.user_id))) WHERE (t1.user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg) SELECT t1.user_id AS col1, t2.user_id AS col2 FROM (public.raw_events_first_13300003 t1 FULL JOIN public.raw_events_second_13300007 t2 ON ((t1.user_id OPERATOR(pg_catalog.=) t2.user_id))) WHERE (t1.user_id IS NOT NULL)
 SET client_min_messages TO INFO;
 -- see that the results are different from the SELECT query
 SELECT
   user_id, value_1_agg
 FROM
   agg_events
 ORDER BY
   user_id, value_1_agg;
  user_id | value_1_agg 
 ---------+-------------
        1 |           1
        2 |            
        3 |           3
        4 |           4
        5 |           5
        6 |           6
        7 |           7
        8 |           8
        9 |           9
-(9 rows)
+         |          10
+(10 rows)
 
 -- we don't want to see constraint violations, so truncate first
 SET client_min_messages TO INFO;
 TRUNCATE agg_events;
 SET client_min_messages TO DEBUG2;
 -- DISTINCT clause
 INSERT INTO agg_events (value_1_agg, user_id)
   SELECT
     DISTINCT value_1, user_id
   FROM
     raw_events_first;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg) SELECT DISTINCT user_id, value_1 FROM public.raw_events_first_13300000 raw_events_first WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg) SELECT DISTINCT user_id, value_1 FROM public.raw_events_first_13300001 raw_events_first WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg) SELECT DISTINCT user_id, value_1 FROM public.raw_events_first_13300002 raw_events_first WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg) SELECT DISTINCT user_id, value_1 FROM public.raw_events_first_13300003 raw_events_first WHERE (user_id IS NOT NULL)
 -- we don't want to see constraint violations, so truncate first
 SET client_min_messages TO INFO;
 truncate agg_events;
 SET client_min_messages TO DEBUG2;
 -- DISTINCT ON clauses are supported
 -- distinct on(non-partition column)
 -- values are pulled to master
 INSERT INTO agg_events (value_1_agg, user_id)
   SELECT
     DISTINCT ON (value_1) value_1, user_id
   FROM
     raw_events_first;
-DEBUG:  DISTINCT ON (non-partition column) clauses are not allowed in distributed INSERT ... SELECT queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 SELECT user_id, value_1_agg FROM agg_events ORDER BY 1,2;
-DEBUG:  Router planner cannot handle multi-shard select queries
  user_id | value_1_agg 
 ---------+-------------
        1 |          10
        2 |          20
        3 |          30
        4 |          40
        5 |          50
        6 |          60
        7 |            
        8 |          80
@@ -640,26 +527,21 @@
 SET client_min_messages TO INFO;
 truncate agg_events;
 SET client_min_messages TO DEBUG2;
 -- distinct on(partition column)
 -- queries are forwared to workers
 INSERT INTO agg_events (value_1_agg, user_id)
   SELECT
     DISTINCT ON (user_id) value_1, user_id
   FROM
     raw_events_first;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg) SELECT DISTINCT ON (user_id) user_id, value_1 FROM public.raw_events_first_13300000 raw_events_first WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg) SELECT DISTINCT ON (user_id) user_id, value_1 FROM public.raw_events_first_13300001 raw_events_first WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg) SELECT DISTINCT ON (user_id) user_id, value_1 FROM public.raw_events_first_13300002 raw_events_first WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg) SELECT DISTINCT ON (user_id) user_id, value_1 FROM public.raw_events_first_13300003 raw_events_first WHERE (user_id IS NOT NULL)
 SELECT user_id, value_1_agg FROM agg_events ORDER BY 1,2;
-DEBUG:  Router planner cannot handle multi-shard select queries
  user_id | value_1_agg 
 ---------+-------------
        1 |          10
        2 |          20
        3 |          30
        4 |          40
        5 |          50
        6 |          60
        7 |            
        8 |          80
@@ -669,326 +551,288 @@
 -- We support CTEs
 BEGIN;
 WITH fist_table_agg AS MATERIALIZED
   (SELECT max(value_1)+1 as v1_agg, user_id FROM raw_events_first GROUP BY user_id)
 INSERT INTO agg_events
             (value_1_agg, user_id)
             SELECT
               v1_agg, user_id
             FROM
               fist_table_agg;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  generating subplan XXX_1 for CTE fist_table_agg: SELECT (max(value_1) OPERATOR(pg_catalog.+) 1) AS v1_agg, user_id FROM public.raw_events_first GROUP BY user_id
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT user_id, v1_agg AS value_1_agg FROM (SELECT fist_table_agg.user_id, fist_table_agg.v1_agg FROM (SELECT intermediate_result.v1_agg, intermediate_result.user_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(v1_agg integer, user_id integer)) fist_table_agg) citus_insert_select_subquery
-DEBUG:  Creating router plan
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 ROLLBACK;
 -- We don't support CTEs that are referenced in the target list
 INSERT INTO agg_events
   WITH sub_cte AS (SELECT 1)
   SELECT
     raw_events_first.user_id, (SELECT * FROM sub_cte)
   FROM
     raw_events_first;
-DEBUG:  CTE sub_cte is going to be inlined via distributed planning
-DEBUG:  Subqueries without relations are not allowed in distributed INSERT ... SELECT queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-DEBUG:  partitioning SELECT query by column index 0 with name 'user_id'
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, value_1_agg FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300000_to_0}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1_agg integer)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, value_1_agg FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300001_to_1}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1_agg integer)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, value_1_agg FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300002_to_2}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1_agg integer)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_1_agg) SELECT user_id, value_1_agg FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300003_to_3}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1_agg integer)
 -- We support set operations via the coordinator
 BEGIN;
 INSERT INTO
   raw_events_first(user_id)
 SELECT
   user_id
 FROM
   ((SELECT user_id FROM raw_events_first) UNION
    (SELECT user_id FROM raw_events_second)) as foo;
-DEBUG:  Set operations are not allowed in distributed INSERT ... SELECT queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-DEBUG:  partitioning SELECT query by column index 0 with name 'user_id'
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300000 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300004_to_0}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300001 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300005_to_1}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300002 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300006_to_2}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300003 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300007_to_3}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
 ROLLBACK;
 -- We do support set operations through recursive planning
 BEGIN;
 SET LOCAL client_min_messages TO DEBUG;
 INSERT INTO
   raw_events_first(user_id)
   (SELECT user_id FROM raw_events_first) INTERSECT
   (SELECT user_id FROM raw_events_first);
-DEBUG:  Set operations are not allowed in distributed INSERT ... SELECT queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  generating subplan XXX_1 for subquery SELECT user_id FROM public.raw_events_first
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  generating subplan XXX_2 for subquery SELECT user_id FROM public.raw_events_first
-DEBUG:  Creating router plan
-DEBUG:  generating subplan XXX_3 for subquery SELECT intermediate_result.user_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer) INTERSECT SELECT intermediate_result.user_id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT user_id FROM (SELECT intermediate_result.user_id FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer)) citus_insert_select_subquery
-DEBUG:  Creating router plan
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 ROLLBACK;
 -- If the query is router plannable then it is executed via the coordinator
 INSERT INTO
   raw_events_first(user_id)
 SELECT
   user_id
 FROM
   ((SELECT user_id FROM raw_events_first WHERE user_id = 15) EXCEPT
    (SELECT user_id FROM raw_events_second where user_id = 17)) as foo;
-DEBUG:  Set operations are not allowed in distributed INSERT ... SELECT queries
-DEBUG:  Creating router plan
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 -- some supported LEFT joins
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first  LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.user_id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300000 raw_events_first LEFT JOIN public.raw_events_second_13300004 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (raw_events_first.user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300001 raw_events_first LEFT JOIN public.raw_events_second_13300005 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (raw_events_first.user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300002 raw_events_first LEFT JOIN public.raw_events_second_13300006 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (raw_events_first.user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300003 raw_events_first LEFT JOIN public.raw_events_second_13300007 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (raw_events_first.user_id IS NOT NULL)
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_second.user_id
  FROM
    reference_table LEFT JOIN raw_events_second ON reference_table.user_id = raw_events_second.user_id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id) SELECT raw_events_second.user_id FROM (public.reference_table_13300012 reference_table LEFT JOIN public.raw_events_second_13300004 raw_events_second ON ((reference_table.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (raw_events_second.user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id) SELECT raw_events_second.user_id FROM (public.reference_table_13300012 reference_table LEFT JOIN public.raw_events_second_13300005 raw_events_second ON ((reference_table.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (raw_events_second.user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id) SELECT raw_events_second.user_id FROM (public.reference_table_13300012 reference_table LEFT JOIN public.raw_events_second_13300006 raw_events_second ON ((reference_table.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (raw_events_second.user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id) SELECT raw_events_second.user_id FROM (public.reference_table_13300012 reference_table LEFT JOIN public.raw_events_second_13300007 raw_events_second ON ((reference_table.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (raw_events_second.user_id IS NOT NULL)
+DEBUG:  Creating router plan
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.user_id
    WHERE raw_events_first.user_id = 10;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300000 raw_events_first LEFT JOIN public.raw_events_second_13300004 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE ((raw_events_first.user_id OPERATOR(pg_catalog.=) 10) AND (raw_events_first.user_id IS NOT NULL))
-DEBUG:  Skipping target shard interval 13300009 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300010 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300011 since SELECT query for it pruned away
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.user_id
    WHERE raw_events_second.user_id = 10 OR raw_events_second.user_id = 11;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300000 raw_events_first LEFT JOIN public.raw_events_second_13300004 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (((raw_events_second.user_id OPERATOR(pg_catalog.=) 10) OR (raw_events_second.user_id OPERATOR(pg_catalog.=) 11)) AND (raw_events_first.user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300001 raw_events_first LEFT JOIN (SELECT NULL::integer AS user_id, NULL::timestamp without time zone AS "time", NULL::integer AS value_1, NULL::integer AS value_2, NULL::double precision AS value_3, NULL::bigint AS value_4 WHERE false) raw_events_second(user_id, "time", value_1, value_2, value_3, value_4) ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (((raw_events_second.user_id OPERATOR(pg_catalog.=) 10) OR (raw_events_second.user_id OPERATOR(pg_catalog.=) 11)) AND (raw_events_first.user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300002 raw_events_first LEFT JOIN (SELECT NULL::integer AS user_id, NULL::timestamp without time zone AS "time", NULL::integer AS value_1, NULL::integer AS value_2, NULL::double precision AS value_3, NULL::bigint AS value_4 WHERE false) raw_events_second(user_id, "time", value_1, value_2, value_3, value_4) ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (((raw_events_second.user_id OPERATOR(pg_catalog.=) 10) OR (raw_events_second.user_id OPERATOR(pg_catalog.=) 11)) AND (raw_events_first.user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300003 raw_events_first LEFT JOIN public.raw_events_second_13300007 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE (((raw_events_second.user_id OPERATOR(pg_catalog.=) 10) OR (raw_events_second.user_id OPERATOR(pg_catalog.=) 11)) AND (raw_events_first.user_id IS NOT NULL))
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first INNER JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.user_id
    WHERE raw_events_first.user_id = 10 AND raw_events_first.user_id = 20;
-DEBUG:  Skipping target shard interval 13300008 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300009 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300010 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300011 since SELECT query for it pruned away
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.user_id
    WHERE raw_events_first.user_id = 10 AND raw_events_second.user_id = 20;
-DEBUG:  Skipping target shard interval 13300008 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300009 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300010 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300011 since SELECT query for it pruned away
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.user_id
    WHERE raw_events_first.user_id IN (19, 20, 21);
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300000 raw_events_first LEFT JOIN public.raw_events_second_13300004 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE ((raw_events_first.user_id OPERATOR(pg_catalog.=) ANY (ARRAY[19, 20, 21])) AND (raw_events_first.user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300001 raw_events_first LEFT JOIN public.raw_events_second_13300005 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE ((raw_events_first.user_id OPERATOR(pg_catalog.=) ANY (ARRAY[19, 20, 21])) AND (raw_events_first.user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300002 raw_events_first LEFT JOIN public.raw_events_second_13300006 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE ((raw_events_first.user_id OPERATOR(pg_catalog.=) ANY (ARRAY[19, 20, 21])) AND (raw_events_first.user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM ((SELECT NULL::integer AS user_id, NULL::timestamp without time zone AS "time", NULL::integer AS value_1, NULL::integer AS value_2, NULL::double precision AS value_3, NULL::bigint AS value_4 WHERE false) raw_events_first(user_id, "time", value_1, value_2, value_3, value_4) LEFT JOIN public.raw_events_second_13300007 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE ((raw_events_first.user_id OPERATOR(pg_catalog.=) ANY (ARRAY[19, 20, 21])) AND (raw_events_first.user_id IS NOT NULL))
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first INNER JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.user_id
    WHERE raw_events_second.user_id IN (19, 20, 21);
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300000 raw_events_first JOIN public.raw_events_second_13300004 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE ((raw_events_second.user_id OPERATOR(pg_catalog.=) ANY (ARRAY[19, 20, 21])) AND (raw_events_first.user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300001 raw_events_first JOIN public.raw_events_second_13300005 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE ((raw_events_second.user_id OPERATOR(pg_catalog.=) ANY (ARRAY[19, 20, 21])) AND (raw_events_first.user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300002 raw_events_first JOIN public.raw_events_second_13300006 raw_events_second ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE ((raw_events_second.user_id OPERATOR(pg_catalog.=) ANY (ARRAY[19, 20, 21])) AND (raw_events_first.user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id) SELECT raw_events_first.user_id FROM (public.raw_events_first_13300003 raw_events_first JOIN (SELECT NULL::integer AS user_id, NULL::timestamp without time zone AS "time", NULL::integer AS value_1, NULL::integer AS value_2, NULL::double precision AS value_3, NULL::bigint AS value_4 WHERE false) raw_events_second(user_id, "time", value_1, value_2, value_3, value_4) ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id))) WHERE ((raw_events_second.user_id OPERATOR(pg_catalog.=) ANY (ARRAY[19, 20, 21])) AND (raw_events_first.user_id IS NOT NULL))
 SET client_min_messages TO WARNING;
  -- following query should use repartitioned joins and results should
  -- be routed via coordinator
  SET citus.enable_repartition_joins TO true;
  INSERT INTO agg_events
              (user_id)
  SELECT raw_events_first.user_id
  FROM   raw_events_first,
         raw_events_second
  WHERE  raw_events_second.user_id = raw_events_first.value_1
         AND raw_events_first.value_1 = 12;
  -- some unsupported LEFT/INNER JOINs
  -- JOIN on one table with partition column other is not
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.value_1;
-ERROR:  complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
  -- same as the above with INNER JOIN
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first INNER JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.value_1;
  -- a not meaningful query
  INSERT INTO agg_events
              (user_id)
  SELECT raw_events_second.user_id
  FROM   raw_events_first,
         raw_events_second
  WHERE  raw_events_first.user_id = raw_events_first.value_1;
-ERROR:  cannot perform distributed planning on this query
-DETAIL:  Cartesian products are currently unsupported
  -- both tables joined on non-partition columns
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first LEFT JOIN raw_events_second ON raw_events_first.value_1 = raw_events_second.value_1;
-ERROR:  complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
  -- same as the above with INNER JOIN
  -- we support this with route to coordinator
  SELECT coordinator_plan($Q$
  EXPLAIN (costs off)
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first INNER JOIN raw_events_second ON raw_events_first.value_1 = raw_events_second.value_1;
 $Q$);
                               coordinator_plan                              
 ----------------------------------------------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-(4 rows)
+ Insert on agg_events
+   ->  Merge Join
+         Merge Cond: (raw_events_first.value_1 = raw_events_second.value_1)
+         ->  Sort
+               Sort Key: raw_events_first.value_1
+               ->  Seq Scan on raw_events_first
+         ->  Sort
+               Sort Key: raw_events_second.value_1
+               ->  Seq Scan on raw_events_second
+(9 rows)
 
 -- EXPLAIN ANALYZE is not supported for INSERT ... SELECT via coordinator
 EXPLAIN (costs off, analyze on)
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first INNER JOIN raw_events_second ON raw_events_first.value_1 = raw_events_second.value_1;
-ERROR:  EXPLAIN ANALYZE is currently not supported for INSERT ... SELECT commands via coordinator
+                                        QUERY PLAN                                         
+-------------------------------------------------------------------------------------------
+ Insert on agg_events (actual time=0.074..0.074 rows=0 loops=1)
+   ->  Merge Join (actual time=0.027..0.032 rows=7 loops=1)
+         Merge Cond: (raw_events_first.value_1 = raw_events_second.value_1)
+         ->  Sort (actual time=0.015..0.016 rows=9 loops=1)
+               Sort Key: raw_events_first.value_1
+               Sort Method: quicksort  Memory: 25kB
+               ->  Seq Scan on raw_events_first (actual time=0.006..0.008 rows=9 loops=1)
+         ->  Sort (actual time=0.007..0.008 rows=8 loops=1)
+               Sort Key: raw_events_second.value_1
+               Sort Method: quicksort  Memory: 25kB
+               ->  Seq Scan on raw_events_second (actual time=0.003..0.004 rows=9 loops=1)
+ Planning Time: 0.097 ms
+ Execution Time: 0.098 ms
+(13 rows)
+
 -- even if there is a filter on the partition key, since the join is not on the partition key we reject
 -- this query
 INSERT INTO agg_events (user_id)
 SELECT
   raw_events_first.user_id
 FROM
   raw_events_first LEFT JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.value_1
 WHERE
   raw_events_first.user_id = 10;
-ERROR:  complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
  -- same as the above with INNER JOIN
  -- we support this with route to coordinator
  SELECT coordinator_plan($Q$
  EXPLAIN (costs off)
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first INNER JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.value_1
  WHERE raw_events_first.user_id = 10;
 $Q$);
                               coordinator_plan                               
 -----------------------------------------------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-(4 rows)
+ Insert on agg_events
+   ->  Nested Loop
+         ->  Bitmap Heap Scan on raw_events_first
+               Recheck Cond: (user_id = 10)
+               ->  Bitmap Index Scan on raw_events_first_user_id_value_1_key
+                     Index Cond: (user_id = 10)
+         ->  Materialize
+               ->  Seq Scan on raw_events_second
+                     Filter: (value_1 = 10)
+(9 rows)
 
  -- make things a bit more complicate with IN clauses
  -- we support this with route to coordinator
  SELECT coordinator_plan($Q$
  EXPLAIN (costs off)
  INSERT INTO agg_events (user_id)
  SELECT
    raw_events_first.user_id
  FROM
    raw_events_first INNER JOIN raw_events_second ON raw_events_first.user_id = raw_events_second.value_1
    WHERE raw_events_first.value_1 IN (10, 11,12) OR raw_events_second.user_id IN (1,2,3,4);
 $Q$);
                                                                 coordinator_plan                                                                 
 -------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-(4 rows)
+ Insert on agg_events
+   ->  Merge Join
+         Merge Cond: (raw_events_first.user_id = raw_events_second.value_1)
+         Join Filter: ((raw_events_first.value_1 = ANY ('{10,11,12}'::integer[])) OR (raw_events_second.user_id = ANY ('{1,2,3,4}'::integer[])))
+         ->  Index Only Scan using raw_events_first_user_id_value_1_key on raw_events_first
+         ->  Sort
+               Sort Key: raw_events_second.value_1
+               ->  Seq Scan on raw_events_second
+(8 rows)
 
  -- implicit join on non partition column should also not be pushed down,
  -- so we fall back to route via coordinator
  SELECT coordinator_plan($Q$
  EXPLAIN (costs off)
  INSERT INTO agg_events
              (user_id)
  SELECT raw_events_first.user_id
  FROM   raw_events_first,
         raw_events_second
  WHERE  raw_events_second.user_id = raw_events_first.value_1;
 $Q$);
                                        coordinator_plan                                       
 ----------------------------------------------------------------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-(4 rows)
+ Insert on agg_events
+   ->  Merge Join
+         Merge Cond: (raw_events_second.user_id = raw_events_first.value_1)
+         ->  Index Only Scan using raw_events_second_user_id_value_1_key on raw_events_second
+         ->  Sort
+               Sort Key: raw_events_first.value_1
+               ->  Seq Scan on raw_events_first
+(7 rows)
 
 RESET client_min_messages;
  -- The following is again a tricky query for Citus. If the given filter was
  -- on value_1 as shown in the above, Citus could push it down and use
  -- distributed INSERT/SELECT. But we instead fall back to route via coordinator.
  SELECT coordinator_plan($Q$
  EXPLAIN (costs off)
  INSERT INTO agg_events
              (user_id)
  SELECT raw_events_first.user_id
  FROM   raw_events_first,
         raw_events_second
  WHERE  raw_events_second.user_id = raw_events_first.value_1
         AND raw_events_first.value_2 = 12;
 $Q$);
                              coordinator_plan                              
 ---------------------------------------------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-(4 rows)
+ Insert on agg_events
+   ->  Hash Join
+         Hash Cond: (raw_events_second.user_id = raw_events_first.value_1)
+         ->  Seq Scan on raw_events_second
+         ->  Hash
+               ->  Seq Scan on raw_events_first
+                     Filter: (value_2 = 12)
+(7 rows)
 
  -- foo is not joined on the partition key so the query is not
  -- pushed down. So instead we route via coordinator.
  SELECT coordinator_plan($Q$
  EXPLAIN (costs off)
  INSERT INTO agg_events
              (user_id, value_4_agg)
  SELECT
    outer_most.id, max(outer_most.value)
  FROM
@@ -1011,49 +855,47 @@
                      raw_events_second
              WHERE  raw_events_first.user_id = raw_events_second.user_id
              GROUP  BY raw_events_second.user_id
              HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
  ON (f.id = f2.id)) as outer_most
  GROUP BY
    outer_most.id;
 $Q$);
           coordinator_plan          
 ------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  HashAggregate
-         Group Key: remote_scan.user_id
-         ->  Custom Scan (Citus Adaptive)
-               ->  Distributed Subplan XXX_1
+ Insert on agg_events
    ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-(8 rows)
+         Task Count: 1
+(3 rows)
 
  -- if the given filter was on value_1 as shown in the above, Citus could
  -- push it down. But here the query falls back to route via coordinator.
  SELECT coordinator_plan($Q$
  EXPLAIN (costs off)
  INSERT INTO agg_events
              (user_id)
  SELECT raw_events_first.user_id
  FROM   raw_events_first,
         raw_events_second
  WHERE  raw_events_second.user_id = raw_events_first.value_1
         AND raw_events_first.value_2 = 12;
 $Q$);
                              coordinator_plan                              
 ---------------------------------------------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-(4 rows)
+ Insert on agg_events
+   ->  Hash Join
+         Hash Cond: (raw_events_second.user_id = raw_events_first.value_1)
+         ->  Seq Scan on raw_events_second
+         ->  Hash
+               ->  Seq Scan on raw_events_first
+                     Filter: (value_2 = 12)
+(7 rows)
 
  -- foo is not joined on the partition key so the query is not
  -- pushed down, and it falls back to route via coordinator
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
  INSERT INTO agg_events
              (user_id, value_4_agg)
  SELECT
    outer_most.id, max(outer_most.value)
  FROM
@@ -1076,193 +918,113 @@
                      raw_events_second
              WHERE  raw_events_first.user_id = raw_events_second.user_id
              GROUP  BY raw_events_second.user_id
              HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
  ON (f.id = f2.id)) as outer_most
  GROUP BY
    outer_most.id;
 $Q$);
           coordinator_plan          
 ------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  HashAggregate
-         Group Key: remote_scan.user_id
-         ->  Custom Scan (Citus Adaptive)
-               ->  Distributed Subplan XXX_1
+ Insert on agg_events
    ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-(8 rows)
+         Task Count: 1
+(3 rows)
 
 INSERT INTO agg_events
             (value_4_agg,
              value_1_agg,
              user_id)
 SELECT v4,
        v1,
        id
 FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
                SUM(raw_events_first.value_1) AS v1,
                raw_events_second.user_id      AS id
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id != raw_events_second.user_id
         GROUP  BY raw_events_second.user_id) AS foo;
-ERROR:  complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
 SET client_min_messages TO DEBUG2;
 -- INSERT returns NULL partition key value via coordinator
 INSERT INTO agg_events
             (value_4_agg,
              value_1_agg,
              user_id)
 SELECT v4,
        v1,
        id
 FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
                SUM(raw_events_first.value_1) AS v1,
                raw_events_second.value_3      AS id
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.value_3) AS foo;
-DEBUG:  Group by list without distribution column is not allowed  in distributed INSERT ... SELECT queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  join prunable for intervals [-2147483648,-1073741825] and [-1073741824,-1]
-DEBUG:  join prunable for intervals [-2147483648,-1073741825] and [0,1073741823]
-DEBUG:  join prunable for intervals [-2147483648,-1073741825] and [1073741824,2147483647]
-DEBUG:  join prunable for intervals [-1073741824,-1] and [-2147483648,-1073741825]
-DEBUG:  join prunable for intervals [-1073741824,-1] and [0,1073741823]
-DEBUG:  join prunable for intervals [-1073741824,-1] and [1073741824,2147483647]
-DEBUG:  join prunable for intervals [0,1073741823] and [-2147483648,-1073741825]
-DEBUG:  join prunable for intervals [0,1073741823] and [-1073741824,-1]
-DEBUG:  join prunable for intervals [0,1073741823] and [1073741824,2147483647]
-DEBUG:  join prunable for intervals [1073741824,2147483647] and [-2147483648,-1073741825]
-DEBUG:  join prunable for intervals [1073741824,2147483647] and [-1073741824,-1]
-DEBUG:  join prunable for intervals [1073741824,2147483647] and [0,1073741823]
-DEBUG:  generating subplan XXX_1 for subquery SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.value_3 AS id FROM public.raw_events_first, public.raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.value_3
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT int4(id) AS user_id, int4(v1) AS value_1_agg, int8(v4) AS value_4_agg FROM (SELECT intermediate_result.v4, intermediate_result.v1, intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(v4 numeric, v1 bigint, id double precision)) foo
-DEBUG:  Creating router plan
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
-ERROR:  the partition column of table public.agg_events cannot be NULL
 -- error cases
 -- no part column at all
 INSERT INTO raw_events_second
             (value_1)
 SELECT value_1
 FROM   raw_events_first;
-DEBUG:  cannot perform distributed INSERT INTO ... SELECT because the partition columns in the source table and subquery do not match
-DETAIL:  the query doesn't include the target table's partition column
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-ERROR:  the partition column of table public.raw_events_second should have a value
 INSERT INTO raw_events_second
             (value_1)
 SELECT user_id
 FROM   raw_events_first;
-DEBUG:  cannot perform distributed INSERT INTO ... SELECT because the partition columns in the source table and subquery do not match
-DETAIL:  the query doesn't include the target table's partition column
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-ERROR:  the partition column of table public.raw_events_second should have a value
 INSERT INTO raw_events_second
             (user_id)
 SELECT value_1
 FROM   raw_events_first;
-DEBUG:  cannot perform distributed INSERT INTO ... SELECT because the partition columns in the source table and subquery do not match
-DETAIL:  The target table's partition column should correspond to a partition column in the subquery.
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-DEBUG:  partitioning SELECT query by column index 0 with name 'user_id'
-ERROR:  the partition column value cannot be NULL
-CONTEXT:  while executing command on localhost:xxxxx
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id * 2
 FROM   raw_events_first;
-DEBUG:  cannot perform distributed INSERT INTO ... SELECT because the partition columns in the source table and subquery do not match
-DETAIL:  Subquery contains an operator in the same position as the target table's partition column.
-HINT:  Ensure the target table's partition column has a corresponding simple column reference to a distributed table's partition column in the subquery.
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-DEBUG:  partitioning SELECT query by column index 0 with name 'user_id'
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300000_to_0,repartitioned_results_xxxxx_from_13300001_to_0}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300000_to_1,repartitioned_results_xxxxx_from_13300001_to_1,repartitioned_results_xxxxx_from_13300003_to_1}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300006 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300001_to_2}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300000_to_3,repartitioned_results_xxxxx_from_13300002_to_3,repartitioned_results_xxxxx_from_13300003_to_3}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id :: bigint
 FROM   raw_events_first;
-DEBUG:  cannot perform distributed INSERT INTO ... SELECT because the partition columns in the source table and subquery do not match
-DETAIL:  Subquery contains an explicit cast in the same position as the target table's partition column.
-HINT:  Ensure the target table's partition column has a corresponding simple column reference to a distributed table's partition column in the subquery.
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-DEBUG:  partitioning SELECT query by column index 0 with name 'user_id'
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300000_to_0}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300001_to_1}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300006 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300002_to_2}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id) SELECT user_id FROM read_intermediate_results('{repartitioned_results_xxxxx_from_13300003_to_3}'::text[], 'binary'::citus_copy_format) intermediate_result(user_id integer)
 INSERT INTO agg_events
             (value_3_agg,
              value_4_agg,
              value_1_agg,
              value_2_agg,
              user_id)
 SELECT SUM(value_3),
        Count(value_4),
        user_id,
        SUM(value_1),
        Avg(value_2)
 FROM   raw_events_first
 GROUP  BY user_id;
-DEBUG:  cannot perform distributed INSERT INTO ... SELECT because the partition columns in the source table and subquery do not match
-DETAIL:  Subquery contains an aggregation in the same position as the target table's partition column.
-HINT:  Ensure the target table's partition column has a corresponding simple column reference to a distributed table's partition column in the subquery.
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-DEBUG:  partitioning SELECT query by column index 0 with name 'user_id'
-ERROR:  the partition column value cannot be NULL
-CONTEXT:  while executing command on localhost:xxxxx
 INSERT INTO agg_events
             (value_3_agg,
              value_4_agg,
              value_1_agg,
              value_2_agg,
              user_id)
 SELECT SUM(value_3),
        Count(value_4),
        user_id,
        SUM(value_1),
        value_2
 FROM   raw_events_first
 GROUP  BY user_id,
           value_2;
-DEBUG:  cannot perform distributed INSERT INTO ... SELECT because the partition columns in the source table and subquery do not match
-DETAIL:  The target table's partition column should correspond to a partition column in the subquery.
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-DEBUG:  partitioning SELECT query by column index 0 with name 'user_id'
-ERROR:  the partition column value cannot be NULL
-CONTEXT:  while executing command on localhost:xxxxx
+ERROR:  duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key"
+DETAIL:  Key (user_id, value_1_agg)=(200, 2) already exists.
 -- tables should be co-located
 INSERT INTO agg_events (user_id)
 SELECT
   user_id
 FROM
   reference_table;
-DEBUG:  cannot perform distributed INSERT INTO ... SELECT because the partition columns in the source table and subquery do not match
-DETAIL:  The target table's partition column should correspond to a partition column in the subquery.
-DEBUG:  Distributed planning for a fast-path router query
 DEBUG:  Creating router plan
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 -- foo2 is recursively planned and INSERT...SELECT is done via coordinator
 INSERT INTO agg_events
             (user_id)
 SELECT f2.id FROM
 (SELECT
       id
 FROM   (SELECT reference_table.user_id      AS id
         FROM   raw_events_first,
                reference_table
         WHERE  raw_events_first.user_id = reference_table.user_id ) AS foo) as f
@@ -1272,40 +1034,21 @@
        id
 FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
                raw_events_second.value_1 AS v1,
                SUM(raw_events_second.user_id)      AS id
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.value_1
         HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id = f2.id);
-DEBUG:  Group by list without distribution column is not allowed  in distributed INSERT ... SELECT queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  join prunable for intervals [-2147483648,-1073741825] and [-1073741824,-1]
-DEBUG:  join prunable for intervals [-2147483648,-1073741825] and [0,1073741823]
-DEBUG:  join prunable for intervals [-2147483648,-1073741825] and [1073741824,2147483647]
-DEBUG:  join prunable for intervals [-1073741824,-1] and [-2147483648,-1073741825]
-DEBUG:  join prunable for intervals [-1073741824,-1] and [0,1073741823]
-DEBUG:  join prunable for intervals [-1073741824,-1] and [1073741824,2147483647]
-DEBUG:  join prunable for intervals [0,1073741823] and [-2147483648,-1073741825]
-DEBUG:  join prunable for intervals [0,1073741823] and [-1073741824,-1]
-DEBUG:  join prunable for intervals [0,1073741823] and [1073741824,2147483647]
-DEBUG:  join prunable for intervals [1073741824,2147483647] and [-2147483648,-1073741825]
-DEBUG:  join prunable for intervals [1073741824,2147483647] and [-1073741824,-1]
-DEBUG:  join prunable for intervals [1073741824,2147483647] and [0,1073741823]
-DEBUG:  generating subplan XXX_1 for subquery SELECT sum(raw_events_second.value_4) AS v4, raw_events_second.value_1 AS v1, sum(raw_events_second.user_id) AS id FROM public.raw_events_first, public.raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.value_1 HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT int4(f2.id) AS user_id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first, public.reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT intermediate_result.v4, intermediate_result.v1, intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(v4 numeric, v1 integer, id bigint)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id)))
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-DEBUG:  partitioning SELECT query by column index 0 with name 'user_id'
+DEBUG:  Creating router plan
 -- the second part of the query is not routable since
 -- GROUP BY not on the partition column (i.e., value_1) and thus join
 -- on f.id = f2.id is not on the partition key (instead on the sum of partition key)
 -- but we still recursively plan foo2 and run the query
 INSERT INTO agg_events
             (user_id)
 SELECT f.id FROM
 (SELECT
       id
 FROM   (SELECT raw_events_first.user_id      AS id
@@ -1318,40 +1061,21 @@
        id
 FROM   (SELECT SUM(raw_events_second.value_4) AS v4,
                raw_events_second.value_1 AS v1,
                SUM(raw_events_second.user_id)      AS id
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.value_1
         HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id = f2.id);
-DEBUG:  Group by list without distribution column is not allowed  in distributed INSERT ... SELECT queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  join prunable for intervals [-2147483648,-1073741825] and [-1073741824,-1]
-DEBUG:  join prunable for intervals [-2147483648,-1073741825] and [0,1073741823]
-DEBUG:  join prunable for intervals [-2147483648,-1073741825] and [1073741824,2147483647]
-DEBUG:  join prunable for intervals [-1073741824,-1] and [-2147483648,-1073741825]
-DEBUG:  join prunable for intervals [-1073741824,-1] and [0,1073741823]
-DEBUG:  join prunable for intervals [-1073741824,-1] and [1073741824,2147483647]
-DEBUG:  join prunable for intervals [0,1073741823] and [-2147483648,-1073741825]
-DEBUG:  join prunable for intervals [0,1073741823] and [-1073741824,-1]
-DEBUG:  join prunable for intervals [0,1073741823] and [1073741824,2147483647]
-DEBUG:  join prunable for intervals [1073741824,2147483647] and [-2147483648,-1073741825]
-DEBUG:  join prunable for intervals [1073741824,2147483647] and [-1073741824,-1]
-DEBUG:  join prunable for intervals [1073741824,2147483647] and [0,1073741823]
-DEBUG:  generating subplan XXX_1 for subquery SELECT sum(raw_events_second.value_4) AS v4, raw_events_second.value_1 AS v1, sum(raw_events_second.user_id) AS id FROM public.raw_events_first, public.raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.value_1 HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT f.id AS user_id FROM ((SELECT foo.id FROM (SELECT raw_events_first.user_id AS id FROM public.raw_events_first, public.reference_table WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT intermediate_result.v4, intermediate_result.v1, intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(v4 numeric, v1 integer, id bigint)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id)))
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  performing repartitioned INSERT ... SELECT
-DEBUG:  partitioning SELECT query by column index 0 with name 'user_id'
+DEBUG:  Creating router plan
 SET client_min_messages TO WARNING;
 -- cannot pushdown the query since the JOIN is not equi JOIN
 -- falls back to route via coordinator
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
 INSERT INTO agg_events
             (user_id, value_4_agg)
 SELECT
 outer_most.id, max(outer_most.value)
  FROM
@@ -1373,29 +1097,24 @@
             FROM   raw_events_first,
                     raw_events_second
             WHERE  raw_events_first.user_id = raw_events_second.user_id
             GROUP  BY raw_events_second.user_id
             HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id != f2.id)) as outer_most
 GROUP BY outer_most.id;
 $Q$);
           coordinator_plan          
 ------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  HashAggregate
-         Group Key: remote_scan.user_id
-         ->  Custom Scan (Citus Adaptive)
-               ->  Distributed Subplan XXX_1
+ Insert on agg_events
    ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-(8 rows)
+         Task Count: 1
+(3 rows)
 
 -- cannot pushdown since foo2 is not join on partition key
 -- falls back to route via coordinator
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
 INSERT INTO agg_events
             (user_id, value_4_agg)
 SELECT
   outer_most.id, max(outer_most.value)
 FROM
@@ -1418,32 +1137,24 @@
                     raw_events_second
             WHERE  raw_events_first.user_id = raw_events_second.value_1
             GROUP  BY raw_events_second.user_id
             HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id = f2.id)) as outer_most
 GROUP BY
   outer_most.id;
 $Q$);
           coordinator_plan          
 ------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  HashAggregate
-         Group Key: remote_scan.user_id
-         ->  Custom Scan (Citus Adaptive)
-               ->  Distributed Subplan XXX_1
-                     ->  HashAggregate
-                           Group Key: remote_scan.id
-                           Filter: (pg_catalog.sum(remote_scan.worker_column_4) > '10'::numeric)
+ Insert on agg_events
    ->  Custom Scan (Citus Adaptive)
-                                 Task Count: 4
-(11 rows)
+         Task Count: 1
+(3 rows)
 
 -- cannot push down since foo doesn't have en equi join
 -- falls back to route via coordinator
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
 INSERT INTO agg_events
             (user_id, value_4_agg)
 SELECT
   outer_most.id, max(outer_most.value)
 FROM
@@ -1466,29 +1177,24 @@
                     raw_events_second
             WHERE  raw_events_first.user_id = raw_events_second.user_id
             GROUP  BY raw_events_second.user_id
             HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id = f2.id)) as outer_most
 GROUP BY
   outer_most.id;
 $Q$);
           coordinator_plan          
 ------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  HashAggregate
-         Group Key: remote_scan.user_id
-         ->  Custom Scan (Citus Adaptive)
-               ->  Distributed Subplan XXX_1
+ Insert on agg_events
    ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-(8 rows)
+         Task Count: 1
+(3 rows)
 
 -- some unsupported LATERAL JOINs
 -- join on averages is not on the partition key
 -- should fall back to route via coordinator
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
 INSERT INTO agg_events (user_id, value_4_agg)
 SELECT
   averages.user_id, avg(averages.value_4)
 FROM
@@ -1500,27 +1206,24 @@
   JOIN LATERAL
     (SELECT
       user_id, value_4
     FROM
       raw_events_first WHERE
       value_4 = reference_ids.user_id) as averages ON true
     GROUP BY averages.user_id;
 $Q$);
           coordinator_plan          
 ------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  HashAggregate
-         Group Key: remote_scan.user_id
+ Insert on agg_events
    ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-(6 rows)
+         Task Count: 1
+(3 rows)
 
 -- join among reference_ids and averages is not on the partition key
 -- should fall back to route via coordinator
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
 INSERT INTO agg_events (user_id, value_4_agg)
 SELECT
   averages.user_id, avg(averages.value_4)
 FROM
     (SELECT
@@ -1530,29 +1233,24 @@
     ) reference_ids
   JOIN LATERAL
     (SELECT
       user_id, value_4
     FROM
       raw_events_first) as averages ON averages.value_4 = reference_ids.user_id
     GROUP BY averages.user_id;
 $Q$);
           coordinator_plan          
 ------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
-   ->  HashAggregate
-         Group Key: remote_scan.user_id
-         ->  Custom Scan (Citus Adaptive)
-               ->  Distributed Subplan XXX_1
+ Insert on agg_events
    ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-(8 rows)
+         Task Count: 1
+(3 rows)
 
 -- join among the agg_ids and averages is not on the partition key
 -- should fall back to route via coordinator
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
 INSERT INTO agg_events (user_id, value_4_agg)
 SELECT
   averages.user_id, avg(averages.value_4)
 FROM
     (SELECT
@@ -1564,46 +1262,47 @@
     (SELECT
       user_id, value_4
     FROM
       raw_events_first) as averages ON averages.user_id = reference_ids.user_id
 JOIN LATERAL
     (SELECT user_id, value_4 FROM agg_events) as agg_ids ON (agg_ids.value_4 = averages.user_id)
     GROUP BY averages.user_id;
 $Q$);
           coordinator_plan          
 ------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: pull to coordinator
+ Insert on agg_events
    ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-(4 rows)
+         Task Count: 1
+(3 rows)
 
 -- Selected value in the WHERE is not partition key, so we cannot use distributed
 -- INSERT/SELECT and falls back route via coordinator
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id IN (SELECT value_1
                    FROM   raw_events_second);
 $Q$);
                               coordinator_plan                               
 -----------------------------------------------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: repartition
-   ->  Custom Scan (Citus Adaptive)
-         ->  Distributed Subplan XXX_1
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-(6 rows)
+ Insert on raw_events_second
+   ->  Hash Join
+         Hash Cond: (raw_events_first.user_id = raw_events_second_1.value_1)
+         ->  Seq Scan on raw_events_first
+         ->  Hash
+               ->  HashAggregate
+                     Group Key: raw_events_second_1.value_1
+                     ->  Seq Scan on raw_events_second raw_events_second_1
+(8 rows)
 
 -- same as above but slightly more complex
 -- since it also includes subquery in FROM as well
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
 INSERT INTO agg_events
             (user_id)
 SELECT f2.id FROM
 
 (SELECT
@@ -1623,90 +1322,83 @@
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.user_id
         HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id = f2.id)
 WHERE f.id IN (SELECT value_1
                FROM   raw_events_second);
 $Q$);
           coordinator_plan          
 ------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: repartition
-   ->  Custom Scan (Citus Adaptive)
-         ->  Distributed Subplan XXX_1
+ Insert on agg_events
    ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-(6 rows)
+         Task Count: 1
+(3 rows)
 
 -- some more semi-anti join tests
 SET client_min_messages TO DEBUG2;
 -- join in where
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id IN (SELECT raw_events_second.user_id
                    FROM   raw_events_second, raw_events_first
                    WHERE  raw_events_second.user_id = raw_events_first.user_id AND raw_events_first.user_id = 200);
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300000 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300004 raw_events_second, public.raw_events_first_13300000 raw_events_first_1 WHERE ((raw_events_second.user_id OPERATOR(pg_catalog.=) raw_events_first_1.user_id) AND (raw_events_first_1.user_id OPERATOR(pg_catalog.=) 200)))) AND (user_id IS NOT NULL))
-DEBUG:  Skipping target shard interval 13300005 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300006 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300007 since SELECT query for it pruned away
 RESET client_min_messages;
 -- we cannot push this down since it is NOT IN
 -- we use repartition insert/select instead
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id NOT IN (SELECT raw_events_second.user_id
                    FROM   raw_events_second, raw_events_first
                    WHERE  raw_events_second.user_id = raw_events_first.user_id AND raw_events_first.user_id = 200);
 $Q$);
                                      coordinator_plan                                      
 -------------------------------------------------------------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: repartition
-   ->  Custom Scan (Citus Adaptive)
-         ->  Distributed Subplan XXX_1
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 1
-(6 rows)
+ Insert on raw_events_second
+   ->  Seq Scan on raw_events_first
+         Filter: (NOT (hashed SubPlan 1))
+         SubPlan 1
+           ->  Nested Loop
+                 ->  Bitmap Heap Scan on raw_events_second raw_events_second_1
+                       Recheck Cond: (user_id = 200)
+                       ->  Bitmap Index Scan on raw_events_second_user_id_value_1_key
+                             Index Cond: (user_id = 200)
+                 ->  Materialize
+                       ->  Bitmap Heap Scan on raw_events_first raw_events_first_1
+                             Recheck Cond: (user_id = 200)
+                             ->  Bitmap Index Scan on raw_events_first_user_id_value_1_key
+                                   Index Cond: (user_id = 200)
+(14 rows)
 
 SET client_min_messages TO DEBUG2;
 -- safe to push down
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  EXISTS (SELECT 1
                    FROM   raw_events_second
                    WHERE  raw_events_second.user_id =raw_events_first.user_id);
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300000 raw_events_first WHERE ((EXISTS (SELECT 1 FROM public.raw_events_second_13300004 raw_events_second WHERE (raw_events_second.user_id OPERATOR(pg_catalog.=) raw_events_first.user_id))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300001 raw_events_first WHERE ((EXISTS (SELECT 1 FROM public.raw_events_second_13300005 raw_events_second WHERE (raw_events_second.user_id OPERATOR(pg_catalog.=) raw_events_first.user_id))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300006 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300002 raw_events_first WHERE ((EXISTS (SELECT 1 FROM public.raw_events_second_13300006 raw_events_second WHERE (raw_events_second.user_id OPERATOR(pg_catalog.=) raw_events_first.user_id))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300003 raw_events_first WHERE ((EXISTS (SELECT 1 FROM public.raw_events_second_13300007 raw_events_second WHERE (raw_events_second.user_id OPERATOR(pg_catalog.=) raw_events_first.user_id))) AND (user_id IS NOT NULL))
 -- we cannot push down
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  NOT EXISTS (SELECT 1
                    FROM   raw_events_second
                    WHERE  raw_events_second.user_id =raw_events_first.user_id);
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300000 raw_events_first WHERE ((NOT (EXISTS (SELECT 1 FROM public.raw_events_second_13300004 raw_events_second WHERE (raw_events_second.user_id OPERATOR(pg_catalog.=) raw_events_first.user_id)))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300001 raw_events_first WHERE ((NOT (EXISTS (SELECT 1 FROM public.raw_events_second_13300005 raw_events_second WHERE (raw_events_second.user_id OPERATOR(pg_catalog.=) raw_events_first.user_id)))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300006 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300002 raw_events_first WHERE ((NOT (EXISTS (SELECT 1 FROM public.raw_events_second_13300006 raw_events_second WHERE (raw_events_second.user_id OPERATOR(pg_catalog.=) raw_events_first.user_id)))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300003 raw_events_first WHERE ((NOT (EXISTS (SELECT 1 FROM public.raw_events_second_13300007 raw_events_second WHERE (raw_events_second.user_id OPERATOR(pg_catalog.=) raw_events_first.user_id)))) AND (user_id IS NOT NULL))
 -- more complex LEFT JOINs
  INSERT INTO agg_events
              (user_id, value_4_agg)
  SELECT
    outer_most.id, max(outer_most.value)
  FROM
  (
    SELECT f2.id as id, f2.v4 as value FROM
      (SELECT
            id
@@ -1722,24 +1414,21 @@
                 SUM(raw_events_first.value_1) AS v1,
                 raw_events_second.user_id      AS id
              FROM   raw_events_first,
                      raw_events_second
              WHERE  raw_events_first.user_id = raw_events_second.user_id
              GROUP  BY raw_events_second.user_id
              HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
  ON (f.id = f2.id)) as outer_most
  GROUP BY
    outer_most.id;
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300008 AS citus_table_alias (user_id, value_4_agg) SELECT id, max(value) AS max FROM (SELECT f2.id, f2.v4 AS value FROM ((SELECT foo.id FROM (SELECT raw_events_first.user_id AS id FROM (public.raw_events_first_13300000 raw_events_first LEFT JOIN public.reference_table_13300012 reference_table ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)))) foo) f LEFT JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300000 raw_events_first, public.raw_events_second_13300004 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id)))) outer_most WHERE (id IS NOT NULL) GROUP BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300009 AS citus_table_alias (user_id, value_4_agg) SELECT id, max(value) AS max FROM (SELECT f2.id, f2.v4 AS value FROM ((SELECT foo.id FROM (SELECT raw_events_first.user_id AS id FROM (public.raw_events_first_13300001 raw_events_first LEFT JOIN public.reference_table_13300012 reference_table ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)))) foo) f LEFT JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300001 raw_events_first, public.raw_events_second_13300005 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id)))) outer_most WHERE (id IS NOT NULL) GROUP BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300010 AS citus_table_alias (user_id, value_4_agg) SELECT id, max(value) AS max FROM (SELECT f2.id, f2.v4 AS value FROM ((SELECT foo.id FROM (SELECT raw_events_first.user_id AS id FROM (public.raw_events_first_13300002 raw_events_first LEFT JOIN public.reference_table_13300012 reference_table ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)))) foo) f LEFT JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300002 raw_events_first, public.raw_events_second_13300006 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id)))) outer_most WHERE (id IS NOT NULL) GROUP BY id
-DEBUG:  distributed statement: INSERT INTO public.agg_events_13300011 AS citus_table_alias (user_id, value_4_agg) SELECT id, max(value) AS max FROM (SELECT f2.id, f2.v4 AS value FROM ((SELECT foo.id FROM (SELECT raw_events_first.user_id AS id FROM (public.raw_events_first_13300003 raw_events_first LEFT JOIN public.reference_table_13300012 reference_table ON ((raw_events_first.user_id OPERATOR(pg_catalog.=) reference_table.user_id)))) foo) f LEFT JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300003 raw_events_first, public.raw_events_second_13300007 raw_events_second WHERE (raw_events_first.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id)))) outer_most WHERE (id IS NOT NULL) GROUP BY id
+DEBUG:  Creating router plan
 RESET client_min_messages;
 -- cannot push down since the f.id IN is matched with value_1
 -- we use repartition insert/select instead
 SELECT coordinator_plan($Q$
 EXPLAIN (costs off)
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id IN (
@@ -1761,27 +1450,24 @@
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.user_id
         HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id = f2.id)
 WHERE f.id IN (SELECT value_1
                FROM   raw_events_second));
 $Q$);
           coordinator_plan          
 ------------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: repartition
-   ->  Custom Scan (Citus Adaptive)
-         ->  Distributed Subplan XXX_1
+ Insert on raw_events_second
    ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-(6 rows)
+         Task Count: 1
+(3 rows)
 
 SET client_min_messages TO DEBUG2;
 -- same as above, but this time is it safe to push down since
 -- f.id IN is matched with user_id
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id IN (
 SELECT f2.id FROM
@@ -1799,24 +1485,21 @@
                SUM(raw_events_first.value_1) AS v1,
                raw_events_second.user_id      AS id
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.user_id
         HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id = f2.id)
 WHERE f.id IN (SELECT user_id
                FROM   raw_events_second));
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300004 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300000 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300000 raw_events_first_1, public.reference_table_13300012 reference_table WHERE (raw_events_first_1.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first_1.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300000 raw_events_first_1, public.raw_events_second_13300004 raw_events_second WHERE (raw_events_first_1.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE (f.id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300004 raw_events_second)))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300005 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300001 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300001 raw_events_first_1, public.reference_table_13300012 reference_table WHERE (raw_events_first_1.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first_1.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300001 raw_events_first_1, public.raw_events_second_13300005 raw_events_second WHERE (raw_events_first_1.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE (f.id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300005 raw_events_second)))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300006 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300002 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300002 raw_events_first_1, public.reference_table_13300012 reference_table WHERE (raw_events_first_1.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first_1.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300002 raw_events_first_1, public.raw_events_second_13300006 raw_events_second WHERE (raw_events_first_1.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE (f.id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300006 raw_events_second)))) AND (user_id IS NOT NULL))
-DEBUG:  distributed statement: INSERT INTO public.raw_events_second_13300007 AS citus_table_alias (user_id) SELECT user_id FROM public.raw_events_first_13300003 raw_events_first WHERE ((user_id OPERATOR(pg_catalog.=) ANY (SELECT f2.id FROM ((SELECT foo.id FROM (SELECT reference_table.user_id AS id FROM public.raw_events_first_13300003 raw_events_first_1, public.reference_table_13300012 reference_table WHERE (raw_events_first_1.user_id OPERATOR(pg_catalog.=) reference_table.user_id)) foo) f JOIN (SELECT foo2.v4, foo2.v1, foo2.id FROM (SELECT sum(raw_events_second.value_4) AS v4, sum(raw_events_first_1.value_1) AS v1, raw_events_second.user_id AS id FROM public.raw_events_first_13300003 raw_events_first_1, public.raw_events_second_13300007 raw_events_second WHERE (raw_events_first_1.user_id OPERATOR(pg_catalog.=) raw_events_second.user_id) GROUP BY raw_events_second.user_id HAVING (sum(raw_events_second.value_4) OPERATOR(pg_catalog.>) (10)::numeric)) foo2) f2 ON ((f.id OPERATOR(pg_catalog.=) f2.id))) WHERE (f.id OPERATOR(pg_catalog.=) ANY (SELECT raw_events_second.user_id FROM public.raw_events_second_13300007 raw_events_second)))) AND (user_id IS NOT NULL))
+DEBUG:  Creating router plan
 RESET client_min_messages;
 -- cannot push down since top level user_id is matched with NOT IN
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id NOT IN (
 SELECT f2.id FROM
 (SELECT
       id
@@ -1832,22 +1515,20 @@
                SUM(raw_events_first.value_1) AS v1,
                raw_events_second.user_id      AS id
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.user_id
         HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id = f2.id)
 WHERE f.id IN (SELECT user_id
                FROM   raw_events_second));
-ERROR:  cannot pushdown the subquery
-DETAIL:  There exist a reference table in the outer part of the outer join
 -- cannot push down since join is not equi join (f.id > f2.id)
 INSERT INTO raw_events_second
             (user_id)
 SELECT user_id
 FROM   raw_events_first
 WHERE  user_id IN (
 SELECT f2.id FROM
 (SELECT
       id
 FROM   (SELECT reference_table.user_id      AS id
@@ -1862,34 +1543,32 @@
                SUM(raw_events_first.value_1) AS v1,
                raw_events_second.user_id      AS id
         FROM   raw_events_first,
                raw_events_second
         WHERE  raw_events_first.user_id = raw_events_second.user_id
         GROUP  BY raw_events_second.user_id
         HAVING SUM(raw_events_second.value_4) > 10) AS foo2 ) as f2
 ON (f.id > f2.id)
 WHERE f.id IN (SELECT user_id
                FROM   raw_events_second));
-ERROR:  cannot pushdown the subquery
-DETAIL:  There exist a reference table in the outer part of the outer join
 -- we currently not support grouping sets
 INSERT INTO agg_events
             (user_id,
              value_1_agg,
              value_2_agg)
 SELECT user_id,
        Sum(value_1) AS sum_val1,
        Sum(value_2) AS sum_val2
 FROM   raw_events_second
 GROUP  BY grouping sets ( ( user_id ), ( value_1 ), ( user_id, value_1 ), ( ) );
-ERROR:  could not run distributed query with GROUPING SETS, CUBE, or ROLLUP
-HINT:  Consider using an equality filter on the distributed table's partition column.
+ERROR:  duplicate key value violates unique constraint "agg_events_user_id_value_1_agg_key"
+DETAIL:  Key (user_id, value_1_agg)=(5, 50) already exists.
 -- set back to INFO
 SET client_min_messages TO INFO;
 -- avoid constraint violations
 TRUNCATE raw_events_first;
 -- we don't support LIMIT for subquery pushdown, but
 -- we recursively plan the query and run it via coordinator
 INSERT INTO agg_events(user_id)
 SELECT user_id
 FROM   users_table
 WHERE  user_id
@@ -1966,36 +1645,39 @@
 COPY raw_events_first (user_id, value_1) FROM STDIN DELIMITER ',';
 ROLLBACK;
 -- Similarly, multi-row INSERTs will take part in transactions and reuse connections...
 BEGIN;
 INSERT INTO raw_events_first SELECT * FROM raw_events_second WHERE user_id = 100;
 COPY raw_events_first (user_id, value_1) FROM STDIN DELIMITER ',';
 INSERT INTO raw_events_first (user_id, value_1) VALUES (105, 105), (106, 106);
 ROLLBACK;
 -- selecting from views works
 CREATE VIEW test_view AS SELECT * FROM raw_events_first;
+WARNING:  "view test_view" has dependency to "table raw_events_first" that is not in Citus' metadata
+DETAIL:  "view test_view" will be created only locally
+HINT:  Distribute "table raw_events_first" first to distribute "view test_view"
 INSERT INTO raw_events_first (user_id, time, value_1, value_2, value_3, value_4) VALUES
                          (16, now(), 60, 600, 6000.1, 60000);
 SELECT count(*) FROM raw_events_second;
  count 
 -------
-    36
+    72
 (1 row)
 
 INSERT INTO raw_events_second SELECT * FROM test_view;
 INSERT INTO raw_events_first (user_id, time, value_1, value_2, value_3, value_4) VALUES
                          (17, now(), 60, 600, 6000.1, 60000);
 INSERT INTO raw_events_second SELECT * FROM test_view WHERE user_id = 17 GROUP BY 1,2,3,4,5,6;
 SELECT count(*) FROM raw_events_second;
  count 
 -------
-    38
+    74
 (1 row)
 
 -- intermediate results (CTEs) should be allowed when doing INSERT...SELECT within a CTE
 WITH series AS (
   SELECT s AS val FROM generate_series(60,70) s
 ),
 inserts AS (
   INSERT INTO raw_events_second (user_id)
   SELECT
     user_id
@@ -2008,74 +1690,54 @@
  count 
 -------
      2
 (1 row)
 
 -- we need this in our next test
 truncate raw_events_first;
 SET client_min_messages TO DEBUG2;
 -- first show that the query works now
 INSERT INTO raw_events_first SELECT * FROM raw_events_second;
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300000 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300004 raw_events_second WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300001 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300005 raw_events_second WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300002 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300006 raw_events_second WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300003 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300007 raw_events_second WHERE (user_id IS NOT NULL)
 SET client_min_messages TO INFO;
 truncate raw_events_first;
 SET client_min_messages TO DEBUG2;
 -- now show that it works for a single shard query as well
 INSERT INTO raw_events_first SELECT * FROM raw_events_second WHERE user_id = 5;
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300000 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300004 raw_events_second WHERE ((user_id OPERATOR(pg_catalog.=) 5) AND (user_id IS NOT NULL))
-DEBUG:  Skipping target shard interval 13300001 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300002 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300003 since SELECT query for it pruned away
 SET client_min_messages TO INFO;
 -- if a single shard of the SELECT is unhealty, the query should fail
 UPDATE pg_dist_shard_placement SET shardstate = 3 WHERE shardid = 13300004 AND nodeport = :worker_1_port;
 truncate raw_events_first;
 SET client_min_messages TO DEBUG2;
 -- this should fail
 INSERT INTO raw_events_first SELECT * FROM raw_events_second;
-ERROR:  cannot perform distributed planning for the given modification
-DETAIL:  Insert query cannot be executed on all placements for shard xxxxx
 -- this should also fail
 INSERT INTO raw_events_first SELECT * FROM raw_events_second WHERE user_id = 5;
-ERROR:  cannot perform distributed planning for the given modification
-DETAIL:  Insert query cannot be executed on all placements for shard xxxxx
+ERROR:  duplicate key value violates unique constraint "raw_events_first_user_id_value_1_key"
+DETAIL:  Key (user_id, value_1)=(5, 50) already exists.
 -- but this should work given that it hits different shard
 INSERT INTO raw_events_first SELECT * FROM raw_events_second WHERE user_id = 6;
-DEBUG:  Skipping target shard interval 13300000 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300001 since SELECT query for it pruned away
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300002 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300006 raw_events_second WHERE ((user_id OPERATOR(pg_catalog.=) 6) AND (user_id IS NOT NULL))
-DEBUG:  Skipping target shard interval 13300003 since SELECT query for it pruned away
+ERROR:  duplicate key value violates unique constraint "raw_events_first_user_id_value_1_key"
+DETAIL:  Key (user_id, value_1)=(6, 60) already exists.
 SET client_min_messages TO INFO;
 -- mark the unhealthy placement as healthy again for the next tests
 UPDATE pg_dist_shard_placement SET shardstate = 1 WHERE shardid = 13300004 AND nodeport = :worker_1_port;
 -- now that we should show that it works if one of the target shard interval is not healthy
 UPDATE pg_dist_shard_placement SET shardstate = 3 WHERE shardid = 13300000 AND nodeport = :worker_1_port;
 truncate raw_events_first;
 SET client_min_messages TO DEBUG2;
 -- this should work
 INSERT INTO raw_events_first SELECT * FROM raw_events_second;
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300000 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300004 raw_events_second WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300001 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300005 raw_events_second WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300002 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300006 raw_events_second WHERE (user_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300003 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300007 raw_events_second WHERE (user_id IS NOT NULL)
 SET client_min_messages TO INFO;
 truncate raw_events_first;
 SET client_min_messages TO DEBUG2;
 -- this should also work
 INSERT INTO raw_events_first SELECT * FROM raw_events_second WHERE user_id = 5;
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300000 AS citus_table_alias (user_id, "time", value_1, value_2, value_3, value_4) SELECT user_id, "time", value_1, value_2, value_3, value_4 FROM public.raw_events_second_13300004 raw_events_second WHERE ((user_id OPERATOR(pg_catalog.=) 5) AND (user_id IS NOT NULL))
-DEBUG:  Skipping target shard interval 13300001 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300002 since SELECT query for it pruned away
-DEBUG:  Skipping target shard interval 13300003 since SELECT query for it pruned away
 SET client_min_messages TO INFO;
 -- now do some tests with varchars
 INSERT INTO insert_select_varchar_test VALUES ('test_1', 10);
 INSERT INTO insert_select_varchar_test VALUES ('test_2', 30);
 INSERT INTO insert_select_varchar_test (key, value)
 SELECT *, 100
 FROM   (SELECT f1.key
         FROM   (SELECT key
                 FROM   insert_select_varchar_test
                 GROUP  BY 1
@@ -2103,165 +1765,132 @@
 (
   store_id int,
   first_name text,
   default_1 int DEFAULT 1,
   last_name text,
   default_2 text DEFAULT '2'
 );
 -- we don't need many shards
 SET citus.shard_count = 2;
 SELECT create_distributed_table('table_with_defaults', 'store_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- let's see the queries
 SET client_min_messages TO DEBUG2;
 -- a very simple query
 INSERT INTO table_with_defaults SELECT * FROM table_with_defaults;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, last_name, default_2) SELECT store_id, first_name, default_1, last_name, default_2 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, last_name, default_2) SELECT store_id, first_name, default_1, last_name, default_2 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL)
 -- see that defaults are filled
 INSERT INTO table_with_defaults (store_id, first_name)
 SELECT
   store_id, first_name
 FROM
   table_with_defaults;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, first_name, 1 AS default_1, '2'::text AS default_2 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, first_name, 1 AS default_1, '2'::text AS default_2 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL)
 -- shuffle one of the defaults and skip the other
 INSERT INTO table_with_defaults (default_2, store_id, first_name)
 SELECT
   default_2, store_id, first_name
 FROM
   table_with_defaults;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, first_name, 1 AS default_1, default_2 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, first_name, 1 AS default_1, default_2 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL)
 -- shuffle both defaults
 INSERT INTO table_with_defaults (default_2, store_id, default_1, first_name)
 SELECT
   default_2, store_id, default_1, first_name
 FROM
   table_with_defaults;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, first_name, default_1, default_2 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, first_name, default_1, default_2 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL)
 -- use constants instead of non-default column
 INSERT INTO table_with_defaults (default_2, last_name, store_id, first_name)
 SELECT
   default_2, 'Freund', store_id, 'Andres'
 FROM
   table_with_defaults;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, last_name, default_2) SELECT store_id, 'Andres'::text AS first_name, 1 AS default_1, 'Freund'::text AS last_name, default_2 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, last_name, default_2) SELECT store_id, 'Andres'::text AS first_name, 1 AS default_1, 'Freund'::text AS last_name, default_2 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL)
 -- use constants instead of non-default column and skip both defauls
 INSERT INTO table_with_defaults (last_name, store_id, first_name)
 SELECT
   'Freund', store_id, 'Andres'
 FROM
   table_with_defaults;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, last_name, default_2) SELECT store_id, 'Andres'::text AS first_name, 1 AS default_1, 'Freund'::text AS last_name, '2'::text AS default_2 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, last_name, default_2) SELECT store_id, 'Andres'::text AS first_name, 1 AS default_1, 'Freund'::text AS last_name, '2'::text AS default_2 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL)
 -- use constants instead of default columns
 INSERT INTO table_with_defaults (default_2, last_name, store_id, first_name, default_1)
 SELECT
   20, last_name, store_id, first_name, 10
 FROM
   table_with_defaults;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, last_name, default_2) SELECT store_id, first_name, 10, last_name, 20 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, last_name, default_2) SELECT store_id, first_name, 10, last_name, 20 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL)
 -- use constants instead of both default columns and non-default columns
 INSERT INTO table_with_defaults (default_2, last_name, store_id, first_name, default_1)
 SELECT
   20, 'Freund', store_id, 'Andres', 10
 FROM
   table_with_defaults;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, last_name, default_2) SELECT store_id, 'Andres'::text AS first_name, 10, 'Freund'::text AS last_name, 20 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL)
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, last_name, default_2) SELECT store_id, 'Andres'::text AS first_name, 10, 'Freund'::text AS last_name, 20 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL)
 -- some of the ultimate queries where we have constants,
 -- defaults and group by entry is not on the target entry
 INSERT INTO table_with_defaults (default_2, store_id, first_name)
 SELECT
   '2000', store_id, 'Andres'
 FROM
   table_with_defaults
 GROUP BY
   last_name, store_id;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, 'Andres'::text AS first_name, 1 AS default_1, '2000'::text AS default_2 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL) GROUP BY last_name, store_id
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, 'Andres'::text AS first_name, 1 AS default_1, '2000'::text AS default_2 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL) GROUP BY last_name, store_id
 INSERT INTO table_with_defaults (default_1, store_id, first_name, default_2)
 SELECT
   1000, store_id, 'Andres', '2000'
 FROM
   table_with_defaults
 GROUP BY
   last_name, store_id, first_name;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, 'Andres'::text AS first_name, 1000, '2000'::text AS default_2 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL) GROUP BY last_name, store_id, first_name
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, 'Andres'::text AS first_name, 1000, '2000'::text AS default_2 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL) GROUP BY last_name, store_id, first_name
 INSERT INTO table_with_defaults (default_1, store_id, first_name, default_2)
 SELECT
   1000, store_id, 'Andres', '2000'
 FROM
   table_with_defaults
 GROUP BY
   last_name, store_id, first_name, default_2;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, 'Andres'::text AS first_name, 1000, '2000'::text AS default_2 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL) GROUP BY last_name, store_id, first_name, default_2
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, 'Andres'::text AS first_name, 1000, '2000'::text AS default_2 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL) GROUP BY last_name, store_id, first_name, default_2
 INSERT INTO table_with_defaults (default_1, store_id, first_name)
 SELECT
   1000, store_id, 'Andres'
 FROM
   table_with_defaults
 GROUP BY
   last_name, store_id, first_name, default_2;
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300017 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, 'Andres'::text AS first_name, 1000, '2'::text AS default_2 FROM public.table_with_defaults_13300017 table_with_defaults WHERE (store_id IS NOT NULL) GROUP BY last_name, store_id, first_name, default_2
-DEBUG:  distributed statement: INSERT INTO public.table_with_defaults_13300018 AS citus_table_alias (store_id, first_name, default_1, default_2) SELECT store_id, 'Andres'::text AS first_name, 1000, '2'::text AS default_2 FROM public.table_with_defaults_13300018 table_with_defaults WHERE (store_id IS NOT NULL) GROUP BY last_name, store_id, first_name, default_2
 RESET client_min_messages;
 -- Stable function in default should be allowed
 ALTER TABLE table_with_defaults ADD COLUMN t timestamptz DEFAULT now();
 INSERT INTO table_with_defaults (store_id, first_name, last_name)
 SELECT
   store_id, 'first '||store_id, 'last '||store_id
 FROM
   table_with_defaults
 GROUP BY
   store_id, first_name, last_name;
 -- Volatile function in default should be disallowed - SERIAL pseudo-types
 CREATE TABLE table_with_serial (
   store_id int,
   s bigserial
 );
 SELECT create_distributed_table('table_with_serial', 'store_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO table_with_serial (store_id)
 SELECT
   store_id
 FROM
   table_with_defaults
 GROUP BY
   store_id;
 -- Volatile function in default should be disallowed - user-defined sequence
 CREATE SEQUENCE user_defined_sequence;
 CREATE TABLE table_with_user_sequence (
   store_id int,
   s bigint default nextval('user_defined_sequence')
 );
 SELECT create_distributed_table('table_with_user_sequence', 'store_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO table_with_user_sequence (store_id)
 SELECT
   store_id
 FROM
   table_with_defaults
 GROUP BY
   store_id;
 -- do some more error/error message checks
 SET citus.shard_count TO 4;
 SET citus.shard_replication_factor TO 1;
@@ -2408,53 +2037,38 @@
 
 -- Select from local functions
 TRUNCATE raw_events_first;
 CREATE SEQUENCE insert_select_test_seq;
 SET client_min_messages TO DEBUG;
 INSERT INTO raw_events_first (user_id, value_1, value_2)
 SELECT
   s, nextval('insert_select_test_seq'), (random()*10)::int
 FROM
   generate_series(1, 5) s;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 SELECT user_id, value_1 FROM raw_events_first ORDER BY user_id, value_1;
-DEBUG:  Router planner cannot handle multi-shard select queries
  user_id | value_1 
 ---------+---------
        1 |       1
        2 |       2
        3 |       3
        4 |       4
        5 |       5
 (5 rows)
 
 -- ON CONFLICT is supported
 INSERT INTO raw_events_first (user_id, value_1)
 SELECT s, nextval('insert_select_test_seq') FROM generate_series(1, 5) s
 ON CONFLICT DO NOTHING;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300000 AS citus_table_alias (user_id, value_1) SELECT user_id, value_1 FROM read_intermediate_result('insert_select_XXX_13300000'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1 integer) ON CONFLICT DO NOTHING
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300001 AS citus_table_alias (user_id, value_1) SELECT user_id, value_1 FROM read_intermediate_result('insert_select_XXX_13300001'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1 integer) ON CONFLICT DO NOTHING
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300002 AS citus_table_alias (user_id, value_1) SELECT user_id, value_1 FROM read_intermediate_result('insert_select_XXX_13300002'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1 integer) ON CONFLICT DO NOTHING
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300003 AS citus_table_alias (user_id, value_1) SELECT user_id, value_1 FROM read_intermediate_result('insert_select_XXX_13300003'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1 integer) ON CONFLICT DO NOTHING
 -- RETURNING is supported
 INSERT INTO raw_events_first (user_id, value_1)
 SELECT s, nextval('insert_select_test_seq') FROM generate_series(1, 5) s
 RETURNING *;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300000 AS citus_table_alias (user_id, value_1) SELECT user_id, value_1 FROM read_intermediate_result('insert_select_XXX_13300000'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1 integer) RETURNING citus_table_alias.user_id, citus_table_alias."time", citus_table_alias.value_1, citus_table_alias.value_2, citus_table_alias.value_3, citus_table_alias.value_4
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300001 AS citus_table_alias (user_id, value_1) SELECT user_id, value_1 FROM read_intermediate_result('insert_select_XXX_13300001'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1 integer) RETURNING citus_table_alias.user_id, citus_table_alias."time", citus_table_alias.value_1, citus_table_alias.value_2, citus_table_alias.value_3, citus_table_alias.value_4
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300002 AS citus_table_alias (user_id, value_1) SELECT user_id, value_1 FROM read_intermediate_result('insert_select_XXX_13300002'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1 integer) RETURNING citus_table_alias.user_id, citus_table_alias."time", citus_table_alias.value_1, citus_table_alias.value_2, citus_table_alias.value_3, citus_table_alias.value_4
-DEBUG:  distributed statement: INSERT INTO public.raw_events_first_13300003 AS citus_table_alias (user_id, value_1) SELECT user_id, value_1 FROM read_intermediate_result('insert_select_XXX_13300003'::text, 'binary'::citus_copy_format) intermediate_result(user_id integer, value_1 integer) RETURNING citus_table_alias.user_id, citus_table_alias."time", citus_table_alias.value_1, citus_table_alias.value_2, citus_table_alias.value_3, citus_table_alias.value_4
  user_id | time | value_1 | value_2 | value_3 | value_4 
 ---------+------+---------+---------+---------+---------
        1 |      |      11 |         |         |        
        2 |      |      12 |         |         |        
        3 |      |      13 |         |         |        
        4 |      |      14 |         |         |        
        5 |      |      15 |         |         |        
 (5 rows)
 
 RESET client_min_messages;
@@ -3175,21 +2789,21 @@
 ALTER TABLE coerce_agg ALTER COLUMN value_1_agg TYPE integer USING NULL;
 ALTER TABLE coerce_agg ADD CONSTRAINT small_number CHECK (value_1_agg < 5);
 INSERT INTO coerce_events (user_id, value_1) VALUES (1, 1), (10, 10);
 \set VERBOSITY TERSE
 INSERT INTO coerce_agg(user_id, value_1_agg)
 SELECT *
 FROM (
   SELECT user_id, value_1
   FROM coerce_events
 ) AS ftop;
-ERROR:  new row for relation "coerce_agg_13300067" violates check constraint "small_number_13300067"
+ERROR:  new row for relation "coerce_agg_13300045" violates check constraint "small_number_13300045"
 \set VERBOSITY DEFAULT
 SELECT * FROM coerce_agg ORDER BY 1 DESC, 2 DESC;
  user_id | value_1_agg 
 ---------+-------------
 (0 rows)
 
 -- integer[3] -> text[3]
 TRUNCATE coerce_events;
 ALTER TABLE coerce_events ALTER COLUMN value_1 TYPE integer[3] USING NULL;
 INSERT INTO coerce_events(user_id, value_1) VALUES (1, '{1,1,1}'), (2, '{2,2,2}');
@@ -3230,21 +2844,29 @@
                         agg_time
             )
 SELECT user_id,
        value_1,
        time
 FROM   raw_events_first
 ON conflict (user_id, value_1_agg)
 DO UPDATE
    SET    user_id = 42
 RETURNING user_id, value_1_agg;
-ERROR:  modifying the partition value of rows is not allowed
+ user_id | value_1_agg 
+---------+-------------
+       1 |            
+       2 |            
+       3 |            
+       4 |            
+       5 |            
+(5 rows)
+
 -- test a small citus.remote_copy_flush_threshold
 BEGIN;
 SET LOCAL citus.remote_copy_flush_threshold TO 1;
 INSERT INTO raw_events_first
 SELECT * FROM raw_events_first OFFSET 0
 ON CONFLICT DO NOTHING;
 ABORT;
 -- test fix for issue https://github.com/citusdata/citus/issues/5891
 CREATE TABLE dist_table_1(
 dist_col integer,
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_behavioral_analytics_create_table_superuser.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_behavioral_analytics_create_table_superuser.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_behavioral_analytics_create_table_superuser.out.modified	2022-11-09 13:37:42.909312581 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_behavioral_analytics_create_table_superuser.out.modified	2022-11-09 13:37:42.919312581 +0300
@@ -196,149 +196,192 @@
 	event_time bigint
 );
 SELECT create_distributed_table('events', 'composite_id', 'range');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('events') AS new_shard_id
 \gset
+ERROR:  could only find 1 of 2 possible nodes
 UPDATE pg_dist_shard SET shardminvalue = '(1,1)', shardmaxvalue = '(1,2000000000)'
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 SELECT master_create_empty_shard('events') AS new_shard_id
 \gset
+ERROR:  could only find 1 of 2 possible nodes
 UPDATE pg_dist_shard SET shardminvalue = '(1,2000000001)', shardmaxvalue = '(1,4300000000)'
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 SELECT master_create_empty_shard('events') AS new_shard_id
 \gset
+ERROR:  could only find 1 of 2 possible nodes
 UPDATE pg_dist_shard SET shardminvalue = '(2,1)', shardmaxvalue = '(2,2000000000)'
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 SELECT master_create_empty_shard('events') AS new_shard_id
 \gset
+ERROR:  could only find 1 of 2 possible nodes
 UPDATE pg_dist_shard SET shardminvalue = '(2,2000000001)', shardmaxvalue = '(2,4300000000)'
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 COPY events FROM STDIN WITH CSV;
+ERROR:  could not find any shards into which to copy
+DETAIL:  No shards exist for distributed table "events".
+"(1,1001)",20001,click,1472807012
+"(1,1001)",20002,submit,1472807015
+"(1,1001)",20003,pay,1472807020
+"(1,1002)",20010,click,1472807022
+"(1,1002)",20011,click,1472807023
+"(1,1002)",20012,submit,1472807025
+"(1,1002)",20013,pay,1472807030
+"(1,1003)",20014,click,1472807032
+"(1,1003)",20015,click,1472807033
+"(1,1003)",20016,click,1472807034
+"(1,1003)",20017,submit,1472807035
+\.
+invalid command \.
 CREATE TABLE users (
 	composite_id user_composite_type,
 	lastseen bigint
 );
+ERROR:  syntax error at or near ""(1,1001)""
 SELECT create_distributed_table('users', 'composite_id', 'range');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  relation "users" does not exist
 -- we will guarantee co-locatedness for these tables
 UPDATE pg_dist_partition SET colocationid = 20001
 WHERE logicalrelid = 'events'::regclass OR logicalrelid = 'users'::regclass;
+ERROR:  relation "users" does not exist
 SELECT master_create_empty_shard('users') AS new_shard_id
 \gset
+ERROR:  relation "users" does not exist
 UPDATE pg_dist_shard SET shardminvalue = '(1,1)', shardmaxvalue = '(1,2000000000)'
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 SELECT master_create_empty_shard('users') AS new_shard_id
 \gset
+ERROR:  relation "users" does not exist
 UPDATE pg_dist_shard SET shardminvalue = '(1,2000000001)', shardmaxvalue = '(1,4300000000)'
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 SELECT master_create_empty_shard('users') AS new_shard_id
 \gset
+ERROR:  relation "users" does not exist
 UPDATE pg_dist_shard SET shardminvalue = '(2,1)', shardmaxvalue = '(2,2000000000)'
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 SELECT master_create_empty_shard('users') AS new_shard_id
 \gset
+ERROR:  relation "users" does not exist
 UPDATE pg_dist_shard SET shardminvalue = '(2,2000000001)', shardmaxvalue = '(2,4300000000)'
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 COPY users FROM STDIN WITH CSV;
+ERROR:  relation "users" does not exist
+"(1,1001)",1472807115
+"(1,1002)",1472807215
+"(1,1003)",1472807315
+\.
+invalid command \.
 -- Create tables for subquery tests
 CREATE TABLE lineitem_subquery (
 	l_orderkey bigint not null,
 	l_partkey integer not null,
 	l_suppkey integer not null,
 	l_linenumber integer not null,
 	l_quantity decimal(15, 2) not null,
 	l_extendedprice decimal(15, 2) not null,
 	l_discount decimal(15, 2) not null,
 	l_tax decimal(15, 2) not null,
 	l_returnflag char(1) not null,
 	l_linestatus char(1) not null,
 	l_shipdate date not null,
 	l_commitdate date not null,
 	l_receiptdate date not null,
 	l_shipinstruct char(25) not null,
 	l_shipmode char(10) not null,
 	l_comment varchar(44) not null,
 	PRIMARY KEY(l_orderkey, l_linenumber) );
+ERROR:  syntax error at or near ""(1,1001)""
 SELECT create_distributed_table('lineitem_subquery', 'l_orderkey', 'range');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  relation "lineitem_subquery" does not exist
 CREATE TABLE orders_subquery (
 	o_orderkey bigint not null,
 	o_custkey integer not null,
 	o_orderstatus char(1) not null,
 	o_totalprice decimal(15,2) not null,
 	o_orderdate date not null,
 	o_orderpriority char(15) not null,
 	o_clerk char(15) not null,
 	o_shippriority integer not null,
 	o_comment varchar(79) not null,
 	PRIMARY KEY(o_orderkey) );
 SELECT create_distributed_table('orders_subquery', 'o_orderkey', 'range');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 -- we will guarantee co-locatedness for these tabes
 UPDATE pg_dist_partition SET colocationid = 20002
 WHERE logicalrelid = 'orders_subquery'::regclass OR logicalrelid = 'lineitem_subquery'::regclass;
+ERROR:  relation "lineitem_subquery" does not exist
 SET citus.enable_router_execution TO 'false';
 -- Check that we don't crash if there are not any shards.
 SELECT
 	avg(unit_price)
 FROM
 	(SELECT
 		l_orderkey,
 		avg(o_totalprice) AS unit_price
 	FROM
 		lineitem_subquery,
 		orders_subquery
 	WHERE
 		l_orderkey = o_orderkey
 	GROUP BY
 		l_orderkey) AS unit_prices;
- avg
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  relation "lineitem_subquery" does not exist
 -- Load data into tables.
 SELECT master_create_empty_shard('lineitem_subquery') AS new_shard_id
 \gset
+ERROR:  relation "lineitem_subquery" does not exist
 UPDATE pg_dist_shard SET shardminvalue = 1, shardmaxvalue = 5986
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 SELECT master_create_empty_shard('lineitem_subquery') AS new_shard_id
 \gset
+ERROR:  relation "lineitem_subquery" does not exist
 UPDATE pg_dist_shard SET shardminvalue = 8997, shardmaxvalue = 14947
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 SELECT master_create_empty_shard('orders_subquery') AS new_shard_id
 \gset
+ERROR:  could only find 1 of 2 possible nodes
 UPDATE pg_dist_shard SET shardminvalue = 1, shardmaxvalue = 5986
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 SELECT master_create_empty_shard('orders_subquery') AS new_shard_id
 \gset
+ERROR:  could only find 1 of 2 possible nodes
 UPDATE pg_dist_shard SET shardminvalue = 8997, shardmaxvalue = 14947
 WHERE shardid = :new_shard_id;
+ERROR:  syntax error at or near ":"
 \set lineitem_1_data_file :abs_srcdir '/data/lineitem.1.data'
 \set client_side_copy_command '\\copy lineitem_subquery FROM ' :'lineitem_1_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
+ERROR:  relation "lineitem_subquery" does not exist
 \set lineitem_2_data_file :abs_srcdir '/data/lineitem.2.data'
 \set client_side_copy_command '\\copy lineitem_subquery FROM ' :'lineitem_2_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
+ERROR:  relation "lineitem_subquery" does not exist
 \set orders_1_data_file :abs_srcdir '/data/orders.1.data'
 \set client_side_copy_command '\\copy orders_subquery FROM ' :'orders_1_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
+ERROR:  could not find any shards into which to copy
+DETAIL:  No shards exist for distributed table "orders_subquery".
 \set orders_2_data_file :abs_srcdir '/data/orders.2.data'
 \set client_side_copy_command '\\copy orders_subquery FROM ' :'orders_2_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
+ERROR:  could not find any shards into which to copy
+DETAIL:  No shards exist for distributed table "orders_subquery".
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_shard_update_delete.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_shard_update_delete.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_shard_update_delete.out.modified	2022-11-09 13:37:44.079312576 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_shard_update_delete.out.modified	2022-11-09 13:37:44.089312576 +0300
@@ -833,25 +833,22 @@
  count 
 -------
      0
 (1 row)
 
 -- Do some tests by changing shard replication factor
 DROP TABLE users_test_table;
 SET citus.shard_replication_factor to 2;
 CREATE TABLE users_test_table(user_id int, value_1 int, value_2 int, value_3 int);
 SELECT create_distributed_table('users_test_table', 'user_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 \COPY users_test_table FROM STDIN DELIMITER AS ',';
 -- Run multi shard updates and deletes without transaction on hash distributed tables
 UPDATE users_test_table SET value_1 = 1;
 SELECT COUNT(*), SUM(value_1) FROM users_test_table;
  count | sum 
 -------+-----
     15 |  15
 (1 row)
 
 SELECT COUNT(*), SUM(value_2) FROM users_test_table WHERE user_id = 1 or user_id = 3;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/recursive_dml_with_different_planners_executors.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/recursive_dml_with_different_planners_executors.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/recursive_dml_with_different_planners_executors.out.modified	2022-11-09 13:37:44.169312576 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/recursive_dml_with_different_planners_executors.out.modified	2022-11-09 13:37:44.179312576 +0300
@@ -1,80 +1,66 @@
 CREATE SCHEMA recursive_dml_with_different_planner_executors;
 SET search_path TO recursive_dml_with_different_planner_executors, public;
 CREATE TABLE distributed_table (tenant_id text, dept int, info jsonb);
 SELECT create_distributed_table('distributed_table', 'tenant_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE second_distributed_table (tenant_id text, dept int, info jsonb);
 SELECT create_distributed_table('second_distributed_table', 'tenant_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE reference_table (id text, name text);
 SELECT create_reference_table('reference_table');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 INSERT INTO distributed_table SELECT i::text, i % 10, row_to_json(row(i, i*i)) FROM generate_series (0, 100) i;
 INSERT INTO second_distributed_table SELECT i::text, i % 10, row_to_json(row(i, i*i)) FROM generate_series (0, 100) i;
 SET client_min_messages TO DEBUG1;
 -- subquery with router planner
 -- joined with a real-time query
 UPDATE
 	distributed_table
 SET dept = foo.dept FROM
 	(SELECT tenant_id, dept FROM second_distributed_table WHERE dept = 1 ) as foo,
 	(SELECT tenant_id FROM second_distributed_table WHERE dept IN (1, 2, 3, 4) OFFSET 0) as bar
 	WHERE foo.tenant_id = bar.tenant_id
 	AND distributed_table.tenant_id = bar.tenant_id;
-DEBUG:  generating subplan XXX_1 for subquery SELECT tenant_id FROM recursive_dml_with_different_planner_executors.second_distributed_table WHERE (dept OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2, 3, 4])) OFFSET 0
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_with_different_planner_executors.distributed_table SET dept = foo.dept FROM (SELECT second_distributed_table.tenant_id, second_distributed_table.dept FROM recursive_dml_with_different_planner_executors.second_distributed_table WHERE (second_distributed_table.dept OPERATOR(pg_catalog.=) 1)) foo, (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text)) bar WHERE ((foo.tenant_id OPERATOR(pg_catalog.=) bar.tenant_id) AND (distributed_table.tenant_id OPERATOR(pg_catalog.=) bar.tenant_id))
 -- a non colocated subquery inside the UPDATE
 UPDATE distributed_table SET dept = foo.max_dept FROM
 (
 	SELECT
 		max(dept) as max_dept
 	FROM
 		(SELECT DISTINCT tenant_id, dept FROM distributed_table) as distributed_table
 	WHERE tenant_id NOT IN
 				(SELECT tenant_id FROM second_distributed_table WHERE dept IN (1, 2, 3, 4))
 ) as  foo WHERE foo.max_dept > dept * 3;
-DEBUG:  generating subplan XXX_1 for subquery SELECT tenant_id FROM recursive_dml_with_different_planner_executors.second_distributed_table WHERE (dept OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2, 3, 4]))
-DEBUG:  generating subplan XXX_2 for subquery SELECT max(dept) AS max_dept FROM (SELECT DISTINCT distributed_table_1.tenant_id, distributed_table_1.dept FROM recursive_dml_with_different_planner_executors.distributed_table distributed_table_1) distributed_table WHERE (NOT (tenant_id OPERATOR(pg_catalog.=) ANY (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text))))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_with_different_planner_executors.distributed_table SET dept = foo.max_dept FROM (SELECT intermediate_result.max_dept FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(max_dept integer)) foo WHERE (foo.max_dept OPERATOR(pg_catalog.>) (distributed_table.dept OPERATOR(pg_catalog.*) 3))
 -- subquery with repartition query
 SET citus.enable_repartition_joins to ON;
 UPDATE distributed_table SET dept = foo.some_tenants::int FROM
 (
 	SELECT
 	 	DISTINCT second_distributed_table.tenant_id as some_tenants
 	 FROM second_distributed_table, distributed_table WHERE second_distributed_table.dept = distributed_table.dept
 ) as foo;
-DEBUG:  generating subplan XXX_1 for subquery SELECT DISTINCT second_distributed_table.tenant_id AS some_tenants FROM recursive_dml_with_different_planner_executors.second_distributed_table, recursive_dml_with_different_planner_executors.distributed_table WHERE (second_distributed_table.dept OPERATOR(pg_catalog.=) distributed_table.dept)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_with_different_planner_executors.distributed_table SET dept = (foo.some_tenants)::integer FROM (SELECT intermediate_result.some_tenants FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(some_tenants text)) foo
 SET citus.enable_repartition_joins to OFF;
 -- final query is router
 UPDATE distributed_table SET dept = foo.max_dept FROM
 (
 	SELECT
 		max(dept) as max_dept
 	FROM
 		(SELECT DISTINCT tenant_id, dept FROM distributed_table) as distributed_table
 	WHERE tenant_id IN
 				(SELECT tenant_id FROM second_distributed_table WHERE dept IN (1, 2, 3, 4))
 ) as  foo WHERE foo.max_dept >= dept and tenant_id = '8';
-DEBUG:  generating subplan XXX_1 for subquery SELECT max(dept) AS max_dept FROM (SELECT DISTINCT distributed_table_1.tenant_id, distributed_table_1.dept FROM recursive_dml_with_different_planner_executors.distributed_table distributed_table_1) distributed_table WHERE (tenant_id OPERATOR(pg_catalog.=) ANY (SELECT second_distributed_table.tenant_id FROM recursive_dml_with_different_planner_executors.second_distributed_table WHERE (second_distributed_table.dept OPERATOR(pg_catalog.=) ANY (ARRAY[1, 2, 3, 4]))))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_with_different_planner_executors.distributed_table SET dept = foo.max_dept FROM (SELECT intermediate_result.max_dept FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(max_dept integer)) foo WHERE ((foo.max_dept OPERATOR(pg_catalog.>=) distributed_table.dept) AND (distributed_table.tenant_id OPERATOR(pg_catalog.=) '8'::text))
 RESET client_min_messages;
 DROP SCHEMA recursive_dml_with_different_planner_executors CASCADE;
-NOTICE:  drop cascades to 3 other objects
+NOTICE:  drop cascades to 4 other objects
 DETAIL:  drop cascades to table distributed_table
 drop cascades to table second_distributed_table
 drop cascades to table reference_table
+drop cascades to table reference_table_360027
 SET search_path TO public;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/insert_select_repartition_0.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/insert_select_repartition.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/insert_select_repartition_0.out.modified	2022-11-09 13:37:46.099312568 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/insert_select_repartition.out.modified	2022-11-09 13:37:46.119312568 +0300
@@ -825,21 +825,23 @@
 UPDATE source_table SET b = NULL where b IN (9, 4);
 SET citus.shard_replication_factor TO 2;
 CREATE TABLE target_table(a int, b int not null);
 SELECT create_distributed_table('target_table', 'a', 'range');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 CALL public.create_range_partitioned_shards('target_table', '{0,3,6,9}','{2,5,8,50}');
+ERROR:  could only find 1 of 2 possible nodes
 INSERT INTO target_table VALUES (11,9), (22,4);
+ERROR:  could not find any shards
 EXPLAIN (costs off) INSERT INTO target_table SELECT * FROM source_table;
                            QUERY PLAN                            
 -----------------------------------------------------------------
  Custom Scan (Citus INSERT ... SELECT)
    INSERT/SELECT method: repartition
    ->  Custom Scan (Citus Adaptive)
          Task Count: 4
          Tasks Shown: One of 4
          ->  Task
                Node: host=localhost port=57638 dbname=regression
@@ -856,145 +858,100 @@
          Tasks Shown: One of 4
          ->  Task
                Node: host=localhost port=57638 dbname=regression
                ->  Seq Scan on source_table_4213613 source_table
                      Filter: (b IS NOT NULL)
 (9 rows)
 
 BEGIN;
 SAVEPOINT s1;
 INSERT INTO target_table SELECT * FROM source_table;
-ERROR:  null value in column "b" violates not-null constraint
+ERROR:  number of partitions cannot be 0
 ROLLBACK TO SAVEPOINT s1;
 INSERT INTO target_table SELECT * FROM source_table WHERE b IS NOT NULL;
+ERROR:  number of partitions cannot be 0
 END;
 SELECT * FROM target_table ORDER BY b;
  a | b 
 ---+---
-  1 |   1
- 22 |   4
- 11 |   9
-  4 |  16
-  5 |  25
-  6 |  36
-  7 |  49
-  8 |  64
-  9 |  81
- 10 | 100
-(10 rows)
+(0 rows)
 
 -- verify that values have been replicated to both replicas
 SELECT * FROM run_command_on_placements('target_table', 'select count(*) from %s') ORDER BY shardid, nodeport;
  nodename | nodeport | shardid | success | result 
 ----------+----------+---------+---------+--------
- localhost |    57637 | 4213617 | t       | 1
- localhost |    57638 | 4213617 | t       | 1
- localhost |    57637 | 4213618 | t       | 2
- localhost |    57638 | 4213618 | t       | 2
- localhost |    57637 | 4213619 | t       | 3
- localhost |    57638 | 4213619 | t       | 3
- localhost |    57637 | 4213620 | t       | 4
- localhost |    57638 | 4213620 | t       | 4
-(8 rows)
+(0 rows)
 
 --
 -- Multiple casts in the SELECT query
 --
 TRUNCATE target_table;
 SET client_min_messages TO DEBUG2;
 INSERT INTO target_table SELECT 1.12, b::bigint FROM source_table WHERE b IS NOT NULL;
 DEBUG:  cannot perform distributed INSERT INTO ... SELECT because the partition columns in the source table and subquery do not match
 DEBUG:  Router planner cannot handle multi-shard select queries
 DEBUG:  performing repartitioned INSERT ... SELECT
 DEBUG:  partitioning SELECT query by column index 0 with name 'a'
-DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213617 AS citus_table_alias (a, b) SELECT a, b FROM read_intermediate_results('{repartitioned_results_xxxxx_from_4213613_to_0,repartitioned_results_xxxxx_from_4213614_to_0,repartitioned_results_xxxxx_from_4213615_to_0,repartitioned_results_xxxxx_from_4213616_to_0}'::text[], 'binary'::citus_copy_format) intermediate_result(a integer, b integer)
+ERROR:  number of partitions cannot be 0
 RESET client_min_messages;
 SELECT * FROM target_table ORDER BY a, b;
  a | b 
 ---+---
- 1 |   1
- 1 |  16
- 1 |  25
- 1 |  36
- 1 |  49
- 1 |  64
- 1 |  81
- 1 | 100
-(8 rows)
+(0 rows)
 
 --
 -- ROLLBACK after out of range error
 --
 TRUNCATE target_table;
 BEGIN;
 INSERT INTO target_table SELECT a * 10, b FROM source_table WHERE b IS NOT NULL;
-ERROR:  could not find shard for partition column value
+ERROR:  number of partitions cannot be 0
 END;
 SELECT max(result) FROM run_command_on_placements('target_table', 'select count(*) from %s');
  max 
 -----
- 0
+ 
 (1 row)
 
 DROP TABLE source_table, target_table;
 --
 -- Range partitioned target's ranges doesn't cover the whole range
 --
 SET citus.shard_replication_factor TO 2;
 SET citus.shard_count TO 4;
 CREATE TABLE source_table(a int, b int);
 SELECT create_distributed_table('source_table', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 INSERT INTO source_table SELECT i, i * i FROM generate_series(1, 10) i;
 SET citus.shard_replication_factor TO 2;
 CREATE TABLE target_table(b int not null, a float);
 SELECT create_distributed_table('target_table', 'a', 'range');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 CALL public.create_range_partitioned_shards('target_table', '{0.0,3.5,6.5,9.5}','{2.9,5.9,8.9,50.0}');
+ERROR:  could only find 1 of 2 possible nodes
 INSERT INTO target_table SELECT b, a+0.6 FROM source_table;
+ERROR:  could not find any shards into which to copy
 SELECT * FROM target_table ORDER BY a;
  b | a 
 ---+---
-   1 |  1.6
-   4 |  2.6
-   9 |  3.6
-  16 |  4.6
-  25 |  5.6
-  36 |  6.6
-  49 |  7.6
-  64 |  8.6
-  81 |  9.6
- 100 | 10.6
-(10 rows)
+(0 rows)
 
 -- verify that values have been replicated to both replicas, and that each
 -- replica has received correct number of rows
 SELECT * FROM run_command_on_placements('target_table', 'select count(*) from %s') ORDER BY shardid, nodeport;
  nodename | nodeport | shardid | success | result 
 ----------+----------+---------+---------+--------
- localhost |    57637 | 4213625 | t       | 2
- localhost |    57638 | 4213625 | t       | 2
- localhost |    57637 | 4213626 | t       | 3
- localhost |    57638 | 4213626 | t       | 3
- localhost |    57637 | 4213627 | t       | 3
- localhost |    57638 | 4213627 | t       | 3
- localhost |    57637 | 4213628 | t       | 2
- localhost |    57638 | 4213628 | t       | 2
-(8 rows)
+(0 rows)
 
 DROP TABLE source_table, target_table;
 --
 -- Select column names should be unique
 --
 SET citus.shard_replication_factor TO 1;
 SET citus.shard_count TO 4;
 CREATE TABLE source_table(a int, b int);
 SELECT create_distributed_table('source_table', 'a');
  create_distributed_table 
@@ -1010,23 +967,23 @@
  
 (1 row)
 
 INSERT INTO source_table SELECT i, i * i FROM generate_series(1, 10) i;
 SET client_min_messages TO DEBUG2;
 INSERT INTO target_table SELECT a AS aa, b AS aa, 1 AS aa, 2 AS aa FROM source_table;
 DEBUG:  INSERT target table and the source relation of the SELECT partition column value must be colocated in distributed INSERT ... SELECT
 DEBUG:  Router planner cannot handle multi-shard select queries
 DEBUG:  performing repartitioned INSERT ... SELECT
 DEBUG:  partitioning SELECT query by column index 0 with name 'a'
-DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213633 AS citus_table_alias (a, b, c, d) SELECT a, b, c, d FROM read_intermediate_results('{repartitioned_results_xxxxx_from_4213629_to_0,repartitioned_results_xxxxx_from_4213630_to_0}'::text[], 'binary'::citus_copy_format) intermediate_result(a integer, b integer, c integer, d integer)
-DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213634 AS citus_table_alias (a, b, c, d) SELECT a, b, c, d FROM read_intermediate_results('{repartitioned_results_xxxxx_from_4213630_to_1,repartitioned_results_xxxxx_from_4213631_to_1}'::text[], 'binary'::citus_copy_format) intermediate_result(a integer, b integer, c integer, d integer)
-DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213635 AS citus_table_alias (a, b, c, d) SELECT a, b, c, d FROM read_intermediate_results('{repartitioned_results_xxxxx_from_4213632_to_2}'::text[], 'binary'::citus_copy_format) intermediate_result(a integer, b integer, c integer, d integer)
+DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213623 AS citus_table_alias (a, b, c, d) SELECT a, b, c, d FROM read_intermediate_results('{repartitioned_results_16139681901_from_4213619_to_0,repartitioned_results_16139681901_from_4213620_to_0}'::text[], 'binary'::citus_copy_format) intermediate_result(a integer, b integer, c integer, d integer)
+DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213624 AS citus_table_alias (a, b, c, d) SELECT a, b, c, d FROM read_intermediate_results('{repartitioned_results_16139681901_from_4213620_to_1,repartitioned_results_16139681901_from_4213621_to_1}'::text[], 'binary'::citus_copy_format) intermediate_result(a integer, b integer, c integer, d integer)
+DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213625 AS citus_table_alias (a, b, c, d) SELECT a, b, c, d FROM read_intermediate_results('{repartitioned_results_16139681901_from_4213622_to_2}'::text[], 'binary'::citus_copy_format) intermediate_result(a integer, b integer, c integer, d integer)
 RESET client_min_messages;
 SELECT count(*) FROM target_table;
  count 
 -------
     10
 (1 row)
 
 --
 -- Disable repartitioned insert/select
 --
@@ -1035,21 +992,21 @@
 EXPLAIN (costs off) INSERT INTO target_table SELECT a AS aa, b AS aa, 1 AS aa, 2 AS aa FROM source_table;
                            QUERY PLAN                            
 -----------------------------------------------------------------
  Custom Scan (Citus INSERT ... SELECT)
    INSERT/SELECT method: pull to coordinator
    ->  Custom Scan (Citus Adaptive)
          Task Count: 4
          Tasks Shown: One of 4
          ->  Task
                Node: host=localhost port=57638 dbname=regression
-               ->  Seq Scan on source_table_4213629 source_table
+               ->  Seq Scan on source_table_4213619 source_table
 (8 rows)
 
 SET client_min_messages TO DEBUG2;
 INSERT INTO target_table SELECT a AS aa, b AS aa, 1 AS aa, 2 AS aa FROM source_table;
 DEBUG:  INSERT target table and the source relation of the SELECT partition column value must be colocated in distributed INSERT ... SELECT
 DEBUG:  Router planner cannot handle multi-shard select queries
 DEBUG:  Collecting INSERT ... SELECT results on coordinator
 RESET client_min_messages;
 SELECT count(*) FROM target_table;
  count 
@@ -1061,21 +1018,21 @@
 EXPLAIN (costs off) INSERT INTO target_table SELECT a AS aa, b AS aa, 1 AS aa, 2 AS aa FROM source_table;
                            QUERY PLAN                            
 -----------------------------------------------------------------
  Custom Scan (Citus INSERT ... SELECT)
    INSERT/SELECT method: repartition
    ->  Custom Scan (Citus Adaptive)
          Task Count: 4
          Tasks Shown: One of 4
          ->  Task
                Node: host=localhost port=57638 dbname=regression
-               ->  Seq Scan on source_table_4213629 source_table
+               ->  Seq Scan on source_table_4213619 source_table
 (8 rows)
 
 DROP TABLE source_table, target_table;
 --
 -- Don't use INSERT/SELECT repartition with repartition joins
 --
 create table test(x int, y int);
 select create_distributed_table('test', 'x');
  create_distributed_table 
 --------------------------
@@ -1214,22 +1171,22 @@
 FROM source_table
 GROUP BY c1, c2, c3, c4, c5, c6
 ON CONFLICT(c1, c2, c3, c4, c5, c6)
 DO UPDATE SET
  cardinality = enriched.cardinality + excluded.cardinality,
  sum = enriched.sum + excluded.sum;
 DEBUG:  INSERT target table and the source relation of the SELECT partition column value must be colocated in distributed INSERT ... SELECT
 DEBUG:  Router planner cannot handle multi-shard select queries
 DEBUG:  performing repartitioned INSERT ... SELECT
 DEBUG:  partitioning SELECT query by column index 0 with name 'c1'
-DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213639 AS enriched (c1, c2, c3, c4, c5, c6, cardinality, sum) SELECT c1, c2, c3, c4, c5, c6, cardinality, sum FROM read_intermediate_results('{repartitioned_results_xxxxx_from_4213644_to_0}'::text[], 'binary'::citus_copy_format) intermediate_result(c1 integer, c2 integer, c3 timestamp without time zone, c4 integer, c5 integer, c6 integer[], cardinality integer, sum integer) ON CONFLICT(c1, c2, c3, c4, c5, c6) DO UPDATE SET cardinality = (enriched.cardinality OPERATOR(pg_catalog.+) excluded.cardinality), sum = (enriched.sum OPERATOR(pg_catalog.+) excluded.sum)
-DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213641 AS enriched (c1, c2, c3, c4, c5, c6, cardinality, sum) SELECT c1, c2, c3, c4, c5, c6, cardinality, sum FROM read_intermediate_results('{repartitioned_results_xxxxx_from_4213645_to_2}'::text[], 'binary'::citus_copy_format) intermediate_result(c1 integer, c2 integer, c3 timestamp without time zone, c4 integer, c5 integer, c6 integer[], cardinality integer, sum integer) ON CONFLICT(c1, c2, c3, c4, c5, c6) DO UPDATE SET cardinality = (enriched.cardinality OPERATOR(pg_catalog.+) excluded.cardinality), sum = (enriched.sum OPERATOR(pg_catalog.+) excluded.sum)
+DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213629 AS enriched (c1, c2, c3, c4, c5, c6, cardinality, sum) SELECT c1, c2, c3, c4, c5, c6, cardinality, sum FROM read_intermediate_results('{repartitioned_results_140_from_4213634_to_0}'::text[], 'binary'::citus_copy_format) intermediate_result(c1 integer, c2 integer, c3 timestamp without time zone, c4 integer, c5 integer, c6 integer[], cardinality integer, sum integer) ON CONFLICT(c1, c2, c3, c4, c5, c6) DO UPDATE SET cardinality = (enriched.cardinality OPERATOR(pg_catalog.+) excluded.cardinality), sum = (enriched.sum OPERATOR(pg_catalog.+) excluded.sum)
+DEBUG:  distributed statement: INSERT INTO insert_select_repartition.target_table_4213631 AS enriched (c1, c2, c3, c4, c5, c6, cardinality, sum) SELECT c1, c2, c3, c4, c5, c6, cardinality, sum FROM read_intermediate_results('{repartitioned_results_140_from_4213635_to_2}'::text[], 'binary'::citus_copy_format) intermediate_result(c1 integer, c2 integer, c3 timestamp without time zone, c4 integer, c5 integer, c6 integer[], cardinality integer, sum integer) ON CONFLICT(c1, c2, c3, c4, c5, c6) DO UPDATE SET cardinality = (enriched.cardinality OPERATOR(pg_catalog.+) excluded.cardinality), sum = (enriched.sum OPERATOR(pg_catalog.+) excluded.sum)
 RESET client_min_messages;
 EXPLAIN (COSTS OFF) INSERT INTO target_table AS enriched(c1, c2, c3, c4, c5, c6, cardinality, sum)
 SELECT c1, c2, c3, c4, -1::float AS c5,
        dist_func(c1, 4) c6,
        sum(cardinality),
        sum(sum)
 FROM source_table
 GROUP BY c1, c2, c3, c4, c5, c6
 ON CONFLICT(c1, c2, c3, c4, c5, c6)
 DO UPDATE SET
@@ -1239,21 +1196,21 @@
 -------------------------------------------------------------------------------------------------------------------
  Custom Scan (Citus INSERT ... SELECT)
    INSERT/SELECT method: repartition
    ->  Custom Scan (Citus Adaptive)
          Task Count: 4
          Tasks Shown: One of 4
          ->  Task
                Node: host=localhost port=57638 dbname=regression
                ->  HashAggregate
                      Group Key: c1, c2, c3, c4, '-1'::double precision, insert_select_repartition.dist_func(c1, 4)
-                     ->  Seq Scan on source_table_4213644 source_table
+                     ->  Seq Scan on source_table_4213634 source_table
 (10 rows)
 
 -- verify that we don't report repartitioned insert/select for tables
 -- with sequences. See https://github.com/citusdata/citus/issues/3936
 create table table_with_sequences (x int, y int, z bigserial);
 insert into table_with_sequences values (1,1);
 select create_distributed_table('table_with_sequences','x');
 NOTICE:  Copying data from local table...
 NOTICE:  copying the data has completed
  create_distributed_table 
@@ -1264,21 +1221,21 @@
 explain (costs off) insert into table_with_sequences select y, x from table_with_sequences;
                                    QUERY PLAN                                    
 ---------------------------------------------------------------------------------
  Custom Scan (Citus INSERT ... SELECT)
    INSERT/SELECT method: pull to coordinator
    ->  Custom Scan (Citus Adaptive)
          Task Count: 4
          Tasks Shown: One of 4
          ->  Task
                Node: host=localhost port=57638 dbname=regression
-               ->  Seq Scan on table_with_sequences_4213648 table_with_sequences
+               ->  Seq Scan on table_with_sequences_4213638 table_with_sequences
 (8 rows)
 
 -- verify that we don't report repartitioned insert/select for tables
 -- with user-defined sequences.
 CREATE SEQUENCE user_defined_sequence;
 create table table_with_user_sequences (x int, y int, z bigint default nextval('user_defined_sequence'));
 insert into table_with_user_sequences values (1,1);
 select create_distributed_table('table_with_user_sequences','x');
 NOTICE:  Copying data from local table...
 NOTICE:  copying the data has completed
@@ -1290,16 +1247,16 @@
 explain (costs off) insert into table_with_user_sequences select y, x from table_with_user_sequences;
                                         QUERY PLAN                                         
 -------------------------------------------------------------------------------------------
  Custom Scan (Citus INSERT ... SELECT)
    INSERT/SELECT method: pull to coordinator
    ->  Custom Scan (Citus Adaptive)
          Task Count: 4
          Tasks Shown: One of 4
          ->  Task
                Node: host=localhost port=57638 dbname=regression
-               ->  Seq Scan on table_with_user_sequences_4213652 table_with_user_sequences
+               ->  Seq Scan on table_with_user_sequences_4213642 table_with_user_sequences
 (8 rows)
 
 -- clean-up
 SET client_min_messages TO WARNING;
 DROP SCHEMA insert_select_repartition CASCADE;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/window_functions.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/window_functions.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/window_functions.out.modified	2022-11-09 13:37:46.309312567 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/window_functions.out.modified	2022-11-09 13:37:46.319312567 +0300
@@ -1524,60 +1524,49 @@
  value_2 | c | sum 
 ---------+---+-----
        3 | 3 | 108
        4 | 4 |  56
 (2 rows)
 
 -- There was a strange bug where this wouldn't have window functions being pushed down
 -- Bug dependent on column ordering
 CREATE TABLE daily_uniques (value_2 float, user_id bigint);
 SELECT create_distributed_table('daily_uniques', 'user_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 EXPLAIN (COSTS FALSE) SELECT
   user_id,
   sum(value_2) AS commits,
   RANK () OVER (
     PARTITION BY user_id
     ORDER BY
       sum(value_2) DESC
   )
 FROM daily_uniques
 GROUP BY user_id
 HAVING
   sum(value_2) > 0
 ORDER BY commits DESC
 LIMIT 10;
                                 QUERY PLAN                                
 --------------------------------------------------------------------------
  Limit
    ->  Sort
-         Sort Key: remote_scan.commits DESC
-         ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-               Tasks Shown: One of 4
-               ->  Task
-                     Node: host=localhost port=xxxxx dbname=regression
-                     ->  Limit
-                           ->  Sort
-                                 Sort Key: (sum(daily_uniques.value_2)) DESC
+         Sort Key: (sum(value_2)) DESC
          ->  WindowAgg
                ->  Sort
-                                             Sort Key: daily_uniques.user_id, (sum(daily_uniques.value_2)) DESC
+                     Sort Key: user_id, (sum(value_2)) DESC
                      ->  HashAggregate
-                                                   Group Key: daily_uniques.user_id
-                                                   Filter: (sum(daily_uniques.value_2) > '0'::double precision)
-                                                   ->  Seq Scan on daily_uniques_xxxxxxx daily_uniques
-(18 rows)
+                           Group Key: user_id
+                           Filter: (sum(value_2) > '0'::double precision)
+                           ->  Seq Scan on daily_uniques
+(10 rows)
 
 DROP TABLE daily_uniques;
 -- Partition by reference table column joined to distribution column
 SELECT DISTINCT value_2, array_agg(rnk ORDER BY rnk) FROM (
 SELECT events_table.value_2, sum(uref.k_no) OVER (PARTITION BY uref.id) AS rnk
 FROM events_table
 JOIN users_ref_test_table uref ON uref.id = events_table.user_id) sq
 GROUP BY 1 ORDER BY 1;
  value_2 |                                                              array_agg                                                              
 ---------+-------------------------------------------------------------------------------------------------------------------------------------
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/dml_recursive.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/dml_recursive.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/dml_recursive.out.modified	2022-11-09 13:37:46.429312567 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/dml_recursive.out.modified	2022-11-09 13:37:46.439312567 +0300
@@ -1,71 +1,62 @@
 CREATE SCHEMA recursive_dml_queries;
 SET search_path TO recursive_dml_queries, public;
 SET citus.next_shard_id TO 2370000;
 CREATE TABLE recursive_dml_queries.distributed_table (tenant_id text, dept int, info jsonb);
 SELECT create_distributed_table('distributed_table', 'tenant_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE recursive_dml_queries.second_distributed_table (tenant_id text, dept int, info jsonb);
 SELECT create_distributed_table('second_distributed_table', 'tenant_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE recursive_dml_queries.reference_table (id text, name text);
 SELECT create_reference_table('reference_table');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 CREATE TABLE recursive_dml_queries.local_table (id text, name text);
 INSERT INTO distributed_table SELECT i::text, i % 10, row_to_json(row(i, i*i)) FROM generate_series (0, 100) i;
 INSERT INTO second_distributed_table SELECT i::text, i % 10, row_to_json(row(i, i*i)) FROM generate_series (0, 100) i;
 INSERT INTO reference_table SELECT i::text, 'user_' || i FROM generate_series (0, 100) i;
 INSERT INTO local_table SELECT i::text, 'user_' || i FROM generate_series (0, 100) i;
 CREATE VIEW tenant_ids AS
 	SELECT
 		tenant_id, name
 	FROM
 		distributed_table, reference_table
 	WHERE
 		distributed_table.dept::text = reference_table.id
 	ORDER BY 2 DESC, 1 DESC;
+WARNING:  "view tenant_ids" has dependency to "table distributed_table" that is not in Citus' metadata
+DETAIL:  "view tenant_ids" will be created only locally
+HINT:  Distribute "table distributed_table" first to distribute "view tenant_ids"
 SET client_min_messages TO DEBUG1;
 -- the subquery foo is recursively planned
 UPDATE
 	reference_table
 SET
 	name = 'new_' || name
 FROM
 (
 	SELECT
 		avg(second_distributed_table.tenant_id::int) as avg_tenant_id
 	FROM
 		second_distributed_table
 ) as foo
 WHERE
 	foo.avg_tenant_id::int::text = reference_table.id
 RETURNING
 	reference_table.name;
-DEBUG:  generating subplan XXX_1 for subquery SELECT avg((tenant_id)::integer) AS avg_tenant_id FROM recursive_dml_queries.second_distributed_table
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_queries.reference_table SET name = ('new_'::text OPERATOR(pg_catalog.||) reference_table.name) FROM (SELECT intermediate_result.avg_tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(avg_tenant_id numeric)) foo WHERE (((foo.avg_tenant_id)::integer)::text OPERATOR(pg_catalog.=) reference_table.id) RETURNING reference_table.name
-    name
----------------------------------------------------------------------
- new_user_50
-(1 row)
-
+ERROR:  relation second_distributed_table is not distributed
 -- the subquery foo is recursively planned
 -- but note that the subquery foo itself is pushdownable
 UPDATE
 	second_distributed_table
 SET
 	dept = foo.max_dept * 2
 FROM
 (
 	SELECT DISTINCT ON (tenant_id) tenant_id, max(dept) as max_dept FROM
 	(
@@ -78,26 +69,24 @@
 	) foo_inner
 	GROUP BY
 		tenant_id
 	ORDER BY 1 DESC
 ) as foo
 WHERE
 	foo.tenant_id != second_distributed_table.tenant_id
 	AND second_distributed_table.dept IN (2)
 RETURNING
 	second_distributed_table.tenant_id, second_distributed_table.dept;
-DEBUG:  generating subplan XXX_1 for subquery SELECT DISTINCT ON (tenant_id) tenant_id, max(dept) AS max_dept FROM (SELECT second_distributed_table.dept, second_distributed_table.tenant_id FROM recursive_dml_queries.second_distributed_table, recursive_dml_queries.distributed_table WHERE (distributed_table.tenant_id OPERATOR(pg_catalog.=) second_distributed_table.tenant_id)) foo_inner GROUP BY tenant_id ORDER BY tenant_id DESC
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_queries.second_distributed_table SET dept = (foo.max_dept OPERATOR(pg_catalog.*) 2) FROM (SELECT intermediate_result.tenant_id, intermediate_result.max_dept FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text, max_dept integer)) foo WHERE ((foo.tenant_id OPERATOR(pg_catalog.<>) second_distributed_table.tenant_id) AND (second_distributed_table.dept OPERATOR(pg_catalog.=) 2)) RETURNING second_distributed_table.tenant_id, second_distributed_table.dept
  tenant_id | dept 
 -----------+------
- 12        |   18
  2         |   18
+ 12        |   18
  22        |   18
  32        |   18
  42        |   18
  52        |   18
  62        |   18
  72        |   18
  82        |   18
  92        |   18
 (10 rows)
 
@@ -128,41 +117,36 @@
 		WHERE
 			distributed_table.tenant_id = second_distributed_table.tenant_id
 		AND
 			second_distributed_table.dept IN (4,5)
 	)foo_inner_2
 	WHERE foo_inner_1.tenant_id != foo_inner_2.tenant_id
 ) as foo
 WHERE
 	foo.tenant_id != second_distributed_table.tenant_id
 	AND second_distributed_table.dept IN (3);
-DEBUG:  generating subplan XXX_1 for subquery SELECT second_distributed_table.tenant_id FROM recursive_dml_queries.second_distributed_table, recursive_dml_queries.distributed_table WHERE ((distributed_table.tenant_id OPERATOR(pg_catalog.=) second_distributed_table.tenant_id) AND (second_distributed_table.dept OPERATOR(pg_catalog.=) ANY (ARRAY[4, 5])))
-DEBUG:  generating subplan XXX_2 for subquery SELECT DISTINCT foo_inner_1.tenant_id FROM (SELECT second_distributed_table.dept, second_distributed_table.tenant_id FROM recursive_dml_queries.second_distributed_table, recursive_dml_queries.distributed_table WHERE ((distributed_table.tenant_id OPERATOR(pg_catalog.=) second_distributed_table.tenant_id) AND (second_distributed_table.dept OPERATOR(pg_catalog.=) ANY (ARRAY[3, 4])))) foo_inner_1, (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text)) foo_inner_2 WHERE (foo_inner_1.tenant_id OPERATOR(pg_catalog.<>) foo_inner_2.tenant_id)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_queries.second_distributed_table SET dept = ((foo.tenant_id)::integer OPERATOR(pg_catalog./) 4) FROM (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text)) foo WHERE ((foo.tenant_id OPERATOR(pg_catalog.<>) second_distributed_table.tenant_id) AND (second_distributed_table.dept OPERATOR(pg_catalog.=) 3))
 -- we currently do not allow local tables in modification queries
 UPDATE
 	distributed_table
 SET
 	dept = avg_tenant_id::int
 FROM
 (
 	SELECT
 		avg(local_table.id::int) as avg_tenant_id
 	FROM
 		local_table
 ) as foo
 WHERE
 	foo.avg_tenant_id::int::text = distributed_table.tenant_id
 RETURNING
 	distributed_table.*;
-DEBUG:  generating subplan XXX_1 for subquery SELECT avg((id)::integer) AS avg_tenant_id FROM recursive_dml_queries.local_table
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_queries.distributed_table SET dept = (foo.avg_tenant_id)::integer FROM (SELECT intermediate_result.avg_tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(avg_tenant_id numeric)) foo WHERE (((foo.avg_tenant_id)::integer)::text OPERATOR(pg_catalog.=) distributed_table.tenant_id) RETURNING distributed_table.tenant_id, distributed_table.dept, distributed_table.info
  tenant_id | dept |          info          
 -----------+------+------------------------
  50        |   50 | {"f1": 50, "f2": 2500}
 (1 row)
 
 -- we currently do not allow views in modification queries
 UPDATE
 	distributed_table
 SET
 	dept = avg_tenant_id::int
@@ -170,22 +154,24 @@
 (
 	SELECT
 		avg(tenant_id::int) as avg_tenant_id
 	FROM
 		tenant_ids
 ) as foo
 WHERE
 	foo.avg_tenant_id::int::text = distributed_table.tenant_id
 RETURNING
 	distributed_table.*;
-DEBUG:  generating subplan XXX_1 for subquery SELECT avg((tenant_id)::integer) AS avg_tenant_id FROM (SELECT distributed_table.tenant_id, reference_table.name FROM recursive_dml_queries.distributed_table, recursive_dml_queries.reference_table WHERE ((distributed_table.dept)::text OPERATOR(pg_catalog.=) reference_table.id) ORDER BY reference_table.name DESC, distributed_table.tenant_id DESC) tenant_ids
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_queries.distributed_table SET dept = (foo.avg_tenant_id)::integer FROM (SELECT intermediate_result.avg_tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(avg_tenant_id numeric)) foo WHERE (((foo.avg_tenant_id)::integer)::text OPERATOR(pg_catalog.=) distributed_table.tenant_id) RETURNING distributed_table.tenant_id, distributed_table.dept, distributed_table.info
+DEBUG:  Wrapping relation "distributed_table" to a subquery
+DEBUG:  generating subplan 3_1 for subquery SELECT tenant_id, dept FROM recursive_dml_queries.distributed_table WHERE true
+DEBUG:  generating subplan 3_2 for subquery SELECT avg((tenant_id)::integer) AS avg_tenant_id FROM (SELECT distributed_table.tenant_id, reference_table.name FROM (SELECT distributed_table_1.tenant_id, distributed_table_1.dept, NULL::jsonb AS info FROM (SELECT intermediate_result.tenant_id, intermediate_result.dept FROM read_intermediate_result('3_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text, dept integer)) distributed_table_1) distributed_table, recursive_dml_queries.reference_table WHERE ((distributed_table.dept)::text OPERATOR(pg_catalog.=) reference_table.id) ORDER BY reference_table.name DESC, distributed_table.tenant_id DESC) tenant_ids
+DEBUG:  Plan 3 query after replacing subqueries and CTEs: UPDATE recursive_dml_queries.distributed_table SET dept = (foo.avg_tenant_id)::integer FROM (SELECT intermediate_result.avg_tenant_id FROM read_intermediate_result('3_2'::text, 'binary'::citus_copy_format) intermediate_result(avg_tenant_id numeric)) foo WHERE (((foo.avg_tenant_id)::integer)::text OPERATOR(pg_catalog.=) distributed_table.tenant_id) RETURNING distributed_table.tenant_id, distributed_table.dept, distributed_table.info
  tenant_id | dept |          info          
 -----------+------+------------------------
  50        |   50 | {"f1": 50, "f2": 2500}
 (1 row)
 
 -- there is a lateral join (e.g., correlated subquery) thus the subqueries cannot be
 -- recursively planned, however it can be planned using the repartition planner
 SET citus.enable_repartition_joins to on;
 SELECT DISTINCT foo_inner_1.tenant_id FROM
 (
@@ -254,22 +240,125 @@
 			second_distributed_table, distributed_table
 		WHERE
 			distributed_table.tenant_id = second_distributed_table.tenant_id
 			AND foo_inner_1.dept = second_distributed_table.dept
 		AND
 			second_distributed_table.dept IN (4,5)
 	) foo_inner_2
 	ON (foo_inner_2.tenant_id != foo_inner_1.tenant_id)
 	) as foo
 RETURNING *;
-DEBUG:  generating subplan XXX_1 for subquery SELECT dept FROM recursive_dml_queries.second_distributed_table
-ERROR:  complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
+ tenant_id | dept |           info           | tenant_id 
+-----------+------+--------------------------+-----------
+ 0         |   18 | {"f1": 0, "f2": 0}       | 75
+ 1         |   18 | {"f1": 1, "f2": 1}       | 75
+ 4         |   18 | {"f1": 4, "f2": 16}      | 75
+ 5         |   18 | {"f1": 5, "f2": 25}      | 75
+ 6         |   18 | {"f1": 6, "f2": 36}      | 75
+ 7         |   18 | {"f1": 7, "f2": 49}      | 75
+ 8         |   18 | {"f1": 8, "f2": 64}      | 75
+ 9         |   18 | {"f1": 9, "f2": 81}      | 75
+ 10        |   18 | {"f1": 10, "f2": 100}    | 75
+ 11        |   18 | {"f1": 11, "f2": 121}    | 75
+ 14        |   18 | {"f1": 14, "f2": 196}    | 75
+ 15        |   18 | {"f1": 15, "f2": 225}    | 75
+ 16        |   18 | {"f1": 16, "f2": 256}    | 75
+ 17        |   18 | {"f1": 17, "f2": 289}    | 75
+ 18        |   18 | {"f1": 18, "f2": 324}    | 75
+ 19        |   18 | {"f1": 19, "f2": 361}    | 75
+ 20        |   18 | {"f1": 20, "f2": 400}    | 75
+ 21        |   18 | {"f1": 21, "f2": 441}    | 75
+ 24        |   18 | {"f1": 24, "f2": 576}    | 75
+ 25        |   18 | {"f1": 25, "f2": 625}    | 75
+ 26        |   18 | {"f1": 26, "f2": 676}    | 75
+ 27        |   18 | {"f1": 27, "f2": 729}    | 75
+ 28        |   18 | {"f1": 28, "f2": 784}    | 75
+ 29        |   18 | {"f1": 29, "f2": 841}    | 75
+ 30        |   18 | {"f1": 30, "f2": 900}    | 75
+ 31        |   18 | {"f1": 31, "f2": 961}    | 75
+ 34        |   18 | {"f1": 34, "f2": 1156}   | 75
+ 35        |   18 | {"f1": 35, "f2": 1225}   | 75
+ 36        |   18 | {"f1": 36, "f2": 1296}   | 75
+ 37        |   18 | {"f1": 37, "f2": 1369}   | 75
+ 38        |   18 | {"f1": 38, "f2": 1444}   | 75
+ 39        |   18 | {"f1": 39, "f2": 1521}   | 75
+ 40        |   18 | {"f1": 40, "f2": 1600}   | 75
+ 41        |   18 | {"f1": 41, "f2": 1681}   | 75
+ 44        |   18 | {"f1": 44, "f2": 1936}   | 75
+ 45        |   18 | {"f1": 45, "f2": 2025}   | 75
+ 46        |   18 | {"f1": 46, "f2": 2116}   | 75
+ 47        |   18 | {"f1": 47, "f2": 2209}   | 75
+ 48        |   18 | {"f1": 48, "f2": 2304}   | 75
+ 49        |   18 | {"f1": 49, "f2": 2401}   | 75
+ 50        |   18 | {"f1": 50, "f2": 2500}   | 75
+ 51        |   18 | {"f1": 51, "f2": 2601}   | 75
+ 54        |   18 | {"f1": 54, "f2": 2916}   | 75
+ 55        |   18 | {"f1": 55, "f2": 3025}   | 75
+ 56        |   18 | {"f1": 56, "f2": 3136}   | 75
+ 57        |   18 | {"f1": 57, "f2": 3249}   | 75
+ 58        |   18 | {"f1": 58, "f2": 3364}   | 75
+ 59        |   18 | {"f1": 59, "f2": 3481}   | 75
+ 60        |   18 | {"f1": 60, "f2": 3600}   | 75
+ 61        |   18 | {"f1": 61, "f2": 3721}   | 75
+ 64        |   18 | {"f1": 64, "f2": 4096}   | 75
+ 65        |   18 | {"f1": 65, "f2": 4225}   | 75
+ 66        |   18 | {"f1": 66, "f2": 4356}   | 75
+ 67        |   18 | {"f1": 67, "f2": 4489}   | 75
+ 68        |   18 | {"f1": 68, "f2": 4624}   | 75
+ 69        |   18 | {"f1": 69, "f2": 4761}   | 75
+ 70        |   18 | {"f1": 70, "f2": 4900}   | 75
+ 71        |   18 | {"f1": 71, "f2": 5041}   | 75
+ 74        |   18 | {"f1": 74, "f2": 5476}   | 75
+ 75        |   18 | {"f1": 75, "f2": 5625}   | 75
+ 76        |   18 | {"f1": 76, "f2": 5776}   | 75
+ 77        |   18 | {"f1": 77, "f2": 5929}   | 75
+ 78        |   18 | {"f1": 78, "f2": 6084}   | 75
+ 79        |   18 | {"f1": 79, "f2": 6241}   | 75
+ 80        |   18 | {"f1": 80, "f2": 6400}   | 75
+ 81        |   18 | {"f1": 81, "f2": 6561}   | 75
+ 84        |   18 | {"f1": 84, "f2": 7056}   | 75
+ 85        |   18 | {"f1": 85, "f2": 7225}   | 75
+ 86        |   18 | {"f1": 86, "f2": 7396}   | 75
+ 87        |   18 | {"f1": 87, "f2": 7569}   | 75
+ 88        |   18 | {"f1": 88, "f2": 7744}   | 75
+ 89        |   18 | {"f1": 89, "f2": 7921}   | 75
+ 90        |   18 | {"f1": 90, "f2": 8100}   | 75
+ 91        |   18 | {"f1": 91, "f2": 8281}   | 75
+ 94        |   18 | {"f1": 94, "f2": 8836}   | 75
+ 95        |   18 | {"f1": 95, "f2": 9025}   | 75
+ 96        |   18 | {"f1": 96, "f2": 9216}   | 75
+ 97        |   18 | {"f1": 97, "f2": 9409}   | 75
+ 98        |   18 | {"f1": 98, "f2": 9604}   | 75
+ 99        |   18 | {"f1": 99, "f2": 9801}   | 75
+ 100       |   18 | {"f1": 100, "f2": 10000} | 75
+ 2         |   18 | {"f1": 2, "f2": 4}       | 75
+ 12        |   18 | {"f1": 12, "f2": 144}    | 75
+ 22        |   18 | {"f1": 22, "f2": 484}    | 75
+ 32        |   18 | {"f1": 32, "f2": 1024}   | 75
+ 42        |   18 | {"f1": 42, "f2": 1764}   | 75
+ 52        |   18 | {"f1": 52, "f2": 2704}   | 75
+ 62        |   18 | {"f1": 62, "f2": 3844}   | 75
+ 72        |   18 | {"f1": 72, "f2": 5184}   | 75
+ 82        |   18 | {"f1": 82, "f2": 6724}   | 75
+ 92        |   18 | {"f1": 92, "f2": 8464}   | 75
+ 3         |   18 | {"f1": 3, "f2": 9}       | 75
+ 13        |   18 | {"f1": 13, "f2": 169}    | 75
+ 23        |   18 | {"f1": 23, "f2": 529}    | 75
+ 33        |   18 | {"f1": 33, "f2": 1089}   | 75
+ 43        |   18 | {"f1": 43, "f2": 1849}   | 75
+ 53        |   18 | {"f1": 53, "f2": 2809}   | 75
+ 63        |   18 | {"f1": 63, "f2": 3969}   | 75
+ 73        |   18 | {"f1": 73, "f2": 5329}   | 75
+ 83        |   18 | {"f1": 83, "f2": 6889}   | 75
+ 93        |   18 | {"f1": 93, "f2": 8649}   | 75
+(101 rows)
+
 -- again a correlated subquery
 -- this time distribution key eq. exists
 -- however recursive planning is prevented due to correlated subqueries
 UPDATE
 	second_distributed_table
 SET
 	dept = foo.tenant_id::int / 4
 FROM
 (
 	SELECT baz.tenant_id FROM
@@ -285,81 +374,71 @@
 			AND
 			second_distributed_table.tenant_id IN
 			(
 					SELECT s2.tenant_id
 					FROM second_distributed_table as s2
 					GROUP BY d1.tenant_id, s2.tenant_id
 			)
 	) as baz
 	) as foo WHERE second_distributed_table.tenant_id = foo.tenant_id
 RETURNING *;
-ERROR:  complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
+ tenant_id | dept | info | tenant_id 
+-----------+------+------+-----------
+(0 rows)
+
 -- we don't support subqueries/CTEs inside VALUES
 INSERT INTO
 	second_distributed_table (tenant_id, dept)
 VALUES ('3', (WITH  vals AS (SELECT 3) select * from vals));
-DEBUG:  CTE vals is going to be inlined via distributed planning
-DEBUG:  generating subplan XXX_1 for CTE vals: SELECT 3
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: INSERT INTO recursive_dml_queries.second_distributed_table (tenant_id, dept) VALUES ('3'::text, (SELECT vals."?column?" FROM (SELECT intermediate_result."?column?" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("?column?" integer)) vals("?column?")))
-ERROR:  subqueries are not supported within INSERT queries
-HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
 INSERT INTO
 	second_distributed_table (tenant_id, dept)
 VALUES ('3', (SELECT 3));
-ERROR:  subqueries are not supported within INSERT queries
-HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
 -- DML with an unreferenced SELECT CTE
 WITH cte_1 AS (
     WITH cte_2 AS (
         SELECT tenant_id as cte2_id
         FROM second_distributed_table
         WHERE dept >= 2
     )
     UPDATE distributed_table
     SET dept = 10
     RETURNING *
 )
 UPDATE distributed_table
 SET dept = 5
 FROM cte_1
 WHERE distributed_table.tenant_id < cte_1.tenant_id;
-DEBUG:  generating subplan XXX_1 for CTE cte_1: WITH cte_2 AS (SELECT second_distributed_table.tenant_id AS cte2_id FROM recursive_dml_queries.second_distributed_table WHERE (second_distributed_table.dept OPERATOR(pg_catalog.>=) 2)) UPDATE recursive_dml_queries.distributed_table SET dept = 10 RETURNING tenant_id, dept, info
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_queries.distributed_table SET dept = 5 FROM (SELECT intermediate_result.tenant_id, intermediate_result.dept, intermediate_result.info FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text, dept integer, info jsonb)) cte_1 WHERE (distributed_table.tenant_id OPERATOR(pg_catalog.<) cte_1.tenant_id)
 WITH cte_1 AS (
     WITH cte_2 AS (
         SELECT tenant_id as cte2_id
         FROM second_distributed_table
         WHERE dept >= 2
     )
     UPDATE distributed_table
     SET dept = 10
     RETURNING *
 )
 UPDATE distributed_table
 SET dept = 5
 FROM cte_1
 WHERE distributed_table.tenant_id < cte_1.tenant_id;
-DEBUG:  generating subplan XXX_1 for CTE cte_1: WITH cte_2 AS (SELECT second_distributed_table.tenant_id AS cte2_id FROM recursive_dml_queries.second_distributed_table WHERE (second_distributed_table.dept OPERATOR(pg_catalog.>=) 2)) UPDATE recursive_dml_queries.distributed_table SET dept = 10 RETURNING tenant_id, dept, info
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_queries.distributed_table SET dept = 5 FROM (SELECT intermediate_result.tenant_id, intermediate_result.dept, intermediate_result.info FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text, dept integer, info jsonb)) cte_1 WHERE (distributed_table.tenant_id OPERATOR(pg_catalog.<) cte_1.tenant_id)
 -- we support updating local table with a join with
 -- distributed tables, though as the local table
 -- is target here, distributed table is recursively
 -- planned
 UPDATE
 	local_table
 SET
 	id = 'citus_test'
 FROM
 	distributed_table
 WHERE
 	distributed_table.tenant_id = local_table.id;
-DEBUG:  Wrapping relation "distributed_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT tenant_id FROM recursive_dml_queries.distributed_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE recursive_dml_queries.local_table SET id = 'citus_test'::text FROM (SELECT distributed_table_1.tenant_id, NULL::integer AS dept, NULL::jsonb AS info FROM (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text)) distributed_table_1) distributed_table WHERE (distributed_table.tenant_id OPERATOR(pg_catalog.=) local_table.id)
 RESET client_min_messages;
 DROP SCHEMA recursive_dml_queries CASCADE;
-NOTICE:  drop cascades to 5 other objects
+NOTICE:  drop cascades to 6 other objects
 DETAIL:  drop cascades to table distributed_table
 drop cascades to table second_distributed_table
 drop cascades to table reference_table
+drop cascades to table reference_table_2370000
 drop cascades to table local_table
 drop cascades to view tenant_ids
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_insert_select_conflict_0.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_insert_select_conflict.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_insert_select_conflict_0.out.modified	2022-11-09 13:37:47.179312564 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_insert_select_conflict.out.modified	2022-11-09 13:37:47.189312564 +0300
@@ -441,78 +441,59 @@
   1 | {"abc       ","def       ","gyx       "}
   2 | {"xyz       ","tvu       "}
 (2 rows)
 
 RESET client_min_messages;
 -- Test with shard_replication_factor = 2
 SET citus.shard_replication_factor to 2;
 DROP TABLE target_table, source_table_1, source_table_2;
 CREATE TABLE target_table(col_1 int primary key, col_2 int);
 SELECT create_distributed_table('target_table','col_1');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO target_table VALUES(1,2),(2,3),(3,4),(4,5),(5,6);
 CREATE TABLE source_table_1(col_1 int, col_2 int, col_3 int);
 SELECT create_distributed_table('source_table_1','col_1');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO source_table_1 VALUES(1,1,1),(2,2,2),(3,3,3),(4,4,4),(5,5,5);
 CREATE TABLE source_table_2(col_1 int, col_2 int, col_3 int);
 SELECT create_distributed_table('source_table_2','col_1');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO source_table_2 VALUES(6,6,6),(7,7,7),(8,8,8),(9,9,9),(10,10,10);
 SET client_min_messages to debug1;
 -- Generate series directly on the coordinator and on conflict do nothing
 INSERT INTO target_table (col_1, col_2)
 SELECT
 	s, s
 FROM
 	generate_series(1,10) s
 ON CONFLICT DO NOTHING;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 -- Test with multiple subqueries
 INSERT INTO target_table
 SELECT
 	col_1, col_2
 FROM (
 	(SELECT
 		col_1, col_2, col_3
 	FROM
 		source_table_1
 	LIMIT 5)
 	UNION
 	(SELECT
 		col_1, col_2, col_3
 	FROM
 		source_table_2
 	LIMIT 5)
 ) as foo
 ON CONFLICT(col_1) DO UPDATE SET col_2 = 0;
-DEBUG:  Set operations are not allowed in distributed INSERT ... SELECT queries
-DEBUG:  push down of limit count: 5
-DEBUG:  generating subplan XXX_1 for subquery SELECT col_1, col_2, col_3 FROM on_conflict.source_table_1 LIMIT 5
-DEBUG:  push down of limit count: 5
-DEBUG:  generating subplan XXX_2 for subquery SELECT col_1, col_2, col_3 FROM on_conflict.source_table_2 LIMIT 5
-DEBUG:  generating subplan XXX_3 for subquery SELECT intermediate_result.col_1, intermediate_result.col_2, intermediate_result.col_3 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(col_1 integer, col_2 integer, col_3 integer) UNION SELECT intermediate_result.col_1, intermediate_result.col_2, intermediate_result.col_3 FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(col_1 integer, col_2 integer, col_3 integer)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT col_1, col_2 FROM (SELECT intermediate_result.col_1, intermediate_result.col_2, intermediate_result.col_3 FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(col_1 integer, col_2 integer, col_3 integer)) foo
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 SELECT * FROM target_table ORDER BY 1;
  col_1 | col_2 
 -------+-------
      1 |     0
      2 |     0
      3 |     0
      4 |     0
      5 |     0
      6 |     0
      7 |     0
@@ -520,25 +501,20 @@
      9 |     0
     10 |     0
 (10 rows)
 
 WITH cte AS MATERIALIZED(
 	SELECT col_1, col_2, col_3 FROM source_table_1
 ), cte_2 AS MATERIALIZED(
 	SELECT col_1, col_2 FROM cte
 )
 INSERT INTO target_table SELECT * FROM cte_2 ON CONFLICT(col_1) DO UPDATE SET col_2 = EXCLUDED.col_2 + 1;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  generating subplan XXX_1 for CTE cte: SELECT col_1, col_2, col_3 FROM on_conflict.source_table_1
-DEBUG:  generating subplan XXX_2 for CTE cte_2: SELECT col_1, col_2 FROM (SELECT intermediate_result.col_1, intermediate_result.col_2, intermediate_result.col_3 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(col_1 integer, col_2 integer, col_3 integer)) cte
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT col_1, col_2 FROM (SELECT cte_2.col_1, cte_2.col_2 FROM (SELECT intermediate_result.col_1, intermediate_result.col_2 FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(col_1 integer, col_2 integer)) cte_2) citus_insert_select_subquery
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 SELECT * FROM target_table ORDER BY 1;
  col_1 | col_2 
 -------+-------
      1 |     2
      2 |     3
      3 |     4
      4 |     5
      5 |     6
      6 |     0
      7 |     0
@@ -546,48 +522,39 @@
      9 |     0
     10 |     0
 (10 rows)
 
 -- make sure that even if COPY switchover happens
 -- the results are correct
 SET citus.copy_switchover_threshold TO 1;
 TRUNCATE target_table;
 -- load some data to make sure copy commands switch over connections
 INSERT INTO target_table SELECT i,0 FROM generate_series(0,500)i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 -- make sure that SELECT only uses 1 connection 1 node
 -- yet still COPY commands use 1 connection per co-located
 -- intermediate result file
 SET citus.max_adaptive_executor_pool_size TO 1;
 INSERT INTO target_table SELECT * FROM target_table LIMIT 10000 ON CONFLICT(col_1) DO UPDATE SET col_2 = EXCLUDED.col_2 + 1;
-DEBUG:  LIMIT clauses are not allowed in distributed INSERT ... SELECT queries
-DEBUG:  push down of limit count: 10000
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 SELECT DISTINCT col_2 FROM target_table;
  col_2 
 -------
      1
 (1 row)
 
 WITH cte_1 AS (INSERT INTO target_table SELECT * FROM target_table LIMIT 10000 ON CONFLICT(col_1) DO UPDATE SET col_2 = EXCLUDED.col_2 + 1 RETURNING *)
 SELECT DISTINCT col_2 FROM cte_1;
-DEBUG:  generating subplan XXX_1 for CTE cte_1: INSERT INTO on_conflict.target_table (col_1, col_2) SELECT col_1, col_2 FROM on_conflict.target_table LIMIT 10000 ON CONFLICT(col_1) DO UPDATE SET col_2 = (excluded.col_2 OPERATOR(pg_catalog.+) 1) RETURNING target_table.col_1, target_table.col_2
-DEBUG:  LIMIT clauses are not allowed in distributed INSERT ... SELECT queries
-DEBUG:  push down of limit count: 10000
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT DISTINCT col_2 FROM (SELECT intermediate_result.col_1, intermediate_result.col_2 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(col_1 integer, col_2 integer)) cte_1
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
  col_2 
 -------
      2
 (1 row)
 
 RESET client_min_messages;
 DROP SCHEMA on_conflict CASCADE;
-NOTICE:  drop cascades to 7 other objects
+NOTICE:  drop cascades to 8 other objects
 DETAIL:  drop cascades to table test_ref_table
+drop cascades to table test_ref_table_1900012
 drop cascades to table source_table_3
 drop cascades to table source_table_4
 drop cascades to table target_table_2
 drop cascades to table target_table
 drop cascades to table source_table_1
 drop cascades to table source_table_2
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/citus_table_triggers.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/citus_table_triggers.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/citus_table_triggers.out.modified	2022-11-09 13:37:47.279312563 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/citus_table_triggers.out.modified	2022-11-09 13:37:47.289312563 +0300
@@ -11,75 +11,66 @@
 -------------------------------------------------------------------------------
 -- create a simple function to be invoked by triggers
 CREATE FUNCTION update_value() RETURNS trigger AS $update_value$
 BEGIN
     NEW.value := value+1 ;
     RETURN NEW;
 END;
 $update_value$ LANGUAGE plpgsql;
 CREATE TABLE distributed_table (value int);
 SELECT create_distributed_table('distributed_table', 'value');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 CREATE TABLE reference_table (value int);
 SELECT create_reference_table('reference_table');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 -- below two should fail
 CREATE TRIGGER update_value_dist
 AFTER INSERT ON distributed_table
 FOR EACH ROW EXECUTE FUNCTION update_value();
-ERROR:  triggers are not supported on distributed tables
 CREATE TRIGGER update_value_ref
 AFTER INSERT ON reference_table
 FOR EACH ROW EXECUTE FUNCTION update_value();
 ERROR:  triggers are not supported on reference tables
 --------------------------------------------------------------------------------
 -- show that we error out for trigger commands on distributed & reference tables
 --------------------------------------------------------------------------------
 SET citus.enable_ddl_propagation to OFF;
 -- create triggers when ddl propagation is off
 CREATE TRIGGER update_value_dist
 AFTER INSERT ON distributed_table
 FOR EACH ROW EXECUTE FUNCTION update_value();
+ERROR:  trigger "update_value_dist" for relation "distributed_table" already exists
 CREATE TRIGGER update_value_ref
 AFTER INSERT ON reference_table
 FOR EACH ROW EXECUTE FUNCTION update_value();
 -- enable ddl propagation back
 SET citus.enable_ddl_propagation to ON;
 -- create an extension for "depends on" commands
 CREATE EXTENSION seg;
 -- below all should error out
 ALTER TRIGGER update_value_dist ON distributed_table RENAME TO update_value_dist1;
-ERROR:  triggers are not supported on distributed tables
 ALTER TRIGGER update_value_dist ON distributed_table DEPENDS ON EXTENSION seg;
-ERROR:  trigger "update_value_dist" depends on an extension and this is not supported for distributed tables and local tables added to metadata
+ERROR:  trigger "update_value_dist" for table "distributed_table" does not exist
 DROP TRIGGER update_value_dist ON distributed_table;
-ERROR:  triggers are not supported on distributed tables
+ERROR:  trigger "update_value_dist" for table "distributed_table" does not exist
 ALTER TABLE distributed_table DISABLE TRIGGER ALL;
-ERROR:  triggers are not supported on distributed tables
 ALTER TABLE distributed_table DISABLE TRIGGER USER;
-ERROR:  triggers are not supported on distributed tables
 ALTER TABLE distributed_table DISABLE TRIGGER update_value_dist;
-ERROR:  triggers are not supported on distributed tables
+ERROR:  trigger "update_value_dist" for table "distributed_table" does not exist
 ALTER TABLE distributed_table ENABLE TRIGGER ALL;
-ERROR:  triggers are not supported on distributed tables
 ALTER TABLE distributed_table ENABLE TRIGGER USER;
-ERROR:  triggers are not supported on distributed tables
 ALTER TABLE distributed_table ENABLE TRIGGER update_value_dist;
-ERROR:  triggers are not supported on distributed tables
+ERROR:  trigger "update_value_dist" for table "distributed_table" does not exist
 -- below all should error out
 ALTER TRIGGER update_value_ref ON reference_table RENAME TO update_value_ref1;
 ERROR:  triggers are not supported on reference tables
 ALTER TRIGGER update_value_ref ON reference_table DEPENDS ON EXTENSION seg;
 ERROR:  trigger "update_value_ref" depends on an extension and this is not supported for distributed tables and local tables added to metadata
 DROP TRIGGER update_value_ref ON reference_table;
 ERROR:  triggers are not supported on reference tables
 ALTER TABLE reference_table DISABLE TRIGGER ALL;
 ERROR:  triggers are not supported on reference tables
 ALTER TABLE reference_table DISABLE TRIGGER USER;
@@ -148,11 +139,11 @@
  CREATE TRIGGER test_table_delete AFTER DELETE ON table_triggers_schema.test_table FOR EACH STATEMENT EXECUTE FUNCTION table_triggers_schema.test_table_trigger_function()
  ALTER TABLE table_triggers_schema.test_table ENABLE TRIGGER test_table_delete;
  CREATE CONSTRAINT TRIGGER test_table_insert AFTER INSERT ON table_triggers_schema.test_table DEFERRABLE INITIALLY IMMEDIATE FOR EACH ROW WHEN (((new.id > 5) OR ((new.text_col IS NOT NULL) AND ((new.id)::numeric < to_number(new.text_number, '9999'::text))))) EXECUTE FUNCTION table_triggers_schema.test_table_trigger_function()
  ALTER TABLE table_triggers_schema.test_table ENABLE TRIGGER test_table_insert;
  CREATE CONSTRAINT TRIGGER test_table_update AFTER UPDATE OF id ON table_triggers_schema.test_table NOT DEFERRABLE INITIALLY IMMEDIATE FOR EACH ROW WHEN (((NOT (old.* IS DISTINCT FROM new.*)) AND (old.text_number IS NOT NULL))) EXECUTE FUNCTION table_triggers_schema.test_table_trigger_function()
  ALTER TABLE table_triggers_schema.test_table ENABLE TRIGGER test_table_update;
 (8 rows)
 
 -- cleanup at exit
 DROP SCHEMA table_triggers_schema CASCADE;
-NOTICE:  drop cascades to 8 other objects
+NOTICE:  drop cascades to 9 other objects
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_row_insert.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_row_insert.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_row_insert.out.modified	2022-11-09 13:37:47.559312562 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_row_insert.out.modified	2022-11-09 13:37:47.569312562 +0300
@@ -14,21 +14,21 @@
 SELECT * FROM pg_dist_shard WHERE logicalrelid='source_table_xyz'::regclass::oid ORDER BY shardid;
    logicalrelid   | shardid | shardstorage | shardminvalue | shardmaxvalue 
 ------------------+---------+--------------+---------------+---------------
  source_table_xyz | 4213581 | t            | (0,a)         | (24,a)
  source_table_xyz | 4213582 | t            | (25,z)        | (49,z)
 (2 rows)
 
 SELECT shardid, nodename, nodeport FROM pg_dist_shard_placement WHERE EXISTS(SELECT shardid FROM pg_dist_shard WHERE shardid=pg_dist_shard_placement.shardid AND logicalrelid='source_table_xyz'::regclass::oid) ORDER BY 1, 2, 3;
  shardid | nodename  | nodeport 
 ---------+-----------+----------
- 4213581 | localhost |    57637
+ 4213581 | localhost |    57638
  4213582 | localhost |    57638
 (2 rows)
 
 INSERT INTO source_table_xyz VALUES ((0, 'a'), 1, (0, 'a')),
                                 ((1, 'b'), 2, (26, 'b')),
                                 ((2, 'c'), 3, (3, 'c')),
                                 ('(4,d)', 4, (27, 'd')),
                                 ((30, 'e'), 5, (30, 'e')),
                                 ((31, 'f'), 6, (31, 'f')),
                                 ((32, 'g'), 7, (8, 'g'));
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/insert_select_into_local_table.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/insert_select_into_local_table.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/insert_select_into_local_table.out.modified	2022-11-09 13:37:47.669312562 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/insert_select_into_local_table.out.modified	2022-11-09 13:37:47.679312562 +0300
@@ -1,21 +1,18 @@
 CREATE SCHEMA insert_select_into_local_table;
 SET search_path TO insert_select_into_local_table;
 SET citus.shard_count = 4;
 SET citus.next_shard_id TO 11235800;
 CREATE TABLE dist_table (a INT, b INT, c TEXT);
 SELECT create_distributed_table('dist_table', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO dist_table VALUES (1, 6, 'txt1'), (2, 7, 'txt2'), (3, 8, 'txt3');
 CREATE TABLE non_dist_1 (a INT, b INT, c TEXT);
 CREATE TABLE non_dist_2 (a INT, c TEXT);
 CREATE TABLE non_dist_3 (a INT);
 -- test non-router queries
 INSERT INTO non_dist_1 SELECT * FROM dist_table;
 INSERT INTO non_dist_2 SELECT a, c FROM dist_table;
 INSERT INTO non_dist_3 SELECT a FROM dist_table;
 SELECT * FROM non_dist_1 ORDER BY 1, 2, 3;
  a | b |  c   
@@ -74,40 +71,30 @@
  7 | 2 | txt2
  8 | 3 | txt3
 (3 rows)
 
 TRUNCATE non_dist_1;
 -- test EXPLAIN
 EXPLAIN (COSTS FALSE) INSERT INTO non_dist_1 SELECT * FROM dist_table;
           QUERY PLAN          
 ------------------------------
  Insert on non_dist_1
-   ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-         Tasks Shown: One of 4
-         ->  Task
-               Node: host=localhost port=xxxxx dbname=regression
-               ->  Seq Scan on dist_table_11235800 dist_table
-(7 rows)
+   ->  Seq Scan on dist_table
+(2 rows)
 
 EXPLAIN (COSTS FALSE) INSERT INTO non_dist_1 SELECT * FROM dist_table WHERE a = 1;
           QUERY PLAN          
 ------------------------------
  Insert on non_dist_1
-   ->  Custom Scan (Citus Adaptive)
-         Task Count: 1
-         Tasks Shown: All
-         ->  Task
-               Node: host=localhost port=xxxxx dbname=regression
-               ->  Seq Scan on dist_table_11235800 dist_table
+   ->  Seq Scan on dist_table
          Filter: (a = 1)
-(8 rows)
+(3 rows)
 
 -- test RETURNING
 INSERT INTO non_dist_1 SELECT * FROM dist_table ORDER BY 1, 2, 3 RETURNING *;
  a | b |  c   
 ---+---+------
  1 | 6 | txt1
  2 | 7 | txt2
  3 | 8 | txt3
 (3 rows)
 
@@ -439,39 +426,33 @@
 );
 ALTER TABLE local_dest_table DROP COLUMN drop_col;
 CREATE TABLE dist_source_table_1(
   int_col integer,
   drop_col text,
   text_col_1 text,
   dist_col integer,
   text_col_2 text
 );
 SELECT create_distributed_table('dist_source_table_1', 'dist_col');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ALTER TABLE dist_source_table_1 DROP COLUMN drop_col;
 INSERT INTO dist_source_table_1 VALUES (1, 'value', 1, 'value');
 INSERT INTO dist_source_table_1 VALUES (2, 'value2', 1, 'value');
 INSERT INTO dist_source_table_1 VALUES (3, 'value', 3, 'value3');
 CREATE TABLE dist_source_table_2(
   dist_col integer,
   int_col integer
 );
 SELECT create_distributed_table('dist_source_table_2', 'dist_col');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO dist_source_table_2 VALUES (1, 1);
 INSERT INTO dist_source_table_2 VALUES (2, 2);
 INSERT INTO dist_source_table_2 VALUES (4, 4);
 CREATE TABLE local_source_table_1 AS SELECT * FROM dist_source_table_1;
 CREATE TABLE local_source_table_2 AS SELECT * FROM dist_source_table_2;
 /*
  * query_results_equal compares the effect of two queries on local_dest_table.
  * We use this to ensure that INSERT INTO local_dest_table SELECT behaves
  * the same when selecting from a regular table (postgres handles it) and
  * a distributed table (Citus handles it).
@@ -1105,11 +1086,11 @@
 RETURNING *;
  col_1 | col_2 |  col_3  |  col_4  | col_5 | col_6 | col_7 | col_8 | col_9 
 -------+-------+---------+---------+-------+-------+-------+-------+-------
        |       | string2 | string1 |       |       | col_7 |       |     5
        |       | string2 | string1 |       |       | col_7 |       |     6
 (2 rows)
 
 ROLLBACK;
 \set VERBOSITY terse
 DROP SCHEMA insert_select_into_local_table CASCADE;
-NOTICE:  drop cascades to 12 other objects
+NOTICE:  drop cascades to 13 other objects
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/alter_index.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/alter_index.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/alter_index.out.modified	2022-11-09 13:37:47.799312561 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/alter_index.out.modified	2022-11-09 13:37:47.809312561 +0300
@@ -44,23 +44,15 @@
  test_idx2 |          3737
 (4 rows)
 
 \c - - - :worker_1_port
 SELECT c.relname, a.attstattarget
 FROM pg_attribute a
 JOIN pg_class c ON a.attrelid = c.oid AND c.relname SIMILAR TO 'test\_idx%\_\d%'
 ORDER BY c.relname, a.attnum;
  relname | attstattarget 
 ---------+---------------
- test_idx2_980004 |            -1
- test_idx2_980004 |         10000
- test_idx2_980004 |          3737
- test_idx2_980006 |            -1
- test_idx2_980006 |         10000
- test_idx2_980006 |          3737
- test_idx_980000  |          4646
- test_idx_980002  |          4646
-(8 rows)
+(0 rows)
 
 \c - - - :master_port
 SET client_min_messages TO WARNING;
 DROP SCHEMA alterindex CASCADE;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/insert_select_connection_leak.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/insert_select_connection_leak.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/insert_select_connection_leak.out.modified	2022-11-09 13:37:48.179312560 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/insert_select_connection_leak.out.modified	2022-11-09 13:37:48.189312560 +0300
@@ -4,45 +4,33 @@
 SET citus.shard_count TO 64;
 CREATE OR REPLACE FUNCTION
 worker_connection_count(nodeport int)
 RETURNS int AS $$
   SELECT result::int - 1 FROM
   run_command_on_workers($Q$select count(*) from pg_stat_activity where backend_type = 'client backend';$Q$)
   WHERE nodeport = nodeport
 $$ LANGUAGE SQL;
 CREATE TABLE source_table(a int, b int);
 SELECT create_distributed_table('source_table', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE target_table(a numeric, b int not null);
 SELECT create_distributed_table('target_table', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO source_table SELECT i, 2 * i FROM generate_series(1, 100) i;
 EXPLAIN (costs off) INSERT INTO target_table SELECT * FROM source_table;
            QUERY PLAN           
 --------------------------------
- Custom Scan (Citus INSERT ... SELECT)
-   INSERT/SELECT method: repartition
-   ->  Custom Scan (Citus Adaptive)
-         Task Count: 64
-         Tasks Shown: One of 64
-         ->  Task
-               Node: host=localhost port=xxxxx dbname=regression
-               ->  Seq Scan on source_table_4213581 source_table
-(8 rows)
+ Insert on target_table
+   ->  Seq Scan on source_table
+(2 rows)
 
 SELECT worker_connection_count(:worker_1_port) AS pre_xact_worker_1_connections,
        worker_connection_count(:worker_2_port) AS pre_xact_worker_2_connections \gset
 BEGIN;
 INSERT INTO target_table SELECT * FROM source_table;
 SELECT worker_connection_count(:worker_1_port) AS worker_1_connections,
        worker_connection_count(:worker_2_port) AS worker_2_connections \gset
 INSERT INTO target_table SELECT * FROM source_table;
 INSERT INTO target_table SELECT * FROM source_table;
 INSERT INTO target_table SELECT * FROM source_table;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/replicated_partitioned_table.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/replicated_partitioned_table.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/replicated_partitioned_table.out.modified	2022-11-09 13:37:48.599312558 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/replicated_partitioned_table.out.modified	2022-11-09 13:37:48.609312558 +0300
@@ -19,193 +19,138 @@
 	PARTITION OF collections (key, ts, collection_id, value)
 	FOR VALUES IN ( 2 );
 -- load some data data
 INSERT INTO collections (key, ts, collection_id, value) VALUES (1, '2009-01-01', 1, 1);
 INSERT INTO collections (key, ts, collection_id, value) VALUES (2, '2009-01-01', 1, 2);
 INSERT INTO collections (key, ts, collection_id, value) VALUES (3, '2009-01-01', 2, 1);
 INSERT INTO collections (key, ts, collection_id, value) VALUES (4, '2009-01-01', 2, 2);
 -- in the first case, we'll distributed the
 -- already existing partitioninong hierarcy
 SELECT create_distributed_table('collections', 'key');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$partitioned_table_replicated.collections_1$$)
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$partitioned_table_replicated.collections_2$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- now create partition of a already distributed table
 CREATE TABLE collections_3 PARTITION OF collections FOR VALUES IN ( 3 );
 -- now attaching non distributed table to a distributed table
 CREATE TABLE collections_4 AS SELECT * FROM collections LIMIT 0;
 -- load some data
 INSERT INTO collections_4 SELECT i, '2009-01-01', 4, i FROM generate_series (0, 10) i;
 ALTER TABLE collections ATTACH PARTITION collections_4 FOR VALUES IN ( 4 );
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$partitioned_table_replicated.collections_4$$)
 -- finally attach a distributed table to a distributed table
 CREATE TABLE collections_5 AS SELECT * FROM collections LIMIT 0;
 SELECT create_distributed_table('collections_5', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- load some data
 INSERT INTO collections_5 SELECT i, '2009-01-01', 5, i FROM generate_series (0, 10) i;
 ALTER TABLE collections ATTACH PARTITION collections_5 FOR VALUES IN ( 5 );
 -- make sure that we've all the placements
 SELECT
 	logicalrelid, count(*) as placement_count
 FROM
 	pg_dist_shard, pg_dist_shard_placement
 WHERE
 	logicalrelid::text LIKE '%collections%' AND
 	pg_dist_shard.shardid = pg_dist_shard_placement.shardid
 GROUP BY
 	logicalrelid
 ORDER BY
 	1,2;
  logicalrelid | placement_count 
 --------------+-----------------
- collections   |               8
- collections_1 |               8
- collections_2 |               8
- collections_3 |               8
- collections_4 |               8
- collections_5 |               8
-(6 rows)
+(0 rows)
 
 -- and, make sure that all tables are colocated
 SELECT
 	count(DISTINCT colocationid)
 FROM
 	pg_dist_partition
 WHERE
 	logicalrelid::text LIKE '%collections%';
  count 
 -------
-     1
+     0
 (1 row)
 
 -- make sure that any kind of modification is disallowed on partitions
 -- given that replication factor > 1
 INSERT INTO collections_4 (key, ts, collection_id, value) VALUES (4, '2009-01-01', 2, 2);
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
+ERROR:  new row for relation "collections_4" violates partition constraint
+DETAIL:  Failing row contains (4, Thu Jan 01 00:00:00 2009 PST, 2, 2).
 -- single shard update/delete not allowed
 UPDATE collections_1 SET ts = now() WHERE key = 1;
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
 DELETE FROM collections_1 WHERE ts = now() AND key = 1;
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
 -- multi shard update/delete are not allowed
 UPDATE collections_1 SET ts = now();
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
 DELETE FROM collections_1 WHERE ts = now();
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
 -- insert..select pushdown
 INSERT INTO collections_1 SELECT * FROM collections_1;
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
 -- insert..select via coordinator
 INSERT INTO collections_1 SELECT * FROM collections_1 OFFSET 0;
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
 -- COPY is not allowed
 COPY collections_1 FROM STDIN;
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
-\.
-invalid command \.
 -- DDLs are not allowed
 CREATE INDEX index_on_partition ON collections_1(key);
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
 -- EXPLAIN with modifications is not allowed as well
 UPDATE collections_1 SET ts = now() WHERE key = 1;
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
 -- TRUNCATE is also not allowed
 TRUNCATE collections_1;
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
 TRUNCATE collections, collections_1;
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
 -- modifying CTEs are also not allowed
 WITH collections_5_cte AS
 (
 	DELETE FROM collections_5 RETURNING *
 )
 SELECT * FROM collections_5_cte;
-ERROR:  modifications on partitions when replication factor is greater than 1 is not supported
-HINT:  Run the query on the parent table "collections" instead.
+ key | ts | collection_id | value 
+-----+----+---------------+-------
+(0 rows)
+
 -- foreign key creation is disallowed due to replication factor > 1
 CREATE TABLE fkey_test (key bigint PRIMARY KEY);
 SELECT create_distributed_table('fkey_test', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ALTER TABLE
 	collections_5
 ADD CONSTRAINT
 	fkey_delete FOREIGN KEY(key)
 REFERENCES
 	fkey_test(key) ON DELETE CASCADE;
-ERROR:  cannot create foreign key constraint
-DETAIL:  Citus currently supports foreign key constraints only for "citus.shard_replication_factor = 1".
-HINT:  Please change "citus.shard_replication_factor to 1". To learn more about using foreign keys with other replication factors, please contact us at https://citusdata.com/about/contact_us.
 -- we should be able to attach and detach partitions
 -- given that those DDLs are on the parent table
 CREATE TABLE collections_6
 	PARTITION OF collections (key, ts, collection_id, value)
 	FOR VALUES IN ( 6 );
 ALTER TABLE collections DETACH PARTITION collections_6;
 ALTER TABLE collections ATTACH PARTITION collections_6 FOR VALUES IN ( 6 );
 -- read queries works just fine
 SELECT count(*) FROM collections_1 WHERE key = 1;
  count 
 -------
-     1
+     0
 (1 row)
 
 SELECT count(*) FROM collections_1 WHERE key != 1;
  count 
 -------
-     1
+     0
 (1 row)
 
 -- rollups SELECT'ing from partitions should work just fine
 CREATE TABLE collections_agg (
 	key bigint,
 	sum_value numeric
 );
 SELECT create_distributed_table('collections_agg', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- pushdown roll-up
 INSERT INTO collections_agg SELECT key, sum(key) FROM collections_1 GROUP BY key;
 -- coordinator roll-up
 INSERT INTO collections_agg SELECT collection_id, sum(key) FROM collections_1 GROUP BY collection_id;
 -- now make sure that copy functionality works fine
 -- create a table and create its distribution metadata
 CREATE TABLE customer_engagements (id integer, event_id int) PARTITION BY LIST ( event_id );
 CREATE TABLE customer_engagements_1
 	PARTITION OF customer_engagements
 	FOR VALUES IN ( 1 );
@@ -214,75 +159,56 @@
 	FOR VALUES IN ( 2 );
 -- add some indexes
 CREATE INDEX ON customer_engagements (id);
 CREATE INDEX ON customer_engagements (event_id);
 CREATE INDEX ON customer_engagements (id, event_id);
 -- distribute the table
 -- create a single shard on the first worker
 SET citus.shard_count TO 1;
 SET citus.shard_replication_factor TO 2;
 SELECT create_distributed_table('customer_engagements', 'id', 'hash', colocate_with := 'none');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- ingest some data for the tests
 INSERT INTO customer_engagements VALUES (1, 1);
 INSERT INTO customer_engagements VALUES (2, 1);
 INSERT INTO customer_engagements VALUES (1, 2);
 INSERT INTO customer_engagements VALUES (2, 2);
 -- get the newshardid
 SELECT shardid as newshardid FROM pg_dist_shard WHERE logicalrelid = 'customer_engagements'::regclass
 \gset
+no rows returned for \gset
 -- delete all the placements on the second node belonging to partitioning hierarchy
 DELETE FROM pg_dist_shard_placement p USING pg_dist_shard s
 WHERE s.shardid = p.shardid AND nodeport = :worker_2_port AND logicalrelid::text LIKE 'customer_engagements%';
 -- cannot copy a shard after a modification (transaction still open during copy)
 BEGIN;
 INSERT INTO customer_engagements VALUES (1, 1);
 SELECT citus_copy_shard_placement(:newshardid, 'localhost', :worker_1_port, 'localhost', :worker_2_port, transfer_mode := 'block_writes');
-ERROR:  cannot open new connections after the first modification command within a transaction
+ERROR:  syntax error at or near ":"
 ROLLBACK;
 -- modifications after copying a shard are fine (will use new metadata)
 BEGIN;
 SELECT citus_copy_shard_placement(:newshardid, 'localhost', :worker_1_port, 'localhost', :worker_2_port, transfer_mode := 'block_writes');
- citus_copy_shard_placement
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  syntax error at or near ":"
 ALTER TABLE customer_engagements ADD COLUMN value float DEFAULT 1.0;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT * FROM customer_engagements ORDER BY 1,2,3;
- id | event_id | value
----------------------------------------------------------------------
-  1 |        1 |     1
-  1 |        2 |     1
-  2 |        1 |     1
-  2 |        2 |     1
-(4 rows)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 ROLLBACK;
 BEGIN;
 SELECT citus_copy_shard_placement(:newshardid, 'localhost', :worker_1_port, 'localhost', :worker_2_port, transfer_mode := 'block_writes');
- citus_copy_shard_placement
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  syntax error at or near ":"
 INSERT INTO customer_engagements VALUES (1, 1);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT count(*) FROM customer_engagements;
- count
----------------------------------------------------------------------
-     5
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 ROLLBACK;
 -- TRUNCATE is allowed on the parent table
 -- try it just before dropping the table
 TRUNCATE collections;
 SET search_path TO public;
 DROP SCHEMA partitioned_table_replicated CASCADE;
 NOTICE:  drop cascades to 4 other objects
 DETAIL:  drop cascades to table partitioned_table_replicated.collections
 drop cascades to table partitioned_table_replicated.fkey_test
 drop cascades to table partitioned_table_replicated.collections_agg
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_partitioning.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_partitioning.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_partitioning.out.modified	2022-11-09 13:38:00.059312511 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_partitioning.out.modified	2022-11-09 13:38:00.139312511 +0300
@@ -1937,36 +1937,30 @@
 ALTER TABLE partitioning_test DETACH PARTITION partitioning_test_2013;
 DROP TABLE partitioning_test_2008, partitioning_test_2009, partitioning_test_2010,
            partitioning_test_2011, partitioning_test_2013, reference_table_2;
 -- verify this doesn't crash and gives a debug message for dropped table
 SET client_min_messages TO DEBUG1;
 DROP TABLE partitioning_test, reference_table;
 DEBUG:  drop cascades to constraint partitioning_reference_fkey on table partitioning_schema.partitioning_test
 DETAIL:  from localhost:57638
 CONTEXT:  SQL statement "SELECT master_remove_distributed_table_metadata_from_workers(v_obj.objid, v_obj.schema_name, v_obj.object_name)"
 PL/pgSQL function citus_drop_trigger() line 13 at PERFORM
-DEBUG:  drop cascades to constraint partitioning_reference_fkey on table partitioning_schema.partitioning_test
-DETAIL:  from localhost:xxxxx
-CONTEXT:  SQL statement "SELECT master_remove_distributed_table_metadata_from_workers(v_obj.objid, v_obj.schema_name, v_obj.object_name)"
-PL/pgSQL function citus_drop_trigger() line XX at PERFORM
 DEBUG:  switching to sequential query execution mode
 DETAIL:  Table "<dropped>" is modified, which might lead to data inconsistencies or distributed deadlocks via parallel accesses to hash distributed tables due to foreign keys. Any parallel modification to those hash distributed tables in the same transaction can only be executed in sequential query execution mode
 CONTEXT:  SQL statement "SELECT citus_drop_all_shards(v_obj.objid, v_obj.schema_name, v_obj.object_name, drop_shards_metadata_only := false)"
 PL/pgSQL function citus_drop_trigger() line 25 at PERFORM
-DEBUG:  drop cascades to 2 other objects
-DETAIL:  drop cascades to constraint partitioning_reference_fkey_1660302 on table partitioning_schema.partitioning_test_1660302
-drop cascades to constraint partitioning_reference_fkey_1660304 on table partitioning_schema.partitioning_test_1660304
-DETAIL:  from localhost:xxxxx
 CONTEXT:  SQL statement "SELECT citus_drop_all_shards(v_obj.objid, v_obj.schema_name, v_obj.object_name, drop_shards_metadata_only := false)"
 PL/pgSQL function citus_drop_trigger() line 25 at PERFORM
-DEBUG:  drop cascades to 2 other objects
-DETAIL:  drop cascades to constraint partitioning_reference_fkey_1660303 on table partitioning_schema.partitioning_test_1660303
+DEBUG:  drop cascades to 4 other objects
+DETAIL:  drop cascades to constraint partitioning_reference_fkey_1660302 on table partitioning_schema.partitioning_test_1660302
+drop cascades to constraint partitioning_reference_fkey_1660303 on table partitioning_schema.partitioning_test_1660303
+drop cascades to constraint partitioning_reference_fkey_1660304 on table partitioning_schema.partitioning_test_1660304
 drop cascades to constraint partitioning_reference_fkey_1660305 on table partitioning_schema.partitioning_test_1660305
 DETAIL:  from localhost:57638
 CONTEXT:  SQL statement "SELECT citus_drop_all_shards(v_obj.objid, v_obj.schema_name, v_obj.object_name, drop_shards_metadata_only := false)"
 PL/pgSQL function citus_drop_trigger() line 25 at PERFORM
 RESET client_min_messages;
 RESET SEARCH_PATH;
 -- not timestamp partitioned
 CREATE TABLE not_time_partitioned (x int, y int) PARTITION BY RANGE (x);
 CREATE TABLE not_time_partitioned_p0 PARTITION OF not_time_partitioned DEFAULT;
 CREATE TABLE not_time_partitioned_p1 PARTITION OF not_time_partitioned FOR VALUES FROM (1) TO (2);
@@ -2043,36 +2037,33 @@
                          16384
 (1 row)
 
 RESET client_min_messages;
 CREATE INDEX idx_btree_hobbies ON "events.Energy Added" USING BTREE ((data->>'location'));
  \c - - - :worker_1_port
 -- should not be zero because of TOAST, vm, fms
 SELECT worker_partitioned_table_size(oid) FROM pg_class WHERE relname LIKE '%events.Energy Added%' ORDER BY relname LIMIT 1;
  worker_partitioned_table_size 
 -------------------------------
-                          8192
-(1 row)
+(0 rows)
 
 -- should be zero since no data
 SELECT worker_partitioned_relation_size(oid) FROM pg_class WHERE relname LIKE '%events.Energy Added%' ORDER BY relname LIMIT 1;
  worker_partitioned_relation_size 
 ----------------------------------
-                                0
-(1 row)
+(0 rows)
 
 -- should not be zero because of indexes + pg_table_size()
 SELECT worker_partitioned_relation_total_size(oid) FROM pg_class WHERE relname LIKE '%events.Energy Added%' ORDER BY relname LIMIT 1;
  worker_partitioned_relation_total_size 
 ----------------------------------------
-                                  24576
-(1 row)
+(0 rows)
 
  \c - - - :master_port
 DROP TABLE "events.Energy Added";
 -- test we skip the foreign key validation query on coordinator
 -- that happens when attaching a non-distributed partition to a distributed-partitioned table
 -- with a foreign key to another distributed table
 SET search_path = partitioning_schema;
 SET citus.shard_replication_factor = 1;
 CREATE TABLE another_distributed_table (x int primary key, y int);
 SELECT create_distributed_table('another_distributed_table','x');
@@ -3766,21 +3757,20 @@
  pi_table     | event_time       | pi_table_p2021_09_20 | Mon Sep 20 06:22:17.68432 2021 PDT | Thu Sep 23 09:46:11.28959 2021 PDT | heap
  pi_table     | event_time       | pi_table_p2021_09_23 | Thu Sep 23 09:46:11.28959 2021 PDT | Sun Sep 26 13:10:04.89486 2021 PDT | heap
  pi_table     | event_time       | pi_table_p2021_09_26 | Sun Sep 26 13:10:04.89486 2021 PDT | Wed Sep 29 16:33:58.50013 2021 PDT | heap
  pi_table     | event_time       | pi_table_p2021_09_29 | Wed Sep 29 16:33:58.50013 2021 PDT | Sat Oct 02 19:57:52.1054 2021 PDT  | heap
 (20 rows)
 
 ROLLBACK;
 DROP TABLE pi_table;
 -- 6) test with citus local table
 select 1 from citus_add_node('localhost', :master_port, groupid=>0);
-NOTICE:  localhost:xxxxx is the coordinator and already contains metadata, skipping syncing the metadata
  ?column? 
 ----------
         1
 (1 row)
 
 CREATE TABLE date_partitioned_citus_local_table(
  measureid integer,
  eventdate date,
  measure_data jsonb) PARTITION BY RANGE(eventdate);
 SELECT citus_add_local_table_to_metadata('date_partitioned_citus_local_table');
@@ -4252,26 +4242,21 @@
 SELECT create_time_partitions('non_partitioned_table', INTERVAL '1 month', now() + INTERVAL '1 year');
 ERROR:  non_partitioned_table is not partitioned
 CONTEXT:  PL/pgSQL function get_missing_time_partition_ranges(regclass,interval,timestamp with time zone,timestamp with time zone) line 31 at RAISE
 PL/pgSQL function create_time_partitions(regclass,interval,timestamp with time zone,timestamp with time zone) line 20 at FOR over SELECT rows
 CALL drop_old_time_partitions('non_partitioned_table', now());
 ERROR:  non_partitioned_table is not partitioned
 CONTEXT:  PL/pgSQL function drop_old_time_partitions(regclass,timestamp with time zone) line 15 at RAISE
 DROP TABLE non_partitioned_table;
 -- https://github.com/citusdata/citus/issues/4962
 SELECT stop_metadata_sync_to_node('localhost', :worker_1_port);
-NOTICE:  dropping metadata on the node (localhost,57637)
- stop_metadata_sync_to_node
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  node (localhost,57637) does not exist
 SET citus.shard_replication_factor TO 1;
 SET citus.next_shard_id TO 361168;
 CREATE TABLE part_table_with_very_long_name (
     dist_col integer,
     long_named_integer_col integer,
     long_named_part_col timestamp
 ) PARTITION BY RANGE (long_named_part_col);
 CREATE TABLE part_table_with_long_long_long_long_name
 PARTITION OF part_table_with_very_long_name
 FOR VALUES FROM ('2010-01-01') TO ('2015-01-01');
@@ -4288,92 +4273,75 @@
 WHERE schemaname = 'partitioning_schema' AND tablename ilike '%part_table_with_%' ORDER BY 1, 2;
                 tablename                 |                            indexname                            
 ------------------------------------------+-----------------------------------------------------------------
  part_table_with_long_long_long_long_name | part_table_with_long_long_lon_long_named_integer_col_long_n_idx
  part_table_with_very_long_name           | part_table_with_very_long_nam_long_named_integer_col_long_n_idx
 (2 rows)
 
 -- should work properly - no names clashes
 SET client_min_messages TO WARNING;
 SELECT 1 FROM citus_activate_node('localhost', :worker_1_port);
- ?column?
----------------------------------------------------------------------
-        1
-(1 row)
-
+ERROR:  node at "localhost:57637" does not exist
 RESET client_min_messages;
 \c - - - :worker_1_port
 -- check that indexes are named properly
 SELECT tablename, indexname FROM pg_indexes
 WHERE schemaname = 'partitioning_schema' AND tablename ilike '%part_table_with_%' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- part_table_with_long_long_long_long_name        | part_table_with_long_long_lon_long_named_integer_col_long_n_idx
- part_table_with_long_long_long_long_name_361172 | part_table_with_long_long_lon_long_named_intege_f9175544_361172
- part_table_with_long_long_long_long_name_361174 | part_table_with_long_long_lon_long_named_intege_f9175544_361174
- part_table_with_very_long_name                  | part_table_with_very_long_nam_long_named_integer_col_long_n_idx
- part_table_with_very_long_name_361168           | part_table_with_very_long_nam_long_named_intege_73d4b078_361168
- part_table_with_very_long_name_361170           | part_table_with_very_long_nam_long_named_intege_73d4b078_361170
-(6 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO partitioning_schema;
 -- create parent table
 CREATE TABLE stxdinp(i int, a int, b int) PARTITION BY RANGE (i);
 -- create partition
 CREATE TABLE stxdinp1 PARTITION OF stxdinp FOR VALUES FROM (1) TO (100);
 -- populate table
 INSERT INTO stxdinp SELECT 1, a/100, a/100 FROM generate_series(1, 999) a;
 -- create extended statistics
 CREATE STATISTICS stxdinp ON a, b FROM stxdinp;
 -- distribute parent table
 SELECT create_distributed_table('stxdinp', 'i');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$partitioning_schema.stxdinp1$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- run select query, works fine
 SELECT a, b FROM stxdinp GROUP BY 1, 2;
  a | b 
 ---+---
  1 | 1
  3 | 3
  7 | 7
  2 | 2
  8 | 8
  0 | 0
  5 | 5
- 6 | 6
  9 | 9
+ 6 | 6
  4 | 4
 (10 rows)
 
 -- partitions are processed recursively for PG15+
 VACUUM ANALYZE stxdinp;
 SELECT a, b FROM stxdinp GROUP BY 1, 2;
  a | b 
 ---+---
- 1 | 1
- 3 | 3
- 7 | 7
- 2 | 2
- 8 | 8
  0 | 0
+ 8 | 8
+ 2 | 2
  5 | 5
- 6 | 6
- 9 | 9
+ 1 | 1
  4 | 4
+ 7 | 7
+ 3 | 3
+ 9 | 9
+ 6 | 6
 (10 rows)
 
 DROP SCHEMA partitioning_schema CASCADE;
 NOTICE:  drop cascades to 5 other objects
 DETAIL:  drop cascades to table "schema-test"
 drop cascades to table another_distributed_table
 drop cascades to table distributed_parent_table
 drop cascades to table part_table_with_very_long_name
 drop cascades to table stxdinp
 RESET search_path;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/partitioning_issue_3970.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/partitioning_issue_3970.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/partitioning_issue_3970.out.modified	2022-11-09 13:38:00.569312509 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/partitioning_issue_3970.out.modified	2022-11-09 13:38:00.579312509 +0300
@@ -66,57 +66,21 @@
 
 -- check the constraint names on the worker node
 -- verify that check constraints do not have a shardId suffix
 \c - - - :worker_1_port
 SELECT relname, conname, pg_catalog.pg_get_constraintdef(con.oid, true)
 FROM pg_constraint con JOIN pg_class rel ON (rel.oid=con.conrelid)
 WHERE relname SIMILAR TO 'part_table%\_\d%'
 ORDER BY 1,2,3;
  relname | conname | pg_get_constraintdef 
 ---------+---------+----------------------
- part_table_1690000         | ck_012345678901234567890123456789012345678901234567890123456789 | CHECK (my_seq > 10)
- part_table_1690000         | dist_fk_1690000                                                 | FOREIGN KEY (seq) REFERENCES test_3970.dist_1690004(seq)
- part_table_1690000         | my_seq                                                          | CHECK (my_seq > 0)
- part_table_1690000         | part_table_pkey_1690000                                         | PRIMARY KEY (seq, work_ymdt)
- part_table_1690000         | uniq_1690000                                                    | UNIQUE (seq, work_ymdt)
- part_table_1690000         | work_memo_check                                                 | CHECK (octet_length(work_memo::text) <= 150)
- part_table_1690002         | ck_012345678901234567890123456789012345678901234567890123456789 | CHECK (my_seq > 10)
- part_table_1690002         | dist_fk_1690002                                                 | FOREIGN KEY (seq) REFERENCES test_3970.dist_1690006(seq)
- part_table_1690002         | my_seq                                                          | CHECK (my_seq > 0)
- part_table_1690002         | part_table_pkey_1690002                                         | PRIMARY KEY (seq, work_ymdt)
- part_table_1690002         | uniq_1690002                                                    | UNIQUE (seq, work_ymdt)
- part_table_1690002         | work_memo_check                                                 | CHECK (octet_length(work_memo::text) <= 150)
- part_table_p202008_1690008 | ck_012345678901234567890123456789012345678901234567890123456789 | CHECK (my_seq > 10)
- part_table_p202008_1690008 | dist_fk_1690000                                                 | FOREIGN KEY (seq) REFERENCES test_3970.dist_1690004(seq)
- part_table_p202008_1690008 | my_seq                                                          | CHECK (my_seq > 0)
- part_table_p202008_1690008 | part_table_p202008_pkey_1690008                                 | PRIMARY KEY (seq, work_ymdt)
- part_table_p202008_1690008 | part_table_p202008_seq_work_ymdt_key_1690008                    | UNIQUE (seq, work_ymdt)
- part_table_p202008_1690008 | work_memo_check                                                 | CHECK (octet_length(work_memo::text) <= 150)
- part_table_p202008_1690010 | ck_012345678901234567890123456789012345678901234567890123456789 | CHECK (my_seq > 10)
- part_table_p202008_1690010 | dist_fk_1690002                                                 | FOREIGN KEY (seq) REFERENCES test_3970.dist_1690006(seq)
- part_table_p202008_1690010 | my_seq                                                          | CHECK (my_seq > 0)
- part_table_p202008_1690010 | part_table_p202008_pkey_1690010                                 | PRIMARY KEY (seq, work_ymdt)
- part_table_p202008_1690010 | part_table_p202008_seq_work_ymdt_key_1690010                    | UNIQUE (seq, work_ymdt)
- part_table_p202008_1690010 | work_memo_check                                                 | CHECK (octet_length(work_memo::text) <= 150)
- part_table_p202009_1690012 | ck_012345678901234567890123456789012345678901234567890123456789 | CHECK (my_seq > 10)
- part_table_p202009_1690012 | dist_fk_1690000                                                 | FOREIGN KEY (seq) REFERENCES test_3970.dist_1690004(seq)
- part_table_p202009_1690012 | my_seq                                                          | CHECK (my_seq > 0)
- part_table_p202009_1690012 | part_table_p202009_pkey_1690012                                 | PRIMARY KEY (seq, work_ymdt)
- part_table_p202009_1690012 | part_table_p202009_seq_work_ymdt_key_1690012                    | UNIQUE (seq, work_ymdt)
- part_table_p202009_1690012 | work_memo_check                                                 | CHECK (octet_length(work_memo::text) <= 150)
- part_table_p202009_1690014 | ck_012345678901234567890123456789012345678901234567890123456789 | CHECK (my_seq > 10)
- part_table_p202009_1690014 | dist_fk_1690002                                                 | FOREIGN KEY (seq) REFERENCES test_3970.dist_1690006(seq)
- part_table_p202009_1690014 | my_seq                                                          | CHECK (my_seq > 0)
- part_table_p202009_1690014 | part_table_p202009_pkey_1690014                                 | PRIMARY KEY (seq, work_ymdt)
- part_table_p202009_1690014 | part_table_p202009_seq_work_ymdt_key_1690014                    | UNIQUE (seq, work_ymdt)
- part_table_p202009_1690014 | work_memo_check                                                 | CHECK (octet_length(work_memo::text) <= 150)
-(36 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path = test_3970;
 -- Add tests for curently unsupported DROP/RENAME commands so that we do not forget about
 -- this edge case when we increase our SQL coverage.
 ALTER TABLE part_table RENAME CONSTRAINT my_seq TO my_seq_check;
 ERROR:  renaming constraints belonging to distributed tables is currently unsupported
 ALTER TABLE part_table ALTER CONSTRAINT my_seq DEFERRABLE;
 ERROR:  alter table command is currently unsupported
 DETAIL:  Only ADD|DROP COLUMN, SET|DROP NOT NULL, SET|DROP DEFAULT, ADD|DROP|VALIDATE CONSTRAINT, SET (), RESET (), ENABLE|DISABLE|NO FORCE|FORCE ROW LEVEL SECURITY, ATTACH|DETACH PARTITION and TYPE subcommands are supported.
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/drop_partitioned_table.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/drop_partitioned_table.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/drop_partitioned_table.out.modified	2022-11-09 13:38:01.399312505 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/drop_partitioned_table.out.modified	2022-11-09 13:38:01.419312505 +0300
@@ -74,40 +74,33 @@
 NOTICE:  dropped object: drop_partitioned_table.child1 original: f normal: f
 NOTICE:  dropped object: drop_partitioned_table.child2 original: f normal: f
 SELECT * FROM drop_partitioned_table.tables_info;
  Schema | Name | Type | Owner 
 --------+------+------+-------
 (0 rows)
 
 \c - - - :worker_1_port
 SET search_path = drop_partitioned_table;
 SELECT * FROM drop_partitioned_table.tables_info;
- Schema | Name | Type | Owner
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  relation "drop_partitioned_table.tables_info" does not exist at character 15
 \c - - - :master_port
 SET search_path = drop_partitioned_table;
 SET citus.next_shard_id TO 722000;
 -- CASE 2
 -- Dropping the parent table, but including children in the DROP command
 CREATE TABLE parent (x text, t timestamptz DEFAULT now()) PARTITION BY RANGE (t);
 CREATE TABLE child1 (x text, t timestamptz DEFAULT now());
 ALTER TABLE parent ATTACH PARTITION child1 FOR VALUES FROM ('2021-05-31') TO ('2021-06-01');
 CREATE TABLE child2 (x text, t timestamptz DEFAULT now());
 ALTER TABLE parent ATTACH PARTITION child2 FOR VALUES FROM ('2021-06-30') TO ('2021-07-01');
 SELECT create_distributed_table('parent','x');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 SELECT * FROM drop_partitioned_table.tables_info;
          Schema         |  Name  |       Type        |  Owner   
 ------------------------+--------+-------------------+----------
  drop_partitioned_table | child1 | table             | postgres
  drop_partitioned_table | child2 | table             | postgres
  drop_partitioned_table | parent | partitioned table | postgres
 (3 rows)
 
 \set VERBOSITY terse
 DROP TABLE child1, parent, child2;
@@ -115,53 +108,45 @@
 NOTICE:  dropped object: drop_partitioned_table.child2 original: t normal: f
 NOTICE:  dropped object: drop_partitioned_table.child1 original: t normal: f
 SELECT * FROM drop_partitioned_table.tables_info;
  Schema | Name | Type | Owner 
 --------+------+------+-------
 (0 rows)
 
 \c - - - :worker_1_port
 SET search_path = drop_partitioned_table;
 SELECT * FROM drop_partitioned_table.tables_info;
- Schema | Name | Type | Owner
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  relation "drop_partitioned_table.tables_info" does not exist at character 15
 \c - - - :master_port
 SET search_path = drop_partitioned_table;
 SET citus.next_shard_id TO 723000;
 -- CASE 3
 -- DROP OWNED BY role1; Only parent is owned by role1, children are owned by another owner
 SET client_min_messages TO ERROR;
 CREATE ROLE role1;
 SELECT 1 FROM run_command_on_workers('CREATE ROLE role1');
  ?column? 
 ----------
         1
-        1
-(2 rows)
+(1 row)
 
 RESET client_min_messages;
 GRANT ALL ON SCHEMA drop_partitioned_table TO role1;
 SET ROLE role1;
 CREATE TABLE drop_partitioned_table.parent (x text, t timestamptz DEFAULT now()) PARTITION BY RANGE (t);
 RESET ROLE;
 CREATE TABLE child1 (x text, t timestamptz DEFAULT now());
 ALTER TABLE parent ATTACH PARTITION child1 FOR VALUES FROM ('2021-05-31') TO ('2021-06-01');
 CREATE TABLE child2 (x text, t timestamptz DEFAULT now());
 ALTER TABLE parent ATTACH PARTITION child2 FOR VALUES FROM ('2021-06-30') TO ('2021-07-01');
 SELECT create_distributed_table('parent','x');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 SELECT * FROM drop_partitioned_table.tables_info;
          Schema         |  Name  |       Type        |  Owner   
 ------------------------+--------+-------------------+----------
  drop_partitioned_table | child1 | table             | postgres
  drop_partitioned_table | child2 | table             | postgres
  drop_partitioned_table | parent | partitioned table | role1
 (3 rows)
 
 \set VERBOSITY terse
 DROP OWNED BY role1;
@@ -169,43 +154,36 @@
 NOTICE:  dropped object: drop_partitioned_table.child1 original: f normal: f
 NOTICE:  dropped object: drop_partitioned_table.child2 original: f normal: f
 SELECT * FROM drop_partitioned_table.tables_info;
  Schema | Name | Type | Owner 
 --------+------+------+-------
 (0 rows)
 
 \c - - - :worker_1_port
 SET search_path = drop_partitioned_table;
 SELECT * FROM drop_partitioned_table.tables_info;
- Schema | Name | Type | Owner
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  relation "drop_partitioned_table.tables_info" does not exist at character 15
 \c - - - :master_port
 SET search_path = drop_partitioned_table;
 SET citus.next_shard_id TO 724000;
 -- CASE 4
 -- DROP OWNED BY role1; Parent and children are owned by role1
 GRANT ALL ON SCHEMA drop_partitioned_table TO role1;
 SET ROLE role1;
 CREATE TABLE drop_partitioned_table.parent (x text, t timestamptz DEFAULT now()) PARTITION BY RANGE (t);
 CREATE TABLE drop_partitioned_table.child1 (x text, t timestamptz DEFAULT now());
 ALTER TABLE drop_partitioned_table.parent ATTACH PARTITION drop_partitioned_table.child1 FOR VALUES FROM ('2021-05-31') TO ('2021-06-01');
 CREATE TABLE drop_partitioned_table.child2 (x text, t timestamptz DEFAULT now());
 ALTER TABLE drop_partitioned_table.parent ATTACH PARTITION drop_partitioned_table.child2 FOR VALUES FROM ('2021-06-30') TO ('2021-07-01');
 RESET ROLE;
 SELECT create_distributed_table('parent','x');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 SELECT * FROM drop_partitioned_table.tables_info;
          Schema         |  Name  |       Type        | Owner 
 ------------------------+--------+-------------------+-------
  drop_partitioned_table | child1 | table             | role1
  drop_partitioned_table | child2 | table             | role1
  drop_partitioned_table | parent | partitioned table | role1
 (3 rows)
 
 \set VERBOSITY terse
 DROP OWNED BY role1;
@@ -213,50 +191,42 @@
 NOTICE:  dropped object: drop_partitioned_table.child1 original: t normal: f
 NOTICE:  dropped object: drop_partitioned_table.child2 original: t normal: f
 SELECT * FROM drop_partitioned_table.tables_info;
  Schema | Name | Type | Owner 
 --------+------+------+-------
 (0 rows)
 
 \c - - - :worker_1_port
 SET search_path = drop_partitioned_table;
 SELECT * FROM drop_partitioned_table.tables_info;
- Schema | Name | Type | Owner
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  relation "drop_partitioned_table.tables_info" does not exist at character 15
 \c - - - :master_port
 SET search_path = drop_partitioned_table;
 SET citus.next_shard_id TO 725000;
 REVOKE ALL ON SCHEMA drop_partitioned_table FROM role1;
 DROP ROLE role1;
 SELECT run_command_on_workers('DROP ROLE IF EXISTS role1');
      run_command_on_workers      
 ---------------------------------
- (localhost,57637,t,"DROP ROLE")
  (localhost,57638,t,"DROP ROLE")
-(2 rows)
+(1 row)
 
 -- CASE 5
 -- DROP SCHEMA schema1 CASCADE; Parent is in schema1, children are in another schema
 CREATE SCHEMA schema1;
 CREATE TABLE schema1.parent (x text, t timestamptz DEFAULT now()) PARTITION BY RANGE (t);
 CREATE TABLE child1 (x text, t timestamptz DEFAULT now());
 ALTER TABLE schema1.parent ATTACH PARTITION child1 FOR VALUES FROM ('2021-05-31') TO ('2021-06-01');
 CREATE TABLE child2 (x text, t timestamptz DEFAULT now());
 ALTER TABLE schema1.parent ATTACH PARTITION child2 FOR VALUES FROM ('2021-06-30') TO ('2021-07-01');
 SELECT create_distributed_table('schema1.parent','x');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 SET search_path = drop_partitioned_table, schema1;
 SELECT * FROM drop_partitioned_table.tables_info;
          Schema         |  Name  |       Type        |  Owner   
 ------------------------+--------+-------------------+----------
  drop_partitioned_table | child1 | table             | postgres
  drop_partitioned_table | child2 | table             | postgres
  schema1                | parent | partitioned table | postgres
 (3 rows)
 
 \set VERBOSITY terse
@@ -266,40 +236,33 @@
 NOTICE:  dropped object: drop_partitioned_table.child1 original: f normal: f
 NOTICE:  dropped object: drop_partitioned_table.child2 original: f normal: f
 SELECT * FROM drop_partitioned_table.tables_info;
  Schema | Name | Type | Owner 
 --------+------+------+-------
 (0 rows)
 
 \c - - - :worker_1_port
 SET search_path = drop_partitioned_table, schema1;
 SELECT * FROM drop_partitioned_table.tables_info;
- Schema | Name | Type | Owner
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  relation "drop_partitioned_table.tables_info" does not exist at character 15
 \c - - - :master_port
 SET citus.next_shard_id TO 726000;
 -- CASE 6
 -- DROP SCHEMA schema1 CASCADE; Parent and children are in schema1
 CREATE SCHEMA schema1;
 CREATE TABLE schema1.parent (x text, t timestamptz DEFAULT now()) PARTITION BY RANGE (t);
 CREATE TABLE schema1.child1 (x text, t timestamptz DEFAULT now());
 ALTER TABLE schema1.parent ATTACH PARTITION schema1.child1 FOR VALUES FROM ('2021-05-31') TO ('2021-06-01');
 CREATE TABLE schema1.child2 (x text, t timestamptz DEFAULT now());
 ALTER TABLE schema1.parent ATTACH PARTITION schema1.child2 FOR VALUES FROM ('2021-06-30') TO ('2021-07-01');
 SELECT create_distributed_table('schema1.parent','x');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 SET search_path = drop_partitioned_table, schema1;
 SELECT * FROM drop_partitioned_table.tables_info;
  Schema  |  Name  |       Type        |  Owner   
 ---------+--------+-------------------+----------
  schema1 | child1 | table             | postgres
  schema1 | child2 | table             | postgres
  schema1 | parent | partitioned table | postgres
 (3 rows)
 
 \set VERBOSITY terse
@@ -309,24 +272,21 @@
 NOTICE:  dropped object: schema1.child1 original: f normal: t
 NOTICE:  dropped object: schema1.child2 original: f normal: t
 SELECT * FROM drop_partitioned_table.tables_info;
  Schema | Name | Type | Owner 
 --------+------+------+-------
 (0 rows)
 
 \c - - - :worker_1_port
 SET search_path = drop_partitioned_table, schema1;
 SELECT * FROM drop_partitioned_table.tables_info;
- Schema | Name | Type | Owner
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  relation "drop_partitioned_table.tables_info" does not exist at character 15
 \c - - - :master_port
 SET search_path = drop_partitioned_table;
 -- Check that we actually skip sending remote commands to skip shards
 SET citus.shard_count TO 1;
 SET citus.shard_replication_factor TO 1;
 SET citus.next_shard_id TO 727000;
 ALTER SEQUENCE pg_catalog.pg_dist_colocationid_seq RESTART 1344400;
 DROP EVENT TRIGGER new_trigger_for_drops;
 -- Case 1 - we should skip
 CREATE TABLE parent (x text, t timestamptz DEFAULT now()) PARTITION BY RANGE (t);
@@ -335,60 +295,44 @@
 SELECT create_distributed_table('parent','x');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 BEGIN;
 SET citus.log_remote_commands TO on;
 DROP TABLE parent;
 NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(0, 1956, '2022-11-09 02:38:01.075428-08');
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-NOTICE:  issuing ALTER TABLE IF EXISTS drop_partitioned_table.parent DETACH PARTITION drop_partitioned_table.child1;
 NOTICE:  issuing ALTER TABLE IF EXISTS drop_partitioned_table.parent DETACH PARTITION drop_partitioned_table.child1;
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'on'
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'on'
-NOTICE:  issuing SELECT worker_drop_distributed_table('drop_partitioned_table.parent')
 NOTICE:  issuing SELECT worker_drop_distributed_table('drop_partitioned_table.parent')
 NOTICE:  issuing DROP TABLE IF EXISTS drop_partitioned_table.parent_727000 CASCADE
 NOTICE:  issuing SELECT worker_drop_distributed_table('drop_partitioned_table.child1')
-NOTICE:  issuing SELECT worker_drop_distributed_table('drop_partitioned_table.child1')
-NOTICE:  issuing SELECT pg_catalog.citus_internal_delete_colocation_metadata(1344400)
 NOTICE:  issuing SELECT pg_catalog.citus_internal_delete_colocation_metadata(1344400)
 ROLLBACK;
 NOTICE:  issuing ROLLBACK
-NOTICE:  issuing ROLLBACK
 -- Case 2 - we shouldn't skip
 BEGIN;
 SET citus.log_remote_commands TO on;
 DROP TABLE parent, child1;
 NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(0, 1957, '2022-11-09 02:38:01.092533-08');
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 NOTICE:  issuing ALTER TABLE IF EXISTS drop_partitioned_table.parent DETACH PARTITION drop_partitioned_table.child1;
-NOTICE:  issuing ALTER TABLE IF EXISTS drop_partitioned_table.parent DETACH PARTITION drop_partitioned_table.child1;
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'on'
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'on'
-NOTICE:  issuing SELECT worker_drop_distributed_table('drop_partitioned_table.parent')
 NOTICE:  issuing SELECT worker_drop_distributed_table('drop_partitioned_table.parent')
 NOTICE:  issuing DROP TABLE IF EXISTS drop_partitioned_table.parent_727000 CASCADE
 NOTICE:  issuing SELECT worker_drop_distributed_table('drop_partitioned_table.child1')
-NOTICE:  issuing SELECT worker_drop_distributed_table('drop_partitioned_table.child1')
 NOTICE:  issuing DROP TABLE IF EXISTS drop_partitioned_table.child1_727001 CASCADE
 NOTICE:  issuing SELECT pg_catalog.citus_internal_delete_colocation_metadata(1344400)
-NOTICE:  issuing SELECT pg_catalog.citus_internal_delete_colocation_metadata(1344400)
 ROLLBACK;
 NOTICE:  issuing ROLLBACK
-NOTICE:  issuing ROLLBACK
 DROP SCHEMA drop_partitioned_table CASCADE;
 NOTICE:  drop cascades to 3 other objects
 SET search_path TO public;
 -- dropping the schema should drop the metadata on the workers
 CREATE SCHEMA partitioning_schema;
 SET search_path TO partitioning_schema;
 CREATE TABLE part_table (
       col timestamp
   ) PARTITION BY RANGE (col);
 CREATE TABLE part_table_1
@@ -397,49 +341,45 @@
 SELECT create_distributed_table('part_table', 'col');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 -- show we have pg_dist_partition entries on the workers
 SELECT run_command_on_workers($$SELECT count(*) FROM  pg_dist_partition where exists(select * from pg_class where pg_class.oid=pg_dist_partition.logicalrelid AND relname ILIKE '%part_table%');$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,2)
  (localhost,57638,t,2)
-(2 rows)
+(1 row)
 
 -- show we have pg_dist_object entries on the workers
 SELECT run_command_on_workers($$SELECT count(*) FROM  pg_dist_object as obj where classid = 1259 AND exists(select * from pg_class where pg_class.oid=obj.objid AND relname ILIKE '%part_table%');$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,2)
  (localhost,57638,t,2)
-(2 rows)
+(1 row)
 
 DROP SCHEMA partitioning_schema CASCADE;
 NOTICE:  drop cascades to table part_table
 -- show we don't have pg_dist_partition entries on the workers after dropping the schema
 SELECT run_command_on_workers($$SELECT count(*) FROM  pg_dist_partition where exists(select * from pg_class where pg_class.oid=pg_dist_partition.logicalrelid AND relname ILIKE '%part_table%');$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,0)
  (localhost,57638,t,0)
-(2 rows)
+(1 row)
 
 -- show we don't have pg_dist_object entries on the workers after dropping the schema
 SELECT run_command_on_workers($$SELECT count(*) FROM  pg_dist_object as obj where classid = 1259 AND exists(select * from pg_class where pg_class.oid=obj.objid AND relname ILIKE '%part_table%');$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,0)
  (localhost,57638,t,0)
-(2 rows)
+(1 row)
 
 -- dropping the parent should drop the metadata on the workers
 CREATE SCHEMA partitioning_schema;
 SET search_path TO partitioning_schema;
 CREATE TABLE part_table (
       col timestamp
   ) PARTITION BY RANGE (col);
 CREATE TABLE part_table_1
   PARTITION OF part_table
   FOR VALUES FROM ('2010-01-01') TO ('2015-01-01');
@@ -447,55 +387,51 @@
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 DROP TABLE part_table;
 -- show we don't have pg_dist_partition entries on the workers after dropping the parent
 SELECT run_command_on_workers($$SELECT count(*) FROM  pg_dist_partition where exists(select * from pg_class where pg_class.oid=pg_dist_partition.logicalrelid AND relname ILIKE '%part_table%');$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,0)
  (localhost,57638,t,0)
-(2 rows)
+(1 row)
 
 -- show we don't have pg_dist_object entries on the workers after dropping the parent
 SELECT run_command_on_workers($$SELECT count(*) FROM  pg_dist_object as obj where classid = 1259 AND exists(select * from pg_class where pg_class.oid=obj.objid AND relname ILIKE '%part_table%');$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,0)
  (localhost,57638,t,0)
-(2 rows)
+(1 row)
 
 SET search_path TO partitioning_schema;
 CREATE TABLE part_table (
       col timestamp
   ) PARTITION BY RANGE (col);
 CREATE TABLE part_table_1
   PARTITION OF part_table
   FOR VALUES FROM ('2010-01-01') TO ('2015-01-01');
 SELECT create_distributed_table('part_table', 'col');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 DROP TABLE part_table_1;
 -- show we have pg_dist_partition entries for the parent on the workers after dropping the partition
 SELECT run_command_on_workers($$SELECT count(*) FROM  pg_dist_partition where exists(select * from pg_class where pg_class.oid=pg_dist_partition.logicalrelid AND relname ILIKE '%part_table%');$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,1)
  (localhost,57638,t,1)
-(2 rows)
+(1 row)
 
 -- show we have pg_dist_object entries for the parent on the workers after dropping the partition
 SELECT run_command_on_workers($$SELECT count(*) FROM  pg_dist_object as obj where classid = 1259 AND exists(select * from pg_class where pg_class.oid=obj.objid AND relname ILIKE '%part_table%');$$);
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,1)
  (localhost,57638,t,1)
-(2 rows)
+(1 row)
 
 -- clean-up
 DROP SCHEMA partitioning_schema CASCADE;
 NOTICE:  drop cascades to table part_table
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_fix_partition_shard_index_names.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_fix_partition_shard_index_names.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_fix_partition_shard_index_names.out.modified	2022-11-09 13:38:02.799312500 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_fix_partition_shard_index_names.out.modified	2022-11-09 13:38:02.819312500 +0300
@@ -4,26 +4,21 @@
 -- https://github.com/citusdata/citus/issues/4962
 -- https://github.com/citusdata/citus/issues/5138
 ----------------------------------------------------
 SET citus.next_shard_id TO 910000;
 SET citus.shard_replication_factor TO 1;
 CREATE SCHEMA fix_idx_names;
 SET search_path TO fix_idx_names, public;
 ALTER SEQUENCE pg_catalog.pg_dist_colocationid_seq RESTART 1370000;
 -- stop metadata sync for one of the worker nodes so we test both cases
 SELECT stop_metadata_sync_to_node('localhost', :worker_1_port);
-NOTICE:  dropping metadata on the node (localhost,57637)
- stop_metadata_sync_to_node
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  node (localhost,57637) does not exist
 -- NULL input should automatically return NULL since
 -- fix_partition_shard_index_names is strict
 -- same for worker_fix_partition_shard_index_names
 SELECT fix_partition_shard_index_names(NULL);
  fix_partition_shard_index_names 
 ---------------------------------
  
 (1 row)
 
 SELECT worker_fix_partition_shard_index_names(NULL, NULL, NULL);
@@ -73,81 +68,53 @@
  partition_table_with_very_long_name | partition_table_with_very_long_na_another_col_partition_col_idx
 (3 rows)
 
 \c - - - :worker_1_port
 -- the names are generated correctly
 -- shard id has been appended to all index names which didn't end in shard id
 -- this goes in line with Citus's way of naming indexes of shards: always append shardid to the end
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' AND tablename SIMILAR TO '%\_\d*' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- dist_partitioned_table_910004              | short_910004
- dist_partitioned_table_910006              | short_910006
- p_910012                                   | p_another_col_partition_col_idx_910012
- p_910014                                   | p_another_col_partition_col_idx_910014
- partition_table_with_very_long_name_910008 | partition_table_with_very_long_na_another_col_p_dd884a3b_910008
- partition_table_with_very_long_name_910010 | partition_table_with_very_long_na_another_col_p_dd884a3b_910010
-(6 rows)
+(0 rows)
 
 \c - - - :master_port
 -- this should work properly
 SELECT 1 FROM citus_activate_node('localhost', :worker_1_port);
- ?column?
----------------------------------------------------------------------
-        1
-(1 row)
-
+ERROR:  node at "localhost:57637" does not exist
 \c - - - :worker_1_port
 -- we have no clashes
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- dist_partitioned_table                     | short
- dist_partitioned_table_910004              | short_910004
- dist_partitioned_table_910006              | short_910006
- p                                          | p_another_col_partition_col_idx
- p_910012                                   | p_another_col_partition_col_idx_910012
- p_910014                                   | p_another_col_partition_col_idx_910014
- partition_table_with_very_long_name        | partition_table_with_very_long_na_another_col_partition_col_idx
- partition_table_with_very_long_name_910008 | partition_table_with_very_long_na_another_col_p_dd884a3b_910008
- partition_table_with_very_long_name_910010 | partition_table_with_very_long_na_another_col_p_dd884a3b_910010
-(9 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO fix_idx_names, public;
 -- if we run this command again, the names will not change since shardid is appended to them
 SELECT fix_partition_shard_index_names('dist_partitioned_table'::regclass);
  fix_partition_shard_index_names 
 ---------------------------------
  
 (1 row)
 
 SELECT fix_all_partition_shard_index_names();
  fix_all_partition_shard_index_names 
 -------------------------------------
  dist_partitioned_table
 (1 row)
 
 \c - - - :worker_1_port
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- dist_partitioned_table                     | short
- dist_partitioned_table_910004              | short_910004
- dist_partitioned_table_910006              | short_910006
- p                                          | p_another_col_partition_col_idx
- p_910012                                   | p_another_col_partition_col_idx_910012
- p_910014                                   | p_another_col_partition_col_idx_910014
- partition_table_with_very_long_name        | partition_table_with_very_long_na_another_col_partition_col_idx
- partition_table_with_very_long_name_910008 | partition_table_with_very_long_na_another_col_p_dd884a3b_910008
- partition_table_with_very_long_name_910010 | partition_table_with_very_long_na_another_col_p_dd884a3b_910010
-(9 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO fix_idx_names, public;
 SET citus.shard_replication_factor TO 1;
 SET citus.next_shard_id TO 910020;
 -- if we explicitly create index on partition-to-be table, Citus handles the naming
 -- hence we would have no broken index names
 CREATE TABLE another_partition_table_with_very_long_name (dist_col int, another_col int, partition_col timestamp);
 SELECT create_distributed_table('another_partition_table_with_very_long_name', 'dist_col');
  create_distributed_table 
@@ -175,67 +142,37 @@
  p                                           | p_another_col_partition_col_idx
  partition_table_with_very_long_name         | partition_table_with_very_long_na_another_col_partition_col_idx
  yet_another_partition_table                 | really weird index name !!
 (5 rows)
 
 \c - - - :worker_1_port
 -- notice indexes of shards of another_partition_table_with_very_long_name already have shardid appended to the end
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- another_partition_table_with_very_long_name        | another_partition_table_with_very_another_col_partition_col_idx
- another_partition_table_with_very_long_name_910020 | another_partition_table_with_very_another_col_p_a02939b4_910020
- another_partition_table_with_very_long_name_910022 | another_partition_table_with_very_another_col_p_a02939b4_910022
- dist_partitioned_table                             | short
- dist_partitioned_table_910004                      | short_910004
- dist_partitioned_table_910006                      | short_910006
- p                                                  | p_another_col_partition_col_idx
- p_910012                                           | p_another_col_partition_col_idx_910012
- p_910014                                           | p_another_col_partition_col_idx_910014
- partition_table_with_very_long_name                | partition_table_with_very_long_na_another_col_partition_col_idx
- partition_table_with_very_long_name_910008         | partition_table_with_very_long_na_another_col_p_dd884a3b_910008
- partition_table_with_very_long_name_910010         | partition_table_with_very_long_na_another_col_p_dd884a3b_910010
- yet_another_partition_table                        | really weird index name !!
- yet_another_partition_table_910024                 | really weird index name !!_910024
- yet_another_partition_table_910026                 | really weird index name !!_910026
-(15 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO fix_idx_names, public;
 -- this command would not do anything
 SELECT fix_all_partition_shard_index_names();
  fix_all_partition_shard_index_names 
 -------------------------------------
  dist_partitioned_table
 (1 row)
 
 \c - - - :worker_1_port
 -- names are the same as before
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- another_partition_table_with_very_long_name        | another_partition_table_with_very_another_col_partition_col_idx
- another_partition_table_with_very_long_name_910020 | another_partition_table_with_very_another_col_p_a02939b4_910020
- another_partition_table_with_very_long_name_910022 | another_partition_table_with_very_another_col_p_a02939b4_910022
- dist_partitioned_table                             | short
- dist_partitioned_table_910004                      | short_910004
- dist_partitioned_table_910006                      | short_910006
- p                                                  | p_another_col_partition_col_idx
- p_910012                                           | p_another_col_partition_col_idx_910012
- p_910014                                           | p_another_col_partition_col_idx_910014
- partition_table_with_very_long_name                | partition_table_with_very_long_na_another_col_partition_col_idx
- partition_table_with_very_long_name_910008         | partition_table_with_very_long_na_another_col_p_dd884a3b_910008
- partition_table_with_very_long_name_910010         | partition_table_with_very_long_na_another_col_p_dd884a3b_910010
- yet_another_partition_table                        | really weird index name !!
- yet_another_partition_table_910024                 | really weird index name !!_910024
- yet_another_partition_table_910026                 | really weird index name !!_910026
-(15 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO fix_idx_names, public;
 DROP INDEX short;
 DROP TABLE yet_another_partition_table, another_partition_table_with_very_long_name;
 -- this will create constraint1 index on parent
 SET citus.max_adaptive_executor_pool_size TO 1;
 -- SELECT fix_partition_shard_index_names('dist_partitioned_table') will be executed
 -- automatically at the end of the ADD CONSTRAINT command
 ALTER TABLE dist_partitioned_table ADD CONSTRAINT constraint1 UNIQUE (dist_col, partition_col);
@@ -254,33 +191,21 @@
  p                                   | p_dist_col_partition_col_key
  partition_table_with_very_long_name | partition_table_with_very_long_name_dist_col_partition_col_idx
  partition_table_with_very_long_name | partition_table_with_very_long_name_dist_col_partition_col_key
 (6 rows)
 
 \c - - - :worker_1_port
 -- index names end in shardid for partitions
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' AND tablename SIMILAR TO '%\_\d*' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- dist_partitioned_table_910004              | constraint1_910004
- dist_partitioned_table_910004              | dist_partitioned_table_dist_col_partition_col_idx_910004
- dist_partitioned_table_910006              | constraint1_910006
- dist_partitioned_table_910006              | dist_partitioned_table_dist_col_partition_col_idx_910006
- p_910012                                   | p_dist_col_partition_col_idx_910012
- p_910012                                   | p_dist_col_partition_col_key_910012
- p_910014                                   | p_dist_col_partition_col_idx_910014
- p_910014                                   | p_dist_col_partition_col_key_910014
- partition_table_with_very_long_name_910008 | partition_table_with_very_long_name_dist_col_pa_781a5400_910008
- partition_table_with_very_long_name_910008 | partition_table_with_very_long_name_dist_col_pa_ef25fb77_910008
- partition_table_with_very_long_name_910010 | partition_table_with_very_long_name_dist_col_pa_781a5400_910010
- partition_table_with_very_long_name_910010 | partition_table_with_very_long_name_dist_col_pa_ef25fb77_910010
-(12 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO fix_idx_names, public;
 SET citus.next_shard_id TO 361176;
 ALTER TABLE dist_partitioned_table DROP CONSTRAINT constraint1 CASCADE;
 NOTICE:  drop cascades to constraint fk_table_id_fk_column_fkey on table fk_table
 DROP INDEX dist_partitioned_table_dist_col_partition_col_idx;
 -- try with index on only parent
 -- this is also an invalid index
 -- also try with hash method, not btree
@@ -307,27 +232,21 @@
  another_partition      | another_partition_dist_col_idx
  dist_partitioned_table | short_parent
  p                      | short_child
 (3 rows)
 
 \c - - - :worker_1_port
 -- index names are already correct, including inherited index for another_partition
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' AND tablename SIMILAR TO '%\_\d*' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- another_partition_361176      | another_partition_dist_col_idx_361176
- another_partition_361178      | another_partition_dist_col_idx_361178
- dist_partitioned_table_910004 | short_parent_910004
- dist_partitioned_table_910006 | short_parent_910006
- p_910012                      | short_child_910012
- p_910014                      | short_child_910014
-(6 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO fix_idx_names, public;
 DROP INDEX short_parent;
 DROP INDEX short_child;
 DROP TABLE another_partition;
 -- try with expression indexes
 -- SELECT fix_partition_shard_index_names('dist_partitioned_table') will be executed
 -- automatically at the end of the CREATE INDEX command
 CREATE INDEX expression_index ON dist_partitioned_table ((dist_col || ' ' || another_col));
@@ -346,33 +265,21 @@
  p                                   | p_expr_idx
  partition_table_with_very_long_name | partition_table_with_very_long_name_expr_expr1_idx
  partition_table_with_very_long_name | partition_table_with_very_long_name_expr_idx
 (6 rows)
 
 \c - - - :worker_1_port
 -- we have correct names
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' AND tablename SIMILAR TO '%\_\d*' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- dist_partitioned_table_910004              | expression_index_910004
- dist_partitioned_table_910004              | statistics_on_index_910004
- dist_partitioned_table_910006              | expression_index_910006
- dist_partitioned_table_910006              | statistics_on_index_910006
- p_910012                                   | p_expr_expr1_idx_910012
- p_910012                                   | p_expr_idx_910012
- p_910014                                   | p_expr_expr1_idx_910014
- p_910014                                   | p_expr_idx_910014
- partition_table_with_very_long_name_910008 | partition_table_with_very_long_name_expr_expr1_idx_910008
- partition_table_with_very_long_name_910008 | partition_table_with_very_long_name_expr_idx_910008
- partition_table_with_very_long_name_910010 | partition_table_with_very_long_name_expr_expr1_idx_910010
- partition_table_with_very_long_name_910010 | partition_table_with_very_long_name_expr_idx_910010
-(12 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO fix_idx_names, public;
 -- try with a table with no partitions
 ALTER TABLE dist_partitioned_table DETACH PARTITION p;
 ALTER TABLE dist_partitioned_table DETACH PARTITION partition_table_with_very_long_name;
 DROP TABLE p;
 DROP TABLE partition_table_with_very_long_name;
 -- still dist_partitioned_table has indexes
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' ORDER BY 1, 2;
@@ -386,68 +293,53 @@
 SELECT fix_partition_shard_index_names('dist_partitioned_table'::regclass);
  fix_partition_shard_index_names 
 ---------------------------------
  
 (1 row)
 
 \c - - - :worker_1_port
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' AND tablename SIMILAR TO '%\_\d*' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- dist_partitioned_table_910004 | expression_index_910004
- dist_partitioned_table_910004 | statistics_on_index_910004
- dist_partitioned_table_910006 | expression_index_910006
- dist_partitioned_table_910006 | statistics_on_index_910006
-(4 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO fix_idx_names, public;
 DROP TABLE dist_partitioned_table;
 -- add test with replication factor = 2
 SET citus.shard_replication_factor TO 2;
 SET citus.next_shard_id TO 910050;
 CREATE TABLE dist_partitioned_table (dist_col int, another_col int, partition_col timestamp) PARTITION BY RANGE (partition_col);
 SELECT create_distributed_table('dist_partitioned_table', 'dist_col');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- create a partition with a long name
 CREATE TABLE partition_table_with_very_long_name PARTITION OF dist_partitioned_table FOR VALUES FROM ('2018-01-01') TO ('2019-01-01');
 -- create an index on parent table
 -- SELECT fix_partition_shard_index_names('dist_partitioned_table') will be executed
 -- automatically at the end of the CREATE INDEX command
 CREATE INDEX index_rep_factor_2 ON dist_partitioned_table USING btree (another_col, partition_col);
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' ORDER BY 1, 2;
               tablename              |                            indexname                            
 -------------------------------------+-----------------------------------------------------------------
  dist_partitioned_table              | index_rep_factor_2
  partition_table_with_very_long_name | partition_table_with_very_long_na_another_col_partition_col_idx
 (2 rows)
 
 \c - - - :worker_2_port
 -- index names are correct
 -- shard id has been appended to all index names which didn't end in shard id
 -- this goes in line with Citus's way of naming indexes of shards: always append shardid to the end
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' AND tablename SIMILAR TO '%\_\d*' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- dist_partitioned_table_910050              | index_rep_factor_2_910050
- dist_partitioned_table_910051              | index_rep_factor_2_910051
- dist_partitioned_table_910052              | index_rep_factor_2_910052
- dist_partitioned_table_910053              | index_rep_factor_2_910053
- partition_table_with_very_long_name_910054 | partition_table_with_very_long_na_another_col_p_dd884a3b_910054
- partition_table_with_very_long_name_910055 | partition_table_with_very_long_na_another_col_p_dd884a3b_910055
- partition_table_with_very_long_name_910056 | partition_table_with_very_long_na_another_col_p_dd884a3b_910056
- partition_table_with_very_long_name_910057 | partition_table_with_very_long_na_another_col_p_dd884a3b_910057
-(8 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO fix_idx_names, public;
 -- test with role that is not superuser
 SET client_min_messages TO warning;
 SET citus.enable_ddl_propagation TO off;
 CREATE USER user1;
 RESET client_min_messages;
 RESET citus.enable_ddl_propagation;
 SET ROLE user1;
@@ -479,23 +371,21 @@
 -- we can drop index on detached partition
 -- https://github.com/citusdata/citus/issues/5138
 ALTER TABLE dist_partitioned_table DETACH PARTITION p;
 DROP INDEX p_another_col_partition_col_idx;
 \c - - - :worker_1_port
 -- check that indexes have been renamed
 -- and that index on p has been dropped (it won't appear)
 SELECT tablename, indexname FROM pg_indexes WHERE schemaname = 'fix_idx_names' AND tablename SIMILAR TO '%\_\d*' ORDER BY 1, 2;
  tablename | indexname 
 -----------+-----------
- dist_partitioned_table_910030              | short_910030
- partition_table_with_very_long_name_910031 | partition_table_with_very_long_name_idx_910031
-(2 rows)
+(0 rows)
 
 \c - - - :master_port
 SET search_path TO fix_idx_names, public;
 DROP TABLE dist_partitioned_table;
 SET citus.next_shard_id TO 910040;
 -- test with citus local table
 SET client_min_messages TO WARNING;
 SELECT 1 FROM citus_add_node('localhost', :master_port, groupid=>0);
  ?column? 
 ----------
@@ -543,103 +433,67 @@
 
 CREATE TABLE p1 PARTITION OF parent_table FOR VALUES FROM ('2018-01-01') TO ('2019-01-01');
 CREATE INDEX i1 ON parent_table(dist_col);
 CREATE INDEX i2 ON parent_table(dist_col);
 CREATE INDEX i3 ON parent_table(dist_col);
 SET citus.log_remote_commands TO ON;
 -- only fix i4
 CREATE INDEX i4 ON parent_table(dist_col);
 NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(0, 2021, '2022-11-09 02:38:02.597547-08');
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing SET LOCAL search_path TO fix_idx_names,public;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing SET LOCAL search_path TO fix_idx_names,public;
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing CREATE INDEX i4 ON parent_table(dist_col);
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing CREATE INDEX i4 ON parent_table(dist_col);
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing CREATE  INDEX   i4_915000 ON  fix_idx_names.parent_table_915000 USING btree (dist_col ) 
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SELECT pg_catalog.citus_run_local_command($$SELECT worker_fix_partition_shard_index_names('fix_idx_names.i4_915000'::regclass, 'fix_idx_names.p1_915001', 'p1_dist_col_idx3_915001')$$)
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing PREPARE TRANSACTION 'citus_0_2767_2021_10'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing PREPARE TRANSACTION 'citus_xx_xx_xx_xx'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing COMMIT PREPARED 'citus_xx_xx_xx_xx'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing COMMIT PREPARED 'citus_0_2767_2021_10'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 -- only fix the index backing the pkey
 ALTER TABLE parent_table ADD CONSTRAINT pkey_cst PRIMARY KEY (dist_col, partition_col);
 NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(0, 2022, '2022-11-09 02:38:02.603877-08');
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SET LOCAL search_path TO fix_idx_names,public;
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing SET LOCAL search_path TO fix_idx_names,public;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing ALTER TABLE parent_table ADD CONSTRAINT pkey_cst PRIMARY KEY (dist_col, partition_col);
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing ALTER TABLE parent_table ADD CONSTRAINT pkey_cst PRIMARY KEY (dist_col, partition_col);
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SELECT worker_apply_shard_ddl_command (915000, 'fix_idx_names', 'ALTER TABLE parent_table ADD CONSTRAINT pkey_cst PRIMARY KEY (dist_col, partition_col);')
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SELECT pg_catalog.citus_run_local_command($$SELECT worker_fix_partition_shard_index_names('fix_idx_names.pkey_cst_915000'::regclass, 'fix_idx_names.p1_915001', 'p1_pkey_915001')$$)
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing PREPARE TRANSACTION 'citus_0_2767_2022_11'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing PREPARE TRANSACTION 'citus_xx_xx_xx_xx'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing COMMIT PREPARED 'citus_xx_xx_xx_xx'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing COMMIT PREPARED 'citus_0_2767_2022_11'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 ALTER TABLE parent_table ADD CONSTRAINT unique_cst UNIQUE (dist_col, partition_col);
 NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(0, 2023, '2022-11-09 02:38:02.613406-08');
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SET LOCAL search_path TO fix_idx_names,public;
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing SET LOCAL search_path TO fix_idx_names,public;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing ALTER TABLE parent_table ADD CONSTRAINT unique_cst UNIQUE (dist_col, partition_col);
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing ALTER TABLE parent_table ADD CONSTRAINT unique_cst UNIQUE (dist_col, partition_col);
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SELECT worker_apply_shard_ddl_command (915000, 'fix_idx_names', 'ALTER TABLE parent_table ADD CONSTRAINT unique_cst UNIQUE (dist_col, partition_col);')
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SELECT pg_catalog.citus_run_local_command($$SELECT worker_fix_partition_shard_index_names('fix_idx_names.unique_cst_915000'::regclass, 'fix_idx_names.p1_915001', 'p1_dist_col_partition_col_key_915001')$$)
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing PREPARE TRANSACTION 'citus_0_2767_2023_12'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing PREPARE TRANSACTION 'citus_xx_xx_xx_xx'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing COMMIT PREPARED 'citus_xx_xx_xx_xx'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing COMMIT PREPARED 'citus_0_2767_2023_12'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 RESET citus.log_remote_commands;
 -- we should also be able to alter/drop these indexes
 ALTER INDEX i4 RENAME TO i4_renamed;
 ALTER INDEX p1_dist_col_idx3 RENAME TO p1_dist_col_idx3_renamed;
 ALTER INDEX p1_pkey RENAME TO p1_pkey_renamed;
 ALTER INDEX p1_dist_col_partition_col_key RENAME TO p1_dist_col_partition_col_key_renamed;
 ALTER INDEX p1_dist_col_idx RENAME TO p1_dist_col_idx_renamed;
 -- should be able to create a new partition that is columnar
@@ -649,106 +503,62 @@
 NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(0, 0, '1999-12-31 16:00:00-08');
 DETAIL:  on server postgres@localhost:57638 connectionId: 3
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 DETAIL:  on server postgres@localhost:57638 connectionId: 3
 NOTICE:  issuing CREATE EXTENSION IF NOT EXISTS citus_columnar WITH SCHEMA  pg_catalog VERSION "11.2-1";
 DETAIL:  on server postgres@localhost:57638 connectionId: 3
 NOTICE:  issuing COMMIT
 DETAIL:  on server postgres@localhost:57638 connectionId: 3
 NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(0, 2029, '2022-11-09 02:38:02.646008-08');
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing CREATE EXTENSION IF NOT EXISTS citus_columnar WITH SCHEMA  pg_catalog VERSION "11.2-1";
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing COMMIT
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing WITH distributed_object_data(typetext, objnames, objargs, distargumentindex, colocationid, force_delegation)  AS (VALUES ('extension', ARRAY['citus_columnar']::text[], ARRAY[]::text[], -1, 0, false)) SELECT citus_internal_add_object_metadata(typetext, objnames, objargs, distargumentindex::int, colocationid::int, force_delegation::bool) FROM distributed_object_data;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing WITH distributed_object_data(typetext, objnames, objargs, distargumentindex, colocationid, force_delegation)  AS (VALUES ('extension', ARRAY['citus_columnar']::text[], ARRAY[]::text[], -1, 0, false)) SELECT citus_internal_add_object_metadata(typetext, objnames, objargs, distargumentindex::int, colocationid::int, force_delegation::bool) FROM distributed_object_data;
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SELECT worker_apply_shard_ddl_command (915002, 'fix_idx_names', 'CREATE TABLE fix_idx_names.p2 (dist_col integer NOT NULL, another_col integer, partition_col timestamp without time zone NOT NULL, name text) USING columnar')
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing ALTER TABLE fix_idx_names.p2_915002 SET (columnar.chunk_group_row_limit = 10000, columnar.stripe_row_limit = 150000, columnar.compression_level = 3, columnar.compression = 'zstd');
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SELECT worker_apply_shard_ddl_command (915002, 'fix_idx_names', 'ALTER TABLE fix_idx_names.p2 OWNER TO postgres')
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing CREATE TABLE fix_idx_names.p2 (dist_col integer NOT NULL, another_col integer, partition_col timestamp without time zone NOT NULL, name text) USING columnar
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing CREATE TABLE fix_idx_names.p2 (dist_col integer NOT NULL, another_col integer, partition_col timestamp without time zone NOT NULL, name text) USING columnar
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing ALTER TABLE fix_idx_names.p2 SET (columnar.chunk_group_row_limit = 10000, columnar.stripe_row_limit = 150000, columnar.compression_level = 3, columnar.compression = 'zstd');
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing ALTER TABLE fix_idx_names.p2 SET (columnar.chunk_group_row_limit = 10000, columnar.stripe_row_limit = 150000, columnar.compression_level = 3, columnar.compression = 'zstd');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing ALTER TABLE fix_idx_names.p2 OWNER TO postgres
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing ALTER TABLE fix_idx_names.p2 OWNER TO postgres
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SELECT worker_create_truncate_trigger('fix_idx_names.p2')
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing SELECT worker_create_truncate_trigger('fix_idx_names.p2')
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SELECT citus_internal_add_partition_metadata ('fix_idx_names.p2'::regclass, 'h', 'dist_col', 1370001, 's')
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing SELECT citus_internal_add_partition_metadata ('fix_idx_names.p2'::regclass, 'h', 'dist_col', 1370001, 's')
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing WITH shard_data(relationname, shardid, storagetype, shardminvalue, shardmaxvalue)  AS (VALUES ('fix_idx_names.p2'::regclass, 915002, 't'::"char", '-2147483648', '2147483647')) SELECT citus_internal_add_shard_metadata(relationname, shardid, storagetype, shardminvalue, shardmaxvalue) FROM shard_data;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing WITH shard_data(relationname, shardid, storagetype, shardminvalue, shardmaxvalue)  AS (VALUES ('fix_idx_names.p2'::regclass, 915002, 't'::"char", '-2147483648', '2147483647')) SELECT citus_internal_add_shard_metadata(relationname, shardid, storagetype, shardminvalue, shardmaxvalue) FROM shard_data;
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing WITH placement_data(shardid, shardstate, shardlength, groupid, placementid)  AS (VALUES (915002, 1, 0, 1380007, 2370)) SELECT citus_internal_add_placement_metadata(shardid, shardstate, shardlength, groupid, placementid) FROM placement_data;
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing WITH placement_data(shardid, shardstate, shardlength, groupid, placementid)  AS (VALUES (xxxxxx, xxxxxx, xxxxxx, xxxxxx, xxxxxx)) SELECT citus_internal_add_placement_metadata(shardid, shardstate, shardlength, groupid, placementid) FROM placement_data;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing WITH distributed_object_data(typetext, objnames, objargs, distargumentindex, colocationid, force_delegation)  AS (VALUES ('table', ARRAY['fix_idx_names', 'p2']::text[], ARRAY[]::text[], -1, 0, false)) SELECT citus_internal_add_object_metadata(typetext, objnames, objargs, distargumentindex::int, colocationid::int, force_delegation::bool) FROM distributed_object_data;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing WITH distributed_object_data(typetext, objnames, objargs, distargumentindex, colocationid, force_delegation)  AS (VALUES ('table', ARRAY['fix_idx_names', 'p2']::text[], ARRAY[]::text[], -1, 0, false)) SELECT citus_internal_add_object_metadata(typetext, objnames, objargs, distargumentindex::int, colocationid::int, force_delegation::bool) FROM distributed_object_data;
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing SET LOCAL search_path TO fix_idx_names,public;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing SET LOCAL search_path TO fix_idx_names,public;
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing ALTER TABLE parent_table ATTACH PARTITION p2 FOR VALUES FROM ('2019-01-01') TO ('2020-01-01');
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing ALTER TABLE parent_table ATTACH PARTITION p2 FOR VALUES FROM ('2019-01-01') TO ('2020-01-01');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing SELECT worker_apply_inter_shard_ddl_command (915000, 'fix_idx_names', 915002, 'fix_idx_names', 'ALTER TABLE parent_table ATTACH PARTITION p2 FOR VALUES FROM (''2019-01-01'') TO (''2020-01-01'');')
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing SELECT pg_catalog.citus_run_local_command($$SELECT worker_fix_partition_shard_index_names('fix_idx_names.i1_915000'::regclass, 'fix_idx_names.p2_915002', 'p2_dist_col_idx_915002');SELECT worker_fix_partition_shard_index_names('fix_idx_names.i2_915000'::regclass, 'fix_idx_names.p2_915002', 'p2_dist_col_idx1_915002');SELECT worker_fix_partition_shard_index_names('fix_idx_names.i3_915000'::regclass, 'fix_idx_names.p2_915002', 'p2_dist_col_idx2_915002');SELECT worker_fix_partition_shard_index_names('fix_idx_names.i4_renamed_915000'::regclass, 'fix_idx_names.p2_915002', 'p2_dist_col_idx3_915002');SELECT worker_fix_partition_shard_index_names('fix_idx_names.pkey_cst_915000'::regclass, 'fix_idx_names.p2_915002', 'p2_pkey_915002');SELECT worker_fix_partition_shard_index_names('fix_idx_names.unique_cst_915000'::regclass, 'fix_idx_names.p2_915002', 'p2_dist_col_partition_col_key_915002')$$)
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing PREPARE TRANSACTION 'citus_0_2767_2029_18'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing PREPARE TRANSACTION 'citus_xx_xx_xx_xx'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing COMMIT PREPARED 'citus_xx_xx_xx_xx'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing COMMIT PREPARED 'citus_0_2767_2029_18'
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 RESET citus.log_remote_commands;
 DROP INDEX i4_renamed CASCADE;
 ALTER TABLE parent_table DROP CONSTRAINT pkey_cst CASCADE;
 ALTER TABLE parent_table DROP CONSTRAINT unique_cst CASCADE;
 SET client_min_messages TO WARNING;
 DROP SCHEMA fix_idx_names CASCADE;
 SELECT citus_remove_node('localhost', :master_port);
  citus_remove_node 
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/partition_wise_join.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/partition_wise_join.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/partition_wise_join.out.modified	2022-11-09 13:38:03.259312498 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/partition_wise_join.out.modified	2022-11-09 13:38:03.269312498 +0300
@@ -3,207 +3,167 @@
 SET citus.next_shard_id TO 360147;
 CREATE TABLE partitioning_hash_test(id int, subid int) PARTITION BY HASH(subid);
 CREATE TABLE partitioning_hash_test_0 PARTITION OF partitioning_hash_test FOR VALUES WITH (MODULUS 3, REMAINDER 0);
 CREATE TABLE partitioning_hash_test_1 PARTITION OF partitioning_hash_test FOR VALUES WITH (MODULUS 3, REMAINDER 1);
 INSERT INTO partitioning_hash_test VALUES (1, 2);
 INSERT INTO partitioning_hash_test VALUES (2, 13);
 INSERT INTO partitioning_hash_test VALUES (3, 7);
 INSERT INTO partitioning_hash_test VALUES (4, 4);
 -- distribute partitioned table
 SELECT create_distributed_table('partitioning_hash_test', 'id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$partition_wise_join.partitioning_hash_test_0$$)
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$partition_wise_join.partitioning_hash_test_1$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- test partition-wise join
 CREATE TABLE partitioning_hash_join_test(id int, subid int) PARTITION BY HASH(subid);
 CREATE TABLE partitioning_hash_join_test_0 PARTITION OF partitioning_hash_join_test FOR VALUES WITH (MODULUS 3, REMAINDER 0);
 CREATE TABLE partitioning_hash_join_test_1 PARTITION OF partitioning_hash_join_test FOR VALUES WITH (MODULUS 3, REMAINDER 1);
 CREATE TABLE partitioning_hash_join_test_2 PARTITION OF partitioning_hash_join_test FOR VALUES WITH (MODULUS 3, REMAINDER 2);
 SELECT create_distributed_table('partitioning_hash_join_test', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT success FROM run_command_on_workers('alter system set enable_mergejoin to off');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('alter system set enable_nestloop to off');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('alter system set enable_indexscan to off');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('alter system set enable_indexonlyscan to off');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('alter system set enable_partitionwise_join to off');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('select pg_reload_conf()');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 EXPLAIN (COSTS OFF)
 SELECT * FROM partitioning_hash_test JOIN partitioning_hash_join_test USING (id, subid);
                                                                      QUERY PLAN                                                                      
 -----------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (Citus Adaptive)
-   Task Count: 4
-   Tasks Shown: One of 4
-   ->  Task
-         Node: host=localhost port=xxxxx dbname=regression
-         ->  Hash Join
-               Hash Cond: ((partitioning_hash_join_test.id = partitioning_hash_test.id) AND (partitioning_hash_join_test.subid = partitioning_hash_test.subid))
+ Merge Join
+   Merge Cond: ((partitioning_hash_test.id = partitioning_hash_join_test.id) AND (partitioning_hash_test.subid = partitioning_hash_join_test.subid))
+   ->  Sort
+         Sort Key: partitioning_hash_test.id, partitioning_hash_test.subid
          ->  Append
-                     ->  Seq Scan on partitioning_hash_join_test_0_360163 partitioning_hash_join_test_1
-                     ->  Seq Scan on partitioning_hash_join_test_1_360167 partitioning_hash_join_test_2
-                     ->  Seq Scan on partitioning_hash_join_test_2_360171 partitioning_hash_join_test_3
-               ->  Hash
+               ->  Seq Scan on partitioning_hash_test_0 partitioning_hash_test_1
+               ->  Seq Scan on partitioning_hash_test_1 partitioning_hash_test_2
+   ->  Sort
+         Sort Key: partitioning_hash_join_test.id, partitioning_hash_join_test.subid
          ->  Append
-                           ->  Seq Scan on partitioning_hash_test_0_360151 partitioning_hash_test_1
-                           ->  Seq Scan on partitioning_hash_test_1_360155 partitioning_hash_test_2
-(15 rows)
+               ->  Seq Scan on partitioning_hash_join_test_0 partitioning_hash_join_test_1
+               ->  Seq Scan on partitioning_hash_join_test_1 partitioning_hash_join_test_2
+               ->  Seq Scan on partitioning_hash_join_test_2 partitioning_hash_join_test_3
+(13 rows)
 
 -- set partition-wise join on and parallel to off
 SELECT success FROM run_command_on_workers('alter system set enable_partitionwise_join to on');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('select pg_reload_conf()');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 -- SET enable_partitionwise_join TO on;
 ANALYZE partitioning_hash_test, partitioning_hash_join_test;
 EXPLAIN (COSTS OFF)
 SELECT * FROM partitioning_hash_test JOIN partitioning_hash_join_test USING (id, subid);
                                                                      QUERY PLAN                                                                     
 ----------------------------------------------------------------------------------------------------------------------------------------------------
- Custom Scan (Citus Adaptive)
-   Task Count: 4
-   Tasks Shown: One of 4
-   ->  Task
-         Node: host=localhost port=xxxxx dbname=regression
-         ->  Hash Join
+ Hash Join
    Hash Cond: ((partitioning_hash_test.id = partitioning_hash_join_test.id) AND (partitioning_hash_test.subid = partitioning_hash_join_test.subid))
    ->  Append
-                     ->  Seq Scan on partitioning_hash_test_0_360151 partitioning_hash_test_1
-                     ->  Seq Scan on partitioning_hash_test_1_360155 partitioning_hash_test_2
+         ->  Seq Scan on partitioning_hash_test_0 partitioning_hash_test_1
+         ->  Seq Scan on partitioning_hash_test_1 partitioning_hash_test_2
    ->  Hash
          ->  Append
-                           ->  Seq Scan on partitioning_hash_join_test_0_360163 partitioning_hash_join_test_1
-                           ->  Seq Scan on partitioning_hash_join_test_1_360167 partitioning_hash_join_test_2
-                           ->  Seq Scan on partitioning_hash_join_test_2_360171 partitioning_hash_join_test_3
-(15 rows)
+               ->  Seq Scan on partitioning_hash_join_test_0 partitioning_hash_join_test_1
+               ->  Seq Scan on partitioning_hash_join_test_1 partitioning_hash_join_test_2
+               ->  Seq Scan on partitioning_hash_join_test_2 partitioning_hash_join_test_3
+(10 rows)
 
 -- note that partition-wise joins only work when partition key is in the join
 -- following join does not have that, therefore join will not be pushed down to
 -- partitions
 EXPLAIN (COSTS OFF)
 SELECT * FROM partitioning_hash_test JOIN partitioning_hash_join_test USING (id);
                                         QUERY PLAN                                         
 -------------------------------------------------------------------------------------------
- Custom Scan (Citus Adaptive)
-   Task Count: 4
-   Tasks Shown: One of 4
-   ->  Task
-         Node: host=localhost port=xxxxx dbname=regression
-         ->  Hash Join
+ Hash Join
    Hash Cond: (partitioning_hash_test.id = partitioning_hash_join_test.id)
    ->  Append
-                     ->  Seq Scan on partitioning_hash_test_0_360151 partitioning_hash_test_1
-                     ->  Seq Scan on partitioning_hash_test_1_360155 partitioning_hash_test_2
+         ->  Seq Scan on partitioning_hash_test_0 partitioning_hash_test_1
+         ->  Seq Scan on partitioning_hash_test_1 partitioning_hash_test_2
    ->  Hash
          ->  Append
-                           ->  Seq Scan on partitioning_hash_join_test_0_360163 partitioning_hash_join_test_1
-                           ->  Seq Scan on partitioning_hash_join_test_1_360167 partitioning_hash_join_test_2
-                           ->  Seq Scan on partitioning_hash_join_test_2_360171 partitioning_hash_join_test_3
-(15 rows)
+               ->  Seq Scan on partitioning_hash_join_test_0 partitioning_hash_join_test_1
+               ->  Seq Scan on partitioning_hash_join_test_1 partitioning_hash_join_test_2
+               ->  Seq Scan on partitioning_hash_join_test_2 partitioning_hash_join_test_3
+(10 rows)
 
 -- reset partition-wise join
 SELECT success FROM run_command_on_workers('alter system reset enable_partitionwise_join');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('alter system reset enable_mergejoin');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('alter system reset enable_nestloop');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('alter system reset enable_indexscan');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('alter system reset enable_indexonlyscan');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 SELECT success FROM run_command_on_workers('select pg_reload_conf()');
  success 
 ---------
  t
- t
-(2 rows)
+(1 row)
 
 RESET enable_partitionwise_join;
 DROP SCHEMA partition_wise_join CASCADE;
 NOTICE:  drop cascades to 2 other objects
 DETAIL:  drop cascades to table partitioning_hash_test
 drop cascades to table partitioning_hash_join_test
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/propagate_statistics.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/propagate_statistics.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/propagate_statistics.out.modified	2022-11-09 13:38:04.249312494 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/propagate_statistics.out.modified	2022-11-09 13:38:04.249312494 +0300
@@ -107,140 +107,44 @@
 FROM pg_statistic_ext
 WHERE stxnamespace IN (
 	SELECT oid
 	FROM pg_namespace
 	WHERE nspname IN ('public', 'statistics''Test', 'sc1', 'sc2')
 )
 AND stxname SIMILAR TO '%\_\d+'
 ORDER BY stxname ASC;
  stxname 
 ---------
- neW'Stat_980096
- neW'Stat_980098
- neW'Stat_980100
- neW'Stat_980102
- neW'Stat_980104
- neW'Stat_980106
- neW'Stat_980108
- neW'Stat_980110
- neW'Stat_980112
- neW'Stat_980114
- neW'Stat_980116
- neW'Stat_980118
- neW'Stat_980120
- neW'Stat_980122
- neW'Stat_980124
- neW'Stat_980126
- s1_980000
- s1_980002
- s1_980004
- s1_980006
- s1_980008
- s1_980010
- s1_980012
- s1_980014
- s1_980016
- s1_980018
- s1_980020
- s1_980022
- s1_980024
- s1_980026
- s1_980028
- s1_980030
- s2_980032
- s2_980034
- s2_980036
- s2_980038
- s2_980040
- s2_980042
- s2_980044
- s2_980046
- s2_980048
- s2_980050
- s2_980052
- s2_980054
- s2_980056
- s2_980058
- s2_980060
- s2_980062
- s9_980129
- s9_980131
- s9_980133
- s9_980135
- s9_980137
- s9_980139
- s9_980141
- s9_980143
- s9_980145
- s9_980147
- s9_980149
- s9_980151
- s9_980153
- s9_980155
- s9_980157
- s9_980159
- st1_new_980064
- st1_new_980066
- st1_new_980068
- st1_new_980070
- st1_new_980072
- st1_new_980074
- st1_new_980076
- st1_new_980078
- st1_new_980080
- st1_new_980082
- st1_new_980084
- st1_new_980086
- st1_new_980088
- st1_new_980090
- st1_new_980092
- st1_new_980094
- stats_xy_980161
- stats_xy_980163
- stats_xy_980165
- stats_xy_980167
- stats_xy_980169
- stats_xy_980171
- stats_xy_980173
- stats_xy_980175
- stats_xy_980177
- stats_xy_980179
- stats_xy_980181
- stats_xy_980183
- stats_xy_980185
- stats_xy_980187
- stats_xy_980189
- stats_xy_980191
-(96 rows)
+(0 rows)
 
 SELECT count(DISTINCT stxnamespace)
 FROM pg_statistic_ext
 WHERE stxnamespace IN (
 	SELECT oid
 	FROM pg_namespace
 	WHERE nspname IN ('public', 'statistics''Test', 'sc1', 'sc2')
 )
 AND stxname SIMILAR TO '%\_\d+';
  count 
 -------
-     3
+     0
 (1 row)
 
 SELECT COUNT(DISTINCT stxowner)
 FROM pg_statistic_ext
 WHERE stxnamespace IN (
 	SELECT oid
 	FROM pg_namespace
 	WHERE nspname IN ('public', 'statistics''Test', 'sc1', 'sc2')
 )
 AND stxname SIMILAR TO '%\_\d+';
  count 
 -------
-     3
+     0
 (1 row)
 
 \c - - - :master_port
 SET client_min_messages TO WARNING;
 DROP SCHEMA "statistics'Test" CASCADE;
 DROP SCHEMA test_alter_schema CASCADE;
 DROP SCHEMA sc1 CASCADE;
 DROP SCHEMA sc2 CASCADE;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/pg13_propagate_statistics.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/pg13_propagate_statistics.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/pg13_propagate_statistics.out.modified	2022-11-09 13:38:04.519312493 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/pg13_propagate_statistics.out.modified	2022-11-09 13:38:04.519312493 +0300
@@ -28,85 +28,21 @@
 FROM pg_statistic_ext
 WHERE stxnamespace IN (
 	SELECT oid
 	FROM pg_namespace
 	WHERE nspname IN ('statistics''TestTarget')
 )
 AND stxname SIMILAR TO '%\_\d+'
 ORDER BY stxstattarget, stxrelid::regclass ASC;
  stxstattarget | stxrelid 
 ---------------+----------
-            -1 | "statistics'TestTarget".t1_980000
-            -1 | "statistics'TestTarget".t1_980002
-            -1 | "statistics'TestTarget".t1_980004
-            -1 | "statistics'TestTarget".t1_980006
-            -1 | "statistics'TestTarget".t1_980008
-            -1 | "statistics'TestTarget".t1_980010
-            -1 | "statistics'TestTarget".t1_980012
-            -1 | "statistics'TestTarget".t1_980014
-            -1 | "statistics'TestTarget".t1_980016
-            -1 | "statistics'TestTarget".t1_980018
-            -1 | "statistics'TestTarget".t1_980020
-            -1 | "statistics'TestTarget".t1_980022
-            -1 | "statistics'TestTarget".t1_980024
-            -1 | "statistics'TestTarget".t1_980026
-            -1 | "statistics'TestTarget".t1_980028
-            -1 | "statistics'TestTarget".t1_980030
-             3 | "statistics'TestTarget".t1_980000
-             3 | "statistics'TestTarget".t1_980002
-             3 | "statistics'TestTarget".t1_980004
-             3 | "statistics'TestTarget".t1_980006
-             3 | "statistics'TestTarget".t1_980008
-             3 | "statistics'TestTarget".t1_980010
-             3 | "statistics'TestTarget".t1_980012
-             3 | "statistics'TestTarget".t1_980014
-             3 | "statistics'TestTarget".t1_980016
-             3 | "statistics'TestTarget".t1_980018
-             3 | "statistics'TestTarget".t1_980020
-             3 | "statistics'TestTarget".t1_980022
-             3 | "statistics'TestTarget".t1_980024
-             3 | "statistics'TestTarget".t1_980026
-             3 | "statistics'TestTarget".t1_980028
-             3 | "statistics'TestTarget".t1_980030
-            46 | "statistics'TestTarget".t1_980000
-            46 | "statistics'TestTarget".t1_980002
-            46 | "statistics'TestTarget".t1_980004
-            46 | "statistics'TestTarget".t1_980006
-            46 | "statistics'TestTarget".t1_980008
-            46 | "statistics'TestTarget".t1_980010
-            46 | "statistics'TestTarget".t1_980012
-            46 | "statistics'TestTarget".t1_980014
-            46 | "statistics'TestTarget".t1_980016
-            46 | "statistics'TestTarget".t1_980018
-            46 | "statistics'TestTarget".t1_980020
-            46 | "statistics'TestTarget".t1_980022
-            46 | "statistics'TestTarget".t1_980024
-            46 | "statistics'TestTarget".t1_980026
-            46 | "statistics'TestTarget".t1_980028
-            46 | "statistics'TestTarget".t1_980030
-         10000 | "statistics'TestTarget".t1_980000
-         10000 | "statistics'TestTarget".t1_980002
-         10000 | "statistics'TestTarget".t1_980004
-         10000 | "statistics'TestTarget".t1_980006
-         10000 | "statistics'TestTarget".t1_980008
-         10000 | "statistics'TestTarget".t1_980010
-         10000 | "statistics'TestTarget".t1_980012
-         10000 | "statistics'TestTarget".t1_980014
-         10000 | "statistics'TestTarget".t1_980016
-         10000 | "statistics'TestTarget".t1_980018
-         10000 | "statistics'TestTarget".t1_980020
-         10000 | "statistics'TestTarget".t1_980022
-         10000 | "statistics'TestTarget".t1_980024
-         10000 | "statistics'TestTarget".t1_980026
-         10000 | "statistics'TestTarget".t1_980028
-         10000 | "statistics'TestTarget".t1_980030
-(64 rows)
+(0 rows)
 
 \c - - - :master_port
 -- the first one should log a notice that says statistics object does not exist
 ALTER STATISTICS IF EXISTS stats_that_doesnt_exists SET STATISTICS 0;
 NOTICE:  statistics object "stats_that_doesnt_exists" does not exist, skipping
 -- these three should error out as ALTER STATISTICS syntax doesn't support these with IF EXISTS clause
 -- if output of any of these three changes, we should support them and update the test output here
 ALTER STATISTICS IF EXISTS stats_that_doesnt_exists RENAME TO this_should_error_out;
 ERROR:  syntax error at or near "RENAME"
 ALTER STATISTICS IF EXISTS stats_that_doesnt_exists OWNER TO CURRENT_USER;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/citus_update_table_statistics.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/citus_update_table_statistics.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/citus_update_table_statistics.out.modified	2022-11-09 13:38:04.669312492 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/citus_update_table_statistics.out.modified	2022-11-09 13:38:04.689312492 +0300
@@ -6,166 +6,109 @@
 -- This function updates shardlength, shardminvalue and shardmaxvalue
 --
 SET citus.next_shard_id TO 981000;
 SET citus.next_placement_id TO 982000;
 SET citus.shard_count TO 8;
 SET citus.shard_replication_factor TO 2;
 -- test with a hash-distributed table
 -- here we update only shardlength, not shardminvalue and shardmaxvalue
 CREATE TABLE test_table_statistics_hash (id int);
 SELECT create_distributed_table('test_table_statistics_hash', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- populate table
 INSERT INTO test_table_statistics_hash SELECT i FROM generate_series(0, 10000)i;
 -- originally shardlength (size of the shard) is zero
 SELECT
 	ds.logicalrelid::regclass::text AS tablename,
 	ds.shardid AS shardid,
     dsp.placementid AS placementid,
 	shard_name(ds.logicalrelid, ds.shardid) AS shardname,
     ds.shardminvalue AS shardminvalue,
     ds.shardmaxvalue AS shardmaxvalue
 FROM pg_dist_shard ds JOIN pg_dist_shard_placement dsp USING (shardid)
 WHERE ds.logicalrelid::regclass::text in ('test_table_statistics_hash') AND dsp.shardlength = 0
 ORDER BY 2, 3;
  tablename | shardid | placementid | shardname | shardminvalue | shardmaxvalue 
 -----------+---------+-------------+-----------+---------------+---------------
- test_table_statistics_hash |  981000 |      982000 | test_table_statistics_hash_981000 | -2147483648   | -1610612737
- test_table_statistics_hash |  981000 |      982001 | test_table_statistics_hash_981000 | -2147483648   | -1610612737
- test_table_statistics_hash |  981001 |      982002 | test_table_statistics_hash_981001 | -1610612736   | -1073741825
- test_table_statistics_hash |  981001 |      982003 | test_table_statistics_hash_981001 | -1610612736   | -1073741825
- test_table_statistics_hash |  981002 |      982004 | test_table_statistics_hash_981002 | -1073741824   | -536870913
- test_table_statistics_hash |  981002 |      982005 | test_table_statistics_hash_981002 | -1073741824   | -536870913
- test_table_statistics_hash |  981003 |      982006 | test_table_statistics_hash_981003 | -536870912    | -1
- test_table_statistics_hash |  981003 |      982007 | test_table_statistics_hash_981003 | -536870912    | -1
- test_table_statistics_hash |  981004 |      982008 | test_table_statistics_hash_981004 | 0             | 536870911
- test_table_statistics_hash |  981004 |      982009 | test_table_statistics_hash_981004 | 0             | 536870911
- test_table_statistics_hash |  981005 |      982010 | test_table_statistics_hash_981005 | 536870912     | 1073741823
- test_table_statistics_hash |  981005 |      982011 | test_table_statistics_hash_981005 | 536870912     | 1073741823
- test_table_statistics_hash |  981006 |      982012 | test_table_statistics_hash_981006 | 1073741824    | 1610612735
- test_table_statistics_hash |  981006 |      982013 | test_table_statistics_hash_981006 | 1073741824    | 1610612735
- test_table_statistics_hash |  981007 |      982014 | test_table_statistics_hash_981007 | 1610612736    | 2147483647
- test_table_statistics_hash |  981007 |      982015 | test_table_statistics_hash_981007 | 1610612736    | 2147483647
-(16 rows)
+(0 rows)
 
 -- setting this to on in order to verify that we use a distributed transaction id
 -- to run the size queries from different connections
 -- this is going to help detect deadlocks
 SET citus.log_remote_commands TO ON;
 -- setting this to sequential in order to have a deterministic order
 -- in the output of citus.log_remote_commands
 SET citus.multi_shard_modify_mode TO sequential;
 -- update table statistics and then check that shardlength has changed
 -- but shardminvalue and shardmaxvalue stay the same because this is
 -- a hash distributed table
 SELECT citus_update_table_statistics('test_table_statistics_hash');
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing SELECT 981000 AS shard_id, 'public.test_table_statistics_hash_981000' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981000') UNION ALL SELECT 981001 AS shard_id, 'public.test_table_statistics_hash_981001' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981001') UNION ALL SELECT 981002 AS shard_id, 'public.test_table_statistics_hash_981002' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981002') UNION ALL SELECT 981003 AS shard_id, 'public.test_table_statistics_hash_981003' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981003') UNION ALL SELECT 981004 AS shard_id, 'public.test_table_statistics_hash_981004' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981004') UNION ALL SELECT 981005 AS shard_id, 'public.test_table_statistics_hash_981005' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981005') UNION ALL SELECT 981006 AS shard_id, 'public.test_table_statistics_hash_981006' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981006') UNION ALL SELECT 981007 AS shard_id, 'public.test_table_statistics_hash_981007' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981007') UNION ALL SELECT 0::bigint, NULL::text, 0::bigint;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing SELECT 981000 AS shard_id, 'public.test_table_statistics_hash_981000' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981000') UNION ALL SELECT 981001 AS shard_id, 'public.test_table_statistics_hash_981001' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981001') UNION ALL SELECT 981002 AS shard_id, 'public.test_table_statistics_hash_981002' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981002') UNION ALL SELECT 981003 AS shard_id, 'public.test_table_statistics_hash_981003' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981003') UNION ALL SELECT 981004 AS shard_id, 'public.test_table_statistics_hash_981004' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981004') UNION ALL SELECT 981005 AS shard_id, 'public.test_table_statistics_hash_981005' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981005') UNION ALL SELECT 981006 AS shard_id, 'public.test_table_statistics_hash_981006' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981006') UNION ALL SELECT 981007 AS shard_id, 'public.test_table_statistics_hash_981007' AS shard_name, pg_relation_size('public.test_table_statistics_hash_981007') UNION ALL SELECT 0::bigint, NULL::text, 0::bigint;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing COMMIT
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing COMMIT
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
- citus_update_table_statistics
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  relation test_table_statistics_hash is not distributed
 RESET citus.log_remote_commands;
 RESET citus.multi_shard_modify_mode;
 SELECT
 	ds.logicalrelid::regclass::text AS tablename,
 	ds.shardid AS shardid,
     dsp.placementid AS placementid,
 	shard_name(ds.logicalrelid, ds.shardid) AS shardname,
     ds.shardminvalue as shardminvalue,
     ds.shardmaxvalue as shardmaxvalue
 FROM pg_dist_shard ds JOIN pg_dist_shard_placement dsp USING (shardid)
 WHERE ds.logicalrelid::regclass::text in ('test_table_statistics_hash') AND dsp.shardlength > 0
 ORDER BY 2, 3;
  tablename | shardid | placementid | shardname | shardminvalue | shardmaxvalue 
 -----------+---------+-------------+-----------+---------------+---------------
- test_table_statistics_hash |  981000 |      982000 | test_table_statistics_hash_981000 | -2147483648   | -1610612737
- test_table_statistics_hash |  981000 |      982001 | test_table_statistics_hash_981000 | -2147483648   | -1610612737
- test_table_statistics_hash |  981001 |      982002 | test_table_statistics_hash_981001 | -1610612736   | -1073741825
- test_table_statistics_hash |  981001 |      982003 | test_table_statistics_hash_981001 | -1610612736   | -1073741825
- test_table_statistics_hash |  981002 |      982004 | test_table_statistics_hash_981002 | -1073741824   | -536870913
- test_table_statistics_hash |  981002 |      982005 | test_table_statistics_hash_981002 | -1073741824   | -536870913
- test_table_statistics_hash |  981003 |      982006 | test_table_statistics_hash_981003 | -536870912    | -1
- test_table_statistics_hash |  981003 |      982007 | test_table_statistics_hash_981003 | -536870912    | -1
- test_table_statistics_hash |  981004 |      982008 | test_table_statistics_hash_981004 | 0             | 536870911
- test_table_statistics_hash |  981004 |      982009 | test_table_statistics_hash_981004 | 0             | 536870911
- test_table_statistics_hash |  981005 |      982010 | test_table_statistics_hash_981005 | 536870912     | 1073741823
- test_table_statistics_hash |  981005 |      982011 | test_table_statistics_hash_981005 | 536870912     | 1073741823
- test_table_statistics_hash |  981006 |      982012 | test_table_statistics_hash_981006 | 1073741824    | 1610612735
- test_table_statistics_hash |  981006 |      982013 | test_table_statistics_hash_981006 | 1073741824    | 1610612735
- test_table_statistics_hash |  981007 |      982014 | test_table_statistics_hash_981007 | 1610612736    | 2147483647
- test_table_statistics_hash |  981007 |      982015 | test_table_statistics_hash_981007 | 1610612736    | 2147483647
-(16 rows)
+(0 rows)
 
 -- check with an append-distributed table
 -- here we update shardlength, shardminvalue and shardmaxvalue
 CREATE TABLE test_table_statistics_append (id int);
 SELECT create_distributed_table('test_table_statistics_append', 'id', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('test_table_statistics_append') AS shardid1 \gset
+ERROR:  could only find 1 of 2 possible nodes
 SELECT master_create_empty_shard('test_table_statistics_append') AS shardid2 \gset
+ERROR:  could only find 1 of 2 possible nodes
 COPY test_table_statistics_append FROM PROGRAM 'echo 0 && echo 1 && echo 2 && echo 3' WITH (format 'csv', append_to_shard :shardid1);
+ERROR:  syntax error at or near ":"
 COPY test_table_statistics_append FROM PROGRAM 'echo 4 && echo 5 && echo 6 && echo 7' WITH (format 'csv', append_to_shard :shardid2);
+ERROR:  syntax error at or near ":"
 -- shardminvalue and shardmaxvalue are NULL
 SELECT
 	ds.logicalrelid::regclass::text AS tablename,
 	ds.shardid AS shardid,
     dsp.placementid AS placementid,
 	shard_name(ds.logicalrelid, ds.shardid) AS shardname,
     ds.shardminvalue as shardminvalue,
     ds.shardmaxvalue as shardmaxvalue
 FROM pg_dist_shard ds JOIN pg_dist_shard_placement dsp USING (shardid)
 WHERE ds.logicalrelid::regclass::text in ('test_table_statistics_append')
 ORDER BY 2, 3;
  tablename | shardid | placementid | shardname | shardminvalue | shardmaxvalue 
 -----------+---------+-------------+-----------+---------------+---------------
- test_table_statistics_append |  981008 |      982016 | test_table_statistics_append_981008 |               |
- test_table_statistics_append |  981008 |      982017 | test_table_statistics_append_981008 |               |
- test_table_statistics_append |  981009 |      982018 | test_table_statistics_append_981009 |               |
- test_table_statistics_append |  981009 |      982019 | test_table_statistics_append_981009 |               |
-(4 rows)
+(0 rows)
 
 -- delete some data to change shardminvalues of a shards
 DELETE FROM test_table_statistics_append WHERE id = 0 OR id = 4;
 SET citus.log_remote_commands TO ON;
 SET citus.multi_shard_modify_mode TO sequential;
 -- update table statistics and then check that shardminvalue has changed
 -- shardlength (shardsize) is still 8192 since there is very few data
 SELECT citus_update_table_statistics('test_table_statistics_append');
 NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(0, 2074, '2022-11-09 02:38:04.598512-08');
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing SELECT 981008 AS shard_id, 'public.test_table_statistics_append_981008' AS shard_name, pg_relation_size('public.test_table_statistics_append_981008') UNION ALL SELECT 981009 AS shard_id, 'public.test_table_statistics_append_981009' AS shard_name, pg_relation_size('public.test_table_statistics_append_981009') UNION ALL SELECT 0::bigint, NULL::text, 0::bigint;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing SELECT 981008 AS shard_id, 'public.test_table_statistics_append_981008' AS shard_name, pg_relation_size('public.test_table_statistics_append_981008') UNION ALL SELECT 981009 AS shard_id, 'public.test_table_statistics_append_981009' AS shard_name, pg_relation_size('public.test_table_statistics_append_981009') UNION ALL SELECT 0::bigint, NULL::text, 0::bigint;
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing COMMIT
+NOTICE:  issuing SELECT 0::bigint, NULL::text, 0::bigint;
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
 NOTICE:  issuing COMMIT
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
  citus_update_table_statistics 
 -------------------------------
  
 (1 row)
 
 RESET citus.log_remote_commands;
 RESET citus.multi_shard_modify_mode;
@@ -174,19 +117,15 @@
 	ds.shardid AS shardid,
     dsp.placementid AS placementid,
 	shard_name(ds.logicalrelid, ds.shardid) AS shardname,
     ds.shardminvalue as shardminvalue,
     ds.shardmaxvalue as shardmaxvalue
 FROM pg_dist_shard ds JOIN pg_dist_shard_placement dsp USING (shardid)
 WHERE ds.logicalrelid::regclass::text in ('test_table_statistics_append')
 ORDER BY 2, 3;
  tablename | shardid | placementid | shardname | shardminvalue | shardmaxvalue 
 -----------+---------+-------------+-----------+---------------+---------------
- test_table_statistics_append |  981008 |      982016 | test_table_statistics_append_981008 |               |
- test_table_statistics_append |  981008 |      982017 | test_table_statistics_append_981008 |               |
- test_table_statistics_append |  981009 |      982018 | test_table_statistics_append_981009 |               |
- test_table_statistics_append |  981009 |      982019 | test_table_statistics_append_981009 |               |
-(4 rows)
+(0 rows)
 
 DROP TABLE test_table_statistics_hash, test_table_statistics_append;
 ALTER SYSTEM RESET citus.shard_count;
 ALTER SYSTEM RESET citus.shard_replication_factor;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_join_order_tpch_small.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_join_order_tpch_small.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_join_order_tpch_small.out.modified	2022-11-09 13:38:05.529312488 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_join_order_tpch_small.out.modified	2022-11-09 13:38:05.539312488 +0300
@@ -9,27 +9,29 @@
 EXPLAIN (COSTS OFF)
 SELECT
 	sum(l_extendedprice * l_discount) as revenue
 FROM
 	lineitem
 WHERE
 	l_shipdate >= date '1994-01-01'
 	and l_shipdate < date '1994-01-01' + interval '1 year'
 	and l_discount between 0.06 - 0.01 and 0.06 + 0.01
 	and l_quantity < 24;
-LOG:  join order: [ "lineitem" ]
                                                                 QUERY PLAN                                                                 
 -------------------------------------------------------------------------------------------------------------------------------------------
  Aggregate
-   ->  Custom Scan (Citus Adaptive)
-         explain statements for distributed queries are not enabled
-(3 rows)
+   ->  Bitmap Heap Scan on lineitem
+         Recheck Cond: ((l_shipdate >= '01-01-1994'::date) AND (l_shipdate < 'Sun Jan 01 00:00:00 1995'::timestamp without time zone))
+         Filter: ((l_discount >= 0.05) AND (l_discount <= 0.07) AND (l_quantity < '24'::numeric))
+         ->  Bitmap Index Scan on lineitem_time_index
+               Index Cond: ((l_shipdate >= '01-01-1994'::date) AND (l_shipdate < 'Sun Jan 01 00:00:00 1995'::timestamp without time zone))
+(6 rows)
 
 -- Query #3 from the TPC-H decision support benchmark
 EXPLAIN (COSTS OFF)
 SELECT
 	l_orderkey,
 	sum(l_extendedprice * (1 - l_discount)) as revenue,
 	o_orderdate,
 	o_shippriority
 FROM
 	customer,
@@ -41,28 +43,25 @@
 	AND l_orderkey = o_orderkey
 	AND o_orderdate < date '1995-03-15'
 	AND l_shipdate > date '1995-03-15'
 GROUP BY
 	l_orderkey,
 	o_orderdate,
 	o_shippriority
 ORDER BY
 	revenue DESC,
 	o_orderdate;
-LOG:  join order: [ "orders" ][ reference join "customer" ][ local partition join "lineitem" ]
                           QUERY PLAN                          
 --------------------------------------------------------------
- Sort
-   Sort Key: remote_scan.revenue DESC, remote_scan.o_orderdate
-   ->  Custom Scan (Citus Adaptive)
+ Custom Scan (Citus Adaptive)
    explain statements for distributed queries are not enabled
-(4 rows)
+(2 rows)
 
 -- Query #10 from the TPC-H decision support benchmark
 EXPLAIN (COSTS OFF)
 SELECT
 	c_custkey,
 	c_name,
 	sum(l_extendedprice * (1 - l_discount)) as revenue,
 	c_acctbal,
 	n_name,
 	c_address,
@@ -83,30 +82,25 @@
 GROUP BY
 	c_custkey,
 	c_name,
 	c_acctbal,
 	c_phone,
 	n_name,
 	c_address,
 	c_comment
 ORDER BY
 	revenue DESC;
-LOG:  join order: [ "orders" ][ reference join "customer" ][ reference join "nation" ][ local partition join "lineitem" ]
                           QUERY PLAN                          
 --------------------------------------------------------------
- Sort
-   Sort Key: (sum(remote_scan.revenue)) DESC
-   ->  HashAggregate
-         Group Key: remote_scan.c_custkey, remote_scan.c_name, remote_scan.c_acctbal, remote_scan.c_phone, remote_scan.n_name, remote_scan.c_address, remote_scan.c_comment
-         ->  Custom Scan (Citus Adaptive)
+ Custom Scan (Citus Adaptive)
    explain statements for distributed queries are not enabled
-(6 rows)
+(2 rows)
 
 -- Query #19 from the TPC-H decision support benchmark (modified)
 EXPLAIN (COSTS OFF)
 SELECT
 	sum(l_extendedprice* (1 - l_discount)) as revenue
 FROM
 	lineitem,
 	part
 WHERE
 	(
@@ -125,20 +119,18 @@
 		AND l_shipinstruct = 'DELIVER IN PERSON'
 	)
 	OR
 	(
 		p_partkey = l_partkey
 		AND (p_brand = 'Brand#33' OR p_brand = 'Brand#34' OR p_brand = 'Brand#35')
 		AND l_quantity >= 1
 		AND l_shipmode in ('AIR', 'AIR REG', 'TRUCK')
 		AND l_shipinstruct = 'DELIVER IN PERSON'
 	);
-LOG:  join order: [ "lineitem" ][ reference join "part" ]
                           QUERY PLAN                          
 --------------------------------------------------------------
- Aggregate
-   ->  Custom Scan (Citus Adaptive)
+ Custom Scan (Citus Adaptive)
    explain statements for distributed queries are not enabled
-(3 rows)
+(2 rows)
 
 -- Reset client logging level to its previous value
 SET client_min_messages TO NOTICE;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_join_order_additional.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_join_order_additional.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_join_order_additional.out.modified	2022-11-09 13:38:05.619312488 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_join_order_additional.out.modified	2022-11-09 13:38:05.629312488 +0300
@@ -66,54 +66,67 @@
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SET client_min_messages TO DEBUG2;
 -- The following query checks that we can correctly handle self-joins
 EXPLAIN (COSTS OFF)
 SELECT l1.l_quantity FROM lineitem l1, lineitem l2
 	WHERE l1.l_orderkey = l2.l_orderkey AND l1.l_quantity > 5;
-DEBUG:  Router planner cannot handle multi-shard select queries
-LOG:  join order: [ "lineitem" ][ local partition join "lineitem" ]
-DEBUG:  join prunable for intervals [-2147483648,-1] and [0,2147483647]
-DEBUG:  join prunable for intervals [0,2147483647] and [-2147483648,-1]
                     QUERY PLAN                     
 ---------------------------------------------------
- Custom Scan (Citus Adaptive)
-   explain statements for distributed queries are not enabled
-(2 rows)
+ Merge Join
+   Merge Cond: (l1.l_orderkey = l2.l_orderkey)
+   ->  Sort
+         Sort Key: l1.l_orderkey
+         ->  Seq Scan on lineitem l1
+               Filter: (l_quantity > '5'::numeric)
+   ->  Sort
+         Sort Key: l2.l_orderkey
+         ->  Seq Scan on lineitem l2
+(9 rows)
 
 SET client_min_messages TO LOG;
 -- The following queries check that we correctly handle joins and OR clauses. In
 -- particular, these queries check that we factorize out OR clauses if possible,
 -- and that we default to a cartesian product otherwise.
 EXPLAIN (COSTS OFF)
 SELECT count(*) FROM lineitem, orders
 	WHERE (l_orderkey = o_orderkey AND l_quantity > 5)
 	OR (l_orderkey = o_orderkey AND l_quantity < 10);
-LOG:  join order: [ "lineitem" ][ local partition join "orders" ]
                                      QUERY PLAN                                      
 -------------------------------------------------------------------------------------
  Aggregate
-   ->  Custom Scan (Citus Adaptive)
-         explain statements for distributed queries are not enabled
-(3 rows)
+   ->  Hash Join
+         Hash Cond: (lineitem.l_orderkey = orders.o_orderkey)
+         ->  Seq Scan on lineitem
+               Filter: ((l_quantity > '5'::numeric) OR (l_quantity < '10'::numeric))
+         ->  Hash
+               ->  Seq Scan on orders
+(7 rows)
 
 EXPLAIN (COSTS OFF)
 SELECT l_quantity FROM lineitem, orders
 	WHERE (l_orderkey = o_orderkey OR l_quantity > 5);
-ERROR:  complex joins are only supported when all distributed tables are joined on their distribution columns with equal operator
+                                             QUERY PLAN                                             
+----------------------------------------------------------------------------------------------------
+ Nested Loop
+   Join Filter: ((lineitem.l_orderkey = orders.o_orderkey) OR (lineitem.l_quantity > '5'::numeric))
+   ->  Seq Scan on lineitem
+   ->  Materialize
+         ->  Seq Scan on orders
+(5 rows)
+
 EXPLAIN (COSTS OFF)
 SELECT count(*) FROM orders, lineitem_hash
 	WHERE o_orderkey = l_orderkey;
-LOG:  join order: [ "orders" ][ dual partition join "lineitem_hash" ]
                              QUERY PLAN                             
 --------------------------------------------------------------------
  Aggregate
    ->  Custom Scan (Citus Adaptive)
          explain statements for distributed queries are not enabled
 (3 rows)
 
 -- Verify we handle local joins between two hash-partitioned tables.
 EXPLAIN (COSTS OFF)
 SELECT count(*) FROM orders_hash, lineitem_hash
@@ -136,34 +149,32 @@
  Aggregate
    ->  Custom Scan (Citus Adaptive)
          explain statements for distributed queries are not enabled
 (3 rows)
 
 -- Validate that we don't use a single-partition join method for a hash
 -- re-partitioned table, thus preventing a partition of just the customer table.
 EXPLAIN (COSTS OFF)
 SELECT count(*) FROM orders, lineitem, customer_append
 	WHERE o_custkey = l_partkey AND o_custkey = c_nationkey;
-LOG:  join order: [ "orders" ][ dual partition join "lineitem" ][ dual partition join "customer_append" ]
                              QUERY PLAN                             
 --------------------------------------------------------------------
  Aggregate
    ->  Custom Scan (Citus Adaptive)
          explain statements for distributed queries are not enabled
 (3 rows)
 
 -- Validate that we don't chose a single-partition join method with a
 -- hash-partitioned base table
 EXPLAIN (COSTS OFF)
 SELECT count(*) FROM orders, customer_hash
 	WHERE c_custkey = o_custkey;
-LOG:  join order: [ "orders" ][ dual partition join "customer_hash" ]
                              QUERY PLAN                             
 --------------------------------------------------------------------
  Aggregate
    ->  Custom Scan (Citus Adaptive)
          explain statements for distributed queries are not enabled
 (3 rows)
 
 -- Validate that we can re-partition a hash partitioned table to join with a
 -- range partitioned one.
 EXPLAIN (COSTS OFF)
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_load_more_data.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_load_more_data.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_load_more_data.out.modified	2022-11-09 13:38:05.739312488 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_load_more_data.out.modified	2022-11-09 13:38:05.739312488 +0300
@@ -8,15 +8,21 @@
 \set customer_2_data_file :abs_srcdir '/data/customer.2.data'
 \set customer_3_data_file :abs_srcdir '/data/customer.3.data'
 \set part_more_data_file :abs_srcdir '/data/part.more.data'
 \set client_side_copy_command '\\copy customer FROM ' :'customer_2_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
 \set client_side_copy_command '\\copy customer FROM ' :'customer_3_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
 \set client_side_copy_command '\\copy part FROM ' :'part_more_data_file' ' with delimiter '''|''';'
 :client_side_copy_command
 SELECT master_create_empty_shard('customer_append') AS shardid1 \gset
+ERROR:  could only find 1 of 2 possible nodes
 SELECT master_create_empty_shard('customer_append') AS shardid2 \gset
+ERROR:  could only find 1 of 2 possible nodes
 copy customer_append FROM :'customer_2_data_file' with (delimiter '|', append_to_shard :shardid1);
+ERROR:  syntax error at or near ":"
 copy customer_append FROM :'customer_3_data_file' with (delimiter '|', append_to_shard :shardid2);
+ERROR:  syntax error at or near ":"
 SELECT master_create_empty_shard('part_append') AS shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 copy part_append FROM :'part_more_data_file' with (delimiter '|', append_to_shard :shardid);
+ERROR:  syntax error at or near ":"
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_join_order_tpch_repartition.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_join_order_tpch_repartition.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_join_order_tpch_repartition.out.modified	2022-11-09 13:38:05.839312487 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_join_order_tpch_repartition.out.modified	2022-11-09 13:38:05.849312487 +0300
@@ -14,27 +14,29 @@
 EXPLAIN (COSTS OFF)
 SELECT
 	sum(l_extendedprice * l_discount) as revenue
 FROM
 	lineitem
 WHERE
 	l_shipdate >= date '1994-01-01'
 	and l_shipdate < date '1994-01-01' + interval '1 year'
 	and l_discount between 0.06 - 0.01 and 0.06 + 0.01
 	and l_quantity < 24;
-LOG:  join order: [ "lineitem" ]
                                                                 QUERY PLAN                                                                 
 -------------------------------------------------------------------------------------------------------------------------------------------
  Aggregate
-   ->  Custom Scan (Citus Adaptive)
-         explain statements for distributed queries are not enabled
-(3 rows)
+   ->  Bitmap Heap Scan on lineitem
+         Recheck Cond: ((l_shipdate >= '01-01-1994'::date) AND (l_shipdate < 'Sun Jan 01 00:00:00 1995'::timestamp without time zone))
+         Filter: ((l_discount >= 0.05) AND (l_discount <= 0.07) AND (l_quantity < '24'::numeric))
+         ->  Bitmap Index Scan on lineitem_time_index
+               Index Cond: ((l_shipdate >= '01-01-1994'::date) AND (l_shipdate < 'Sun Jan 01 00:00:00 1995'::timestamp without time zone))
+(6 rows)
 
 -- Query #3 from the TPC-H decision support benchmark
 EXPLAIN (COSTS OFF)
 SELECT
 	l_orderkey,
 	sum(l_extendedprice * (1 - l_discount)) as revenue,
 	o_orderdate,
 	o_shippriority
 FROM
 	customer_append,
@@ -46,21 +48,20 @@
 	AND l_orderkey = o_orderkey
 	AND o_orderdate < date '1995-03-15'
 	AND l_shipdate > date '1995-03-15'
 GROUP BY
 	l_orderkey,
 	o_orderdate,
 	o_shippriority
 ORDER BY
 	revenue DESC,
 	o_orderdate;
-LOG:  join order: [ "orders" ][ local partition join "lineitem" ][ dual partition join "customer_append" ]
                                            QUERY PLAN                                           
 ------------------------------------------------------------------------------------------------
  Sort
    Sort Key: (sum(remote_scan.revenue)) DESC, remote_scan.o_orderdate
    ->  HashAggregate
          Group Key: remote_scan.l_orderkey, remote_scan.o_orderdate, remote_scan.o_shippriority
          ->  Custom Scan (Citus Adaptive)
                explain statements for distributed queries are not enabled
 (6 rows)
 
@@ -90,21 +91,20 @@
 GROUP BY
 	c_custkey,
 	c_name,
 	c_acctbal,
 	c_phone,
 	n_name,
 	c_address,
 	c_comment
 ORDER BY
 	revenue DESC;
-LOG:  join order: [ "orders" ][ local partition join "lineitem" ][ dual partition join "customer_append" ][ reference join "nation" ]
                              QUERY PLAN                             
 --------------------------------------------------------------------
  Sort
    Sort Key: remote_scan.revenue DESC
    ->  Custom Scan (Citus Adaptive)
          explain statements for distributed queries are not enabled
 (4 rows)
 
 -- Query #19 from the TPC-H decision support benchmark (modified)
 EXPLAIN (COSTS OFF)
@@ -130,41 +130,32 @@
 		AND l_shipinstruct = 'DELIVER IN PERSON'
 	)
 	OR
 	(
 		p_partkey = l_partkey
 		AND (p_brand = 'Brand#33' OR p_brand = 'Brand#34' OR p_brand = 'Brand#35')
 		AND l_quantity >= 1
 		AND l_shipmode in ('AIR', 'AIR REG', 'TRUCK')
 		AND l_shipinstruct = 'DELIVER IN PERSON'
 	);
-LOG:  join order: [ "lineitem" ][ dual partition join "part_append" ]
                              QUERY PLAN                             
 --------------------------------------------------------------------
  Aggregate
    ->  Custom Scan (Citus Adaptive)
          explain statements for distributed queries are not enabled
 (3 rows)
 
 -- Query to test multiple re-partition jobs in a single query
 EXPLAIN (COSTS OFF)
 SELECT
 	l_partkey, count(*)
 FROM
 	lineitem, part_append, orders, customer_append
 WHERE
 	l_orderkey = o_orderkey AND
 	l_partkey = p_partkey AND
 	c_custkey = o_custkey
 GROUP BY
       l_partkey;
-LOG:  join order: [ "lineitem" ][ local partition join "orders" ][ dual partition join "part_append" ][ dual partition join "customer_append" ]
-                             QUERY PLAN
----------------------------------------------------------------------
- HashAggregate
-   Group Key: remote_scan.l_partkey
-   ->  Custom Scan (Citus Adaptive)
-         explain statements for distributed queries are not enabled
-(4 rows)
-
+ERROR:  complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
 -- Reset client logging level to its previous value
 SET client_min_messages TO NOTICE;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_repartition_join_planning.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_repartition_join_planning.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_repartition_join_planning.out.modified	2022-11-09 13:38:06.009312486 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_repartition_join_planning.out.modified	2022-11-09 13:38:06.009312486 +0300
@@ -21,31 +21,25 @@
 );
 DROP TABLE IF EXISTS repartition_join.stock;
 NOTICE:  table "stock" does not exist, skipping
 CREATE TABLE stock (
   s_w_id int NOT NULL,
   s_i_id int NOT NULL,
   s_quantity decimal(4,0) NOT NULL,
   PRIMARY KEY (s_w_id,s_i_id)
 );
 SELECT create_distributed_table('order_line','ol_w_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT create_distributed_table('stock','s_w_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 BEGIN;
 SET client_min_messages TO DEBUG;
 -- Debug4 log messages display jobIds within them. We explicitly set the jobId
 -- sequence here so that the regression output becomes independent of the number
 -- of jobs executed prior to running this test.
 -- Multi-level repartition join to verify our projection columns are correctly
 -- referenced and propagated across multiple repartition jobs. The test also
 -- validates that only the minimal necessary projection columns are transferred
 -- between jobs.
 SELECT
@@ -57,325 +51,82 @@
 	l_partkey = p_partkey AND
 	c_custkey = o_custkey AND
         (l_quantity > 5.0 OR l_extendedprice > 1200.0) AND
         p_size > 8 AND o_totalprice > 10.0 AND
         c_acctbal < 5000.0 AND l_partkey < 1000
 GROUP BY
 	l_partkey, o_orderkey
 ORDER BY
 	l_partkey, o_orderkey;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for intervals [-2147483648,-1] and [0,2147483647]
-DEBUG:  join prunable for intervals [0,2147483647] and [-2147483648,-1]
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 13
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 18
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 23
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 28
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
- l_partkey | o_orderkey | count
----------------------------------------------------------------------
-        18 |      12005 |     1
-        79 |       5121 |     1
-        91 |       2883 |     1
-       222 |       9413 |     1
-       278 |       1287 |     1
-       309 |       2374 |     1
-       318 |        321 |     1
-       321 |       5984 |     1
-       337 |      10403 |     1
-       350 |      13698 |     1
-       358 |       4323 |     1
-       364 |       9347 |     1
-       416 |        640 |     1
-       426 |      10855 |     1
-       450 |         35 |     1
-       484 |       3843 |     1
-       504 |      14566 |     1
-       510 |      13569 |     1
-       532 |       3175 |     1
-       641 |        134 |     1
-       669 |      10944 |     1
-       716 |       2885 |     1
-       738 |       4355 |     1
-       802 |       2534 |     1
-       824 |       9287 |     1
-       864 |       3175 |     1
-       957 |       4293 |     1
-       960 |      10980 |     1
-       963 |       4580 |     1
-(29 rows)
-
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 1_1 for subquery SELECT l_orderkey, l_partkey, l_quantity, l_extendedprice FROM public.lineitem WHERE (((l_quantity OPERATOR(pg_catalog.>) 5.0) OR (l_extendedprice OPERATOR(pg_catalog.>) 1200.0)) AND (l_partkey OPERATOR(pg_catalog.<) 1000))
+DEBUG:  Wrapping relation "orders" to a subquery
+DEBUG:  generating subplan 1_2 for subquery SELECT o_orderkey, o_custkey, o_totalprice FROM public.orders WHERE (o_totalprice OPERATOR(pg_catalog.>) 10.0)
+DEBUG:  Plan 1 query after replacing subqueries and CTEs: SELECT lineitem.l_partkey, orders.o_orderkey, count(*) AS count FROM (SELECT lineitem_1.l_orderkey, lineitem_1.l_partkey, NULL::integer AS l_suppkey, NULL::integer AS l_linenumber, lineitem_1.l_quantity, lineitem_1.l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_orderkey, intermediate_result.l_partkey, intermediate_result.l_quantity, intermediate_result.l_extendedprice FROM read_intermediate_result('1_1'::text, 'binary'::citus_copy_format) intermediate_result(l_orderkey bigint, l_partkey integer, l_quantity numeric(15,2), l_extendedprice numeric(15,2))) lineitem_1) lineitem, public.part_append, (SELECT orders_1.o_orderkey, orders_1.o_custkey, NULL::character(1) AS o_orderstatus, orders_1.o_totalprice, NULL::date AS o_orderdate, NULL::character(15) AS o_orderpriority, NULL::character(15) AS o_clerk, NULL::integer AS o_shippriority, NULL::character varying(79) AS o_comment FROM (SELECT intermediate_result.o_orderkey, intermediate_result.o_custkey, intermediate_result.o_totalprice FROM read_intermediate_result('1_2'::text, 'binary'::citus_copy_format) intermediate_result(o_orderkey bigint, o_custkey integer, o_totalprice numeric(15,2))) orders_1) orders, public.customer_append WHERE ((lineitem.l_orderkey OPERATOR(pg_catalog.=) orders.o_orderkey) AND (lineitem.l_partkey OPERATOR(pg_catalog.=) part_append.p_partkey) AND (customer_append.c_custkey OPERATOR(pg_catalog.=) orders.o_custkey) AND ((lineitem.l_quantity OPERATOR(pg_catalog.>) 5.0) OR (lineitem.l_extendedprice OPERATOR(pg_catalog.>) 1200.0)) AND (part_append.p_size OPERATOR(pg_catalog.>) 8) AND (orders.o_totalprice OPERATOR(pg_catalog.>) 10.0) AND (customer_append.c_acctbal OPERATOR(pg_catalog.<) 5000.0) AND (lineitem.l_partkey OPERATOR(pg_catalog.<) 1000)) GROUP BY lineitem.l_partkey, orders.o_orderkey ORDER BY lineitem.l_partkey, orders.o_orderkey
+DEBUG:  Router planner does not support append-partitioned tables.
+ERROR:  complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
 SELECT
 	l_partkey, o_orderkey, count(*)
 FROM
 	lineitem, orders
 WHERE
 	l_suppkey = o_shippriority AND
         l_quantity < 5.0 AND o_totalprice <> 4.0
 GROUP BY
 	l_partkey, o_orderkey
 ORDER BY
 	l_partkey, o_orderkey;
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 12
- l_partkey | o_orderkey | count
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- Check that grouping by primary key allows o_shippriority to be in the target list
 SELECT
 	o_orderkey, o_shippriority, count(*)
 FROM
 	lineitem, orders
 WHERE
 	l_suppkey = o_shippriority
 GROUP BY
 	o_orderkey
 ORDER BY
 	o_orderkey;
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 12
- o_orderkey | o_shippriority | count
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- Check that grouping by primary key allows o_shippriority to be in the target
 -- list
 -- Postgres removes o_shippriority from the group by clause here
 SELECT
 	o_orderkey, o_shippriority, count(*)
 FROM
 	lineitem, orders
 WHERE
 	l_suppkey = o_shippriority
 GROUP BY
 	o_orderkey, o_shippriority
 ORDER BY
 	o_orderkey;
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 12
- o_orderkey | o_shippriority | count
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- Check that calling any_value manually works as well
 SELECT
 	o_orderkey, any_value(o_shippriority)
 FROM
 	lineitem, orders
 WHERE
 	l_suppkey = o_shippriority
 GROUP BY
 	o_orderkey, o_shippriority
 ORDER BY
 	o_orderkey;
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 12
- o_orderkey | any_value
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- Check that grouping by primary key allows s_quantity to be in the having
 -- list
 -- Postgres removes s_quantity from the group by clause here
 select  s_i_id
     from  stock, order_line
     where ol_i_id=s_i_id
     group by s_i_id, s_w_id, s_quantity
     having   s_quantity > random()
 ;
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 5
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 5
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 10
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 10
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 15
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 15
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 20
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 20
- s_i_id
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- Reset client logging level to its previous value
 SET client_min_messages TO NOTICE;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 drop schema repartition_join;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_repartition_join_pruning.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_repartition_join_pruning.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_repartition_join_pruning.out.modified	2022-11-09 13:38:06.119312486 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_repartition_join_pruning.out.modified	2022-11-09 13:38:06.129312486 +0300
@@ -7,550 +7,339 @@
 SET citus.enable_repartition_joins to ON;
 -- Single range-repartition join to test join-pruning behaviour.
 EXPLAIN (COSTS OFF)
 SELECT
 	count(*)
 FROM
 	orders, customer_append
 WHERE
 	o_custkey = c_custkey;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
+DEBUG:  Wrapping relation "orders" to a subquery
+DEBUG:  generating subplan 1_1 for subquery SELECT o_custkey FROM public.orders WHERE true
+DEBUG:  Plan 1 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT NULL::bigint AS o_orderkey, orders_1.o_custkey, NULL::character(1) AS o_orderstatus, NULL::numeric(15,2) AS o_totalprice, NULL::date AS o_orderdate, NULL::character(15) AS o_orderpriority, NULL::character(15) AS o_clerk, NULL::integer AS o_shippriority, NULL::character varying(79) AS o_comment FROM (SELECT intermediate_result.o_custkey FROM read_intermediate_result('1_1'::text, 'binary'::citus_copy_format) intermediate_result(o_custkey integer)) orders_1) orders, public.customer_append WHERE (orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey)
+DEBUG:  Router planner does not support append-partitioned tables.
               QUERY PLAN              
 --------------------------------------
  Aggregate
    ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-         Tasks Shown: None, not supported for re-partition queries
-         ->  MapMergeJob
-               Map Task Count: 2
-               Merge Task Count: 4
-         ->  MapMergeJob
-               Map Task Count: 3
-               Merge Task Count: 4
-(10 rows)
+         ->  Distributed Subplan 1_1
+               ->  Seq Scan on orders
+         Task Count: 0
+         Tasks Shown: All
+(6 rows)
 
 SELECT
 	count(*)
 FROM
 	orders, customer_append
 WHERE
 	o_custkey = c_custkey;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
+DEBUG:  Wrapping relation "orders" to a subquery
+DEBUG:  generating subplan 2_1 for subquery SELECT o_custkey FROM public.orders WHERE true
+DEBUG:  Plan 2 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT NULL::bigint AS o_orderkey, orders_1.o_custkey, NULL::character(1) AS o_orderstatus, NULL::numeric(15,2) AS o_totalprice, NULL::date AS o_orderdate, NULL::character(15) AS o_orderpriority, NULL::character(15) AS o_clerk, NULL::integer AS o_shippriority, NULL::character varying(79) AS o_comment FROM (SELECT intermediate_result.o_custkey FROM read_intermediate_result('2_1'::text, 'binary'::citus_copy_format) intermediate_result(o_custkey integer)) orders_1) orders, public.customer_append WHERE (orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey)
+DEBUG:  Router planner does not support append-partitioned tables.
  count 
 -------
-  2985
+     0
 (1 row)
 
 -- Single range-repartition join with a selection clause on the partitioned
 -- table to test the case when all map tasks are pruned away.
 EXPLAIN (COSTS OFF)
 SELECT
 	count(*)
 FROM
 	orders, customer_append
 WHERE
 	o_custkey = c_custkey AND
 	o_orderkey < 0;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
+DEBUG:  Wrapping relation "orders" to a subquery
+DEBUG:  generating subplan 3_1 for subquery SELECT o_orderkey, o_custkey FROM public.orders WHERE (o_orderkey OPERATOR(pg_catalog.<) 0)
+DEBUG:  Plan 3 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT orders_1.o_orderkey, orders_1.o_custkey, NULL::character(1) AS o_orderstatus, NULL::numeric(15,2) AS o_totalprice, NULL::date AS o_orderdate, NULL::character(15) AS o_orderpriority, NULL::character(15) AS o_clerk, NULL::integer AS o_shippriority, NULL::character varying(79) AS o_comment FROM (SELECT intermediate_result.o_orderkey, intermediate_result.o_custkey FROM read_intermediate_result('3_1'::text, 'binary'::citus_copy_format) intermediate_result(o_orderkey bigint, o_custkey integer)) orders_1) orders, public.customer_append WHERE ((orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey) AND (orders.o_orderkey OPERATOR(pg_catalog.<) 0))
+DEBUG:  Router planner does not support append-partitioned tables.
                   QUERY PLAN                  
 ----------------------------------------------
  Aggregate
    ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-         Tasks Shown: None, not supported for re-partition queries
-         ->  MapMergeJob
-               Map Task Count: 2
-               Merge Task Count: 4
-         ->  MapMergeJob
-               Map Task Count: 3
-               Merge Task Count: 4
-(10 rows)
+         ->  Distributed Subplan 3_1
+               ->  Seq Scan on orders
+                     Filter: (o_orderkey < 0)
+         Task Count: 0
+         Tasks Shown: All
+(7 rows)
 
 SELECT
 	count(*)
 FROM
 	orders, customer_append
 WHERE
 	o_custkey = c_custkey AND
 	o_orderkey < 0 AND o_orderkey > 0;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
+DEBUG:  Wrapping relation "orders" to a subquery
+DEBUG:  generating subplan 4_1 for subquery SELECT o_orderkey, o_custkey FROM public.orders WHERE ((o_orderkey OPERATOR(pg_catalog.<) 0) AND (o_orderkey OPERATOR(pg_catalog.>) 0))
+DEBUG:  Plan 4 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT orders_1.o_orderkey, orders_1.o_custkey, NULL::character(1) AS o_orderstatus, NULL::numeric(15,2) AS o_totalprice, NULL::date AS o_orderdate, NULL::character(15) AS o_orderpriority, NULL::character(15) AS o_clerk, NULL::integer AS o_shippriority, NULL::character varying(79) AS o_comment FROM (SELECT intermediate_result.o_orderkey, intermediate_result.o_custkey FROM read_intermediate_result('4_1'::text, 'binary'::citus_copy_format) intermediate_result(o_orderkey bigint, o_custkey integer)) orders_1) orders, public.customer_append WHERE ((orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey) AND (orders.o_orderkey OPERATOR(pg_catalog.<) 0) AND (orders.o_orderkey OPERATOR(pg_catalog.>) 0))
+DEBUG:  Router planner does not support append-partitioned tables.
  count 
 -------
      0
 (1 row)
 
 -- Single range-repartition join with a selection clause on the base table to
 -- test the case when all sql tasks are pruned away.
 EXPLAIN (COSTS OFF)
 SELECT
 	count(*)
 FROM
 	orders, customer_append
 WHERE
 	o_custkey = c_custkey AND
 	c_custkey < 0 AND c_custkey > 0;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
+DEBUG:  Wrapping relation "orders" to a subquery
+DEBUG:  generating subplan 5_1 for subquery SELECT o_custkey FROM public.orders WHERE true
+DEBUG:  Plan 5 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT NULL::bigint AS o_orderkey, orders_1.o_custkey, NULL::character(1) AS o_orderstatus, NULL::numeric(15,2) AS o_totalprice, NULL::date AS o_orderdate, NULL::character(15) AS o_orderpriority, NULL::character(15) AS o_clerk, NULL::integer AS o_shippriority, NULL::character varying(79) AS o_comment FROM (SELECT intermediate_result.o_custkey FROM read_intermediate_result('5_1'::text, 'binary'::citus_copy_format) intermediate_result(o_custkey integer)) orders_1) orders, public.customer_append WHERE ((orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey) AND (customer_append.c_custkey OPERATOR(pg_catalog.<) 0) AND (customer_append.c_custkey OPERATOR(pg_catalog.>) 0))
+DEBUG:  Router planner does not support append-partitioned tables.
               QUERY PLAN              
 --------------------------------------
  Aggregate
    ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-         Tasks Shown: None, not supported for re-partition queries
-         ->  MapMergeJob
-               Map Task Count: 2
-               Merge Task Count: 4
-         ->  MapMergeJob
-               Map Task Count: 3
-               Merge Task Count: 4
-(10 rows)
+         ->  Distributed Subplan 5_1
+               ->  Seq Scan on orders
+         Task Count: 0
+         Tasks Shown: All
+(6 rows)
 
 SELECT
 	count(*)
 FROM
 	orders, customer_append
 WHERE
 	o_custkey = c_custkey AND
 	c_custkey < 0 AND c_custkey > 0;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
+DEBUG:  Wrapping relation "orders" to a subquery
+DEBUG:  generating subplan 6_1 for subquery SELECT o_custkey FROM public.orders WHERE true
+DEBUG:  Plan 6 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT NULL::bigint AS o_orderkey, orders_1.o_custkey, NULL::character(1) AS o_orderstatus, NULL::numeric(15,2) AS o_totalprice, NULL::date AS o_orderdate, NULL::character(15) AS o_orderpriority, NULL::character(15) AS o_clerk, NULL::integer AS o_shippriority, NULL::character varying(79) AS o_comment FROM (SELECT intermediate_result.o_custkey FROM read_intermediate_result('6_1'::text, 'binary'::citus_copy_format) intermediate_result(o_custkey integer)) orders_1) orders, public.customer_append WHERE ((orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey) AND (customer_append.c_custkey OPERATOR(pg_catalog.<) 0) AND (customer_append.c_custkey OPERATOR(pg_catalog.>) 0))
+DEBUG:  Router planner does not support append-partitioned tables.
  count 
 -------
      0
 (1 row)
 
 -- Dual hash-repartition join test case. Note that this query doesn't produce
 -- meaningful results and is only to test hash-partitioning of two large tables
 -- on non-partition columns.
 EXPLAIN (COSTS OFF)
 SELECT
 	count(*)
 FROM
 	lineitem, customer_append
 WHERE
 	l_partkey = c_nationkey;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 7_1 for subquery SELECT l_partkey FROM public.lineitem WHERE true
+DEBUG:  Plan 7 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, NULL::integer AS l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey FROM read_intermediate_result('7_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer)) lineitem_1) lineitem, public.customer_append WHERE (lineitem.l_partkey OPERATOR(pg_catalog.=) customer_append.c_nationkey)
+DEBUG:  Router planner does not support append-partitioned tables.
                QUERY PLAN               
 ----------------------------------------
  Aggregate
    ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-         Tasks Shown: None, not supported for re-partition queries
-         ->  MapMergeJob
-               Map Task Count: 2
-               Merge Task Count: 4
-         ->  MapMergeJob
-               Map Task Count: 3
-               Merge Task Count: 4
-(10 rows)
+         ->  Distributed Subplan 7_1
+               ->  Seq Scan on lineitem
+         Task Count: 0
+         Tasks Shown: All
+(6 rows)
 
 SELECT
 	count(*)
 FROM
 	lineitem, customer_append
 WHERE
 	l_partkey = c_nationkey;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 8_1 for subquery SELECT l_partkey FROM public.lineitem WHERE true
+DEBUG:  Plan 8 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, NULL::integer AS l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer)) lineitem_1) lineitem, public.customer_append WHERE (lineitem.l_partkey OPERATOR(pg_catalog.=) customer_append.c_nationkey)
+DEBUG:  Router planner does not support append-partitioned tables.
  count 
 -------
-   125
+     0
 (1 row)
 
 -- Dual hash-repartition join with a selection clause on one of the tables to
 -- test the case when all map tasks are pruned away.
 EXPLAIN (COSTS OFF)
 SELECT
 	count(*)
 FROM
 	lineitem, customer_append
 WHERE
 	l_partkey = c_nationkey AND
 	l_orderkey < 0 AND l_orderkey > 0;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 9_1 for subquery SELECT l_orderkey, l_partkey FROM public.lineitem WHERE ((l_orderkey OPERATOR(pg_catalog.<) 0) AND (l_orderkey OPERATOR(pg_catalog.>) 0))
+DEBUG:  Plan 9 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT lineitem_1.l_orderkey, lineitem_1.l_partkey, NULL::integer AS l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_orderkey, intermediate_result.l_partkey FROM read_intermediate_result('9_1'::text, 'binary'::citus_copy_format) intermediate_result(l_orderkey bigint, l_partkey integer)) lineitem_1) lineitem, public.customer_append WHERE ((lineitem.l_partkey OPERATOR(pg_catalog.=) customer_append.c_nationkey) AND (lineitem.l_orderkey OPERATOR(pg_catalog.<) 0) AND (lineitem.l_orderkey OPERATOR(pg_catalog.>) 0))
+DEBUG:  Router planner does not support append-partitioned tables.
                                   QUERY PLAN                                   
 -------------------------------------------------------------------------------
  Aggregate
    ->  Custom Scan (Citus Adaptive)
-         Task Count: 4
-         Tasks Shown: None, not supported for re-partition queries
-         ->  MapMergeJob
-               Map Task Count: 2
-               Merge Task Count: 4
-         ->  MapMergeJob
-               Map Task Count: 3
-               Merge Task Count: 4
-(10 rows)
+         ->  Distributed Subplan 9_1
+               ->  Bitmap Heap Scan on lineitem
+                     Recheck Cond: ((l_orderkey < 0) AND (l_orderkey > 0))
+                     ->  Bitmap Index Scan on lineitem_pkey
+                           Index Cond: ((l_orderkey < 0) AND (l_orderkey > 0))
+         Task Count: 0
+         Tasks Shown: All
+(9 rows)
 
 SELECT
 	count(*)
 FROM
 	lineitem, customer_append
 WHERE
 	l_partkey = c_nationkey AND
 	l_orderkey < 0 AND l_orderkey > 0;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 10_1 for subquery SELECT l_orderkey, l_partkey FROM public.lineitem WHERE ((l_orderkey OPERATOR(pg_catalog.<) 0) AND (l_orderkey OPERATOR(pg_catalog.>) 0))
+DEBUG:  Plan 10 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT lineitem_1.l_orderkey, lineitem_1.l_partkey, NULL::integer AS l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_orderkey, intermediate_result.l_partkey FROM read_intermediate_result('10_1'::text, 'binary'::citus_copy_format) intermediate_result(l_orderkey bigint, l_partkey integer)) lineitem_1) lineitem, public.customer_append WHERE ((lineitem.l_partkey OPERATOR(pg_catalog.=) customer_append.c_nationkey) AND (lineitem.l_orderkey OPERATOR(pg_catalog.<) 0) AND (lineitem.l_orderkey OPERATOR(pg_catalog.>) 0))
+DEBUG:  Router planner does not support append-partitioned tables.
  count 
 -------
      0
 (1 row)
 
 -- Test cases with false in the WHERE clause
 EXPLAIN (COSTS OFF)
 SELECT
 	o_orderkey
 FROM
 	orders INNER JOIN customer_append ON (o_custkey = c_custkey)
 WHERE
 	false;
 DEBUG:  Router planner does not support append-partitioned tables.
+DEBUG:  Wrapping relation "customer_append" to a subquery
+DEBUG:  Router planner does not support append-partitioned tables.
+DEBUG:  generating subplan 11_1 for subquery SELECT c_custkey FROM public.customer_append WHERE false
+DEBUG:  Plan 11 query after replacing subqueries and CTEs: SELECT orders.o_orderkey FROM (public.orders JOIN (SELECT customer_append_1.c_custkey, NULL::character varying(25) AS c_name, NULL::character varying(40) AS c_address, NULL::integer AS c_nationkey, NULL::character(15) AS c_phone, NULL::numeric(15,2) AS c_acctbal, NULL::character(10) AS c_mktsegment, NULL::character varying(117) AS c_comment FROM (SELECT intermediate_result.c_custkey FROM read_intermediate_result('11_1'::text, 'binary'::citus_copy_format) intermediate_result(c_custkey integer)) customer_append_1) customer_append ON ((orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey))) WHERE false
+DEBUG:  Creating router plan
                         QUERY PLAN                         
 -----------------------------------------------------------
  Custom Scan (Citus Adaptive)
+   ->  Distributed Subplan 11_1
+         ->  Custom Scan (Citus Adaptive)
                Task Count: 0
-   Tasks Shown: None, not supported for re-partition queries
-   ->  MapMergeJob
-         Map Task Count: 0
-         Merge Task Count: 0
-   ->  MapMergeJob
-         Map Task Count: 0
-         Merge Task Count: 0
-(9 rows)
+               Tasks Shown: All
+   Task Count: 1
+   Tasks Shown: All
+   ->  Task
+         Node: host=localhost port=57636 dbname=regression
+         ->  Result
+               One-Time Filter: false
+(11 rows)
 
 -- execute once, to verify that's handled
 SELECT
 	o_orderkey
 FROM
 	orders INNER JOIN customer_append ON (o_custkey = c_custkey)
 WHERE
 	false;
 DEBUG:  Router planner does not support append-partitioned tables.
+DEBUG:  Wrapping relation "customer_append" to a subquery
+DEBUG:  Router planner does not support append-partitioned tables.
+DEBUG:  generating subplan 13_1 for subquery SELECT c_custkey FROM public.customer_append WHERE false
+DEBUG:  Plan 13 query after replacing subqueries and CTEs: SELECT orders.o_orderkey FROM (public.orders JOIN (SELECT customer_append_1.c_custkey, NULL::character varying(25) AS c_name, NULL::character varying(40) AS c_address, NULL::integer AS c_nationkey, NULL::character(15) AS c_phone, NULL::numeric(15,2) AS c_acctbal, NULL::character(10) AS c_mktsegment, NULL::character varying(117) AS c_comment FROM (SELECT intermediate_result.c_custkey FROM read_intermediate_result('13_1'::text, 'binary'::citus_copy_format) intermediate_result(c_custkey integer)) customer_append_1) customer_append ON ((orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey))) WHERE false
+DEBUG:  Creating router plan
  o_orderkey 
 ------------
 (0 rows)
 
 EXPLAIN (COSTS OFF)
 SELECT
 	o_orderkey
 FROM
 	orders INNER JOIN customer_append ON (o_custkey = c_custkey)
 WHERE
 	1=0 AND c_custkey < 0;
 DEBUG:  Router planner does not support append-partitioned tables.
+DEBUG:  Wrapping relation "customer_append" to a subquery
+DEBUG:  Router planner does not support append-partitioned tables.
+DEBUG:  generating subplan 15_1 for subquery SELECT c_custkey FROM public.customer_append WHERE false
+DEBUG:  Plan 15 query after replacing subqueries and CTEs: SELECT orders.o_orderkey FROM (public.orders JOIN (SELECT customer_append_1.c_custkey, NULL::character varying(25) AS c_name, NULL::character varying(40) AS c_address, NULL::integer AS c_nationkey, NULL::character(15) AS c_phone, NULL::numeric(15,2) AS c_acctbal, NULL::character(10) AS c_mktsegment, NULL::character varying(117) AS c_comment FROM (SELECT intermediate_result.c_custkey FROM read_intermediate_result('15_1'::text, 'binary'::citus_copy_format) intermediate_result(c_custkey integer)) customer_append_1) customer_append ON ((orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey))) WHERE ((1 OPERATOR(pg_catalog.=) 0) AND (customer_append.c_custkey OPERATOR(pg_catalog.<) 0))
+DEBUG:  Creating router plan
                         QUERY PLAN                         
 -----------------------------------------------------------
  Custom Scan (Citus Adaptive)
+   ->  Distributed Subplan 15_1
+         ->  Custom Scan (Citus Adaptive)
                Task Count: 0
-   Tasks Shown: None, not supported for re-partition queries
-   ->  MapMergeJob
-         Map Task Count: 0
-         Merge Task Count: 0
-   ->  MapMergeJob
-         Map Task Count: 0
-         Merge Task Count: 0
-(9 rows)
+               Tasks Shown: All
+   Task Count: 1
+   Tasks Shown: All
+   ->  Task
+         Node: host=localhost port=57636 dbname=regression
+         ->  Result
+               One-Time Filter: false
+(11 rows)
 
 EXPLAIN (COSTS OFF)
 SELECT
 	o_orderkey
 FROM
 	orders INNER JOIN customer_append ON (o_custkey = c_custkey AND false);
 DEBUG:  Router planner does not support append-partitioned tables.
+DEBUG:  Wrapping relation "customer_append" to a subquery
+DEBUG:  Router planner does not support append-partitioned tables.
+DEBUG:  generating subplan 17_1 for subquery SELECT NULL::integer AS "dummy-1" FROM public.customer_append WHERE false
+DEBUG:  Plan 17 query after replacing subqueries and CTEs: SELECT orders.o_orderkey FROM (public.orders JOIN (SELECT NULL::integer AS c_custkey, NULL::character varying(25) AS c_name, NULL::character varying(40) AS c_address, NULL::integer AS c_nationkey, NULL::character(15) AS c_phone, NULL::numeric(15,2) AS c_acctbal, NULL::character(10) AS c_mktsegment, NULL::character varying(117) AS c_comment FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('17_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) customer_append_1) customer_append ON (((orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey) AND false)))
+DEBUG:  Creating router plan
                         QUERY PLAN                         
 -----------------------------------------------------------
  Custom Scan (Citus Adaptive)
+   ->  Distributed Subplan 17_1
+         ->  Custom Scan (Citus Adaptive)
                Task Count: 0
                Tasks Shown: All
-(3 rows)
+   Task Count: 1
+   Tasks Shown: All
+   ->  Task
+         Node: host=localhost port=57636 dbname=regression
+         ->  Result
+               One-Time Filter: false
+(11 rows)
 
 EXPLAIN (COSTS OFF)
 SELECT
 	o_orderkey
 FROM
 	orders, customer_append
 WHERE
 	o_custkey = c_custkey AND false;
 DEBUG:  Router planner does not support append-partitioned tables.
+DEBUG:  Wrapping relation "customer_append" to a subquery
+DEBUG:  Router planner does not support append-partitioned tables.
+DEBUG:  generating subplan 19_1 for subquery SELECT NULL::integer AS "dummy-1" FROM public.customer_append WHERE false
+DEBUG:  Plan 19 query after replacing subqueries and CTEs: SELECT orders.o_orderkey FROM public.orders, (SELECT NULL::integer AS c_custkey, NULL::character varying(25) AS c_name, NULL::character varying(40) AS c_address, NULL::integer AS c_nationkey, NULL::character(15) AS c_phone, NULL::numeric(15,2) AS c_acctbal, NULL::character(10) AS c_mktsegment, NULL::character varying(117) AS c_comment FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('19_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) customer_append_1) customer_append WHERE ((orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey) AND false)
+DEBUG:  Creating router plan
                         QUERY PLAN                         
 -----------------------------------------------------------
  Custom Scan (Citus Adaptive)
+   ->  Distributed Subplan 19_1
+         ->  Custom Scan (Citus Adaptive)
                Task Count: 0
                Tasks Shown: All
-(3 rows)
+   Task Count: 1
+   Tasks Shown: All
+   ->  Task
+         Node: host=localhost port=57636 dbname=regression
+         ->  Result
+               One-Time Filter: false
+(11 rows)
 
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_repartition_join_task_assignment.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_repartition_join_task_assignment.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_repartition_join_task_assignment.out.modified	2022-11-09 13:38:06.229312486 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_repartition_join_task_assignment.out.modified	2022-11-09 13:38:06.239312485 +0300
@@ -11,172 +11,61 @@
 SET citus.enable_repartition_joins to ON;
 -- Single range repartition join to test anchor-shard based task assignment and
 -- assignment propagation to merge and data-fetch tasks.
 SELECT
 	count(*)
 FROM
 	orders, customer_append
 WHERE
 	o_custkey = c_custkey;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  no shard pruning constraints on orders found
-DEBUG:  shard count after pruning for orders: 2
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  no shard pruning constraints on customer_append found
-DEBUG:  shard count after pruning for customer_append: 3
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
+DEBUG:  Wrapping relation "orders" to a subquery
+DEBUG:  generating subplan 1_1 for subquery SELECT o_custkey FROM public.orders WHERE true
+DEBUG:  Plan 1 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT NULL::bigint AS o_orderkey, orders_1.o_custkey, NULL::character(1) AS o_orderstatus, NULL::numeric(15,2) AS o_totalprice, NULL::date AS o_orderdate, NULL::character(15) AS o_orderpriority, NULL::character(15) AS o_clerk, NULL::integer AS o_shippriority, NULL::character varying(79) AS o_comment FROM (SELECT intermediate_result.o_custkey FROM read_intermediate_result('1_1'::text, 'binary'::citus_copy_format) intermediate_result(o_custkey integer)) orders_1) orders, public.customer_append WHERE (orders.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey)
+DEBUG:  Router planner does not support append-partitioned tables.
  count 
 -------
-  2985
+     0
 (1 row)
 
 -- Single range repartition join, along with a join with a small table containing
 -- more than one shard. This situation results in multiple sql tasks depending on
 -- the same merge task, and tests our constraint group creation and assignment
 -- propagation.
 SELECT
 	count(*)
 FROM
 	orders_reference, customer_append, lineitem
 WHERE
 	o_custkey = c_custkey AND
 	o_orderkey = l_orderkey;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  no shard pruning constraints on customer_append found
-DEBUG:  shard count after pruning for customer_append: 3
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  no shard pruning constraints on lineitem found
-DEBUG:  shard count after pruning for lineitem: 2
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 16
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 2_1 for subquery SELECT l_orderkey FROM public.lineitem WHERE true
+DEBUG:  Plan 2 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM public.orders_reference, public.customer_append, (SELECT lineitem_1.l_orderkey, NULL::integer AS l_partkey, NULL::integer AS l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_orderkey FROM read_intermediate_result('2_1'::text, 'binary'::citus_copy_format) intermediate_result(l_orderkey bigint)) lineitem_1) lineitem WHERE ((orders_reference.o_custkey OPERATOR(pg_catalog.=) customer_append.c_custkey) AND (orders_reference.o_orderkey OPERATOR(pg_catalog.=) lineitem.l_orderkey))
+DEBUG:  Router planner does not support append-partitioned tables.
  count 
 -------
- 12000
+     0
 (1 row)
 
 -- Dual hash repartition join which tests the separate hash repartition join
 -- task assignment algorithm.
 SELECT
 	count(*)
 FROM
 	lineitem, customer_append
 WHERE
 	l_partkey = c_nationkey;
 DEBUG:  Router planner does not support append-partitioned tables.
-DEBUG:  no shard pruning constraints on lineitem found
-DEBUG:  shard count after pruning for lineitem: 2
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  no shard pruning constraints on customer_append found
-DEBUG:  shard count after pruning for customer_append: 3
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 3
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 12
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 16
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
-DEBUG:  assigned task to node localhost:xxxxx
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 3_1 for subquery SELECT l_partkey FROM public.lineitem WHERE true
+DEBUG:  Plan 3 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, NULL::integer AS l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey FROM read_intermediate_result('3_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer)) lineitem_1) lineitem, public.customer_append WHERE (lineitem.l_partkey OPERATOR(pg_catalog.=) customer_append.c_nationkey)
+DEBUG:  Router planner does not support append-partitioned tables.
  count 
 -------
-   125
+     0
 (1 row)
 
 -- Reset client logging level to its previous value
 SET client_min_messages TO NOTICE;
 COMMIT;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_repartition_join_ref.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_repartition_join_ref.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_repartition_join_ref.out.modified	2022-11-09 13:38:06.359312485 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_repartition_join_ref.out.modified	2022-11-09 13:38:06.369312485 +0300
@@ -6,221 +6,157 @@
 FROM
     lineitem, part_append, supplier
 WHERE
     l_partkey = p_partkey
     AND l_suppkey < s_suppkey
 GROUP BY
     l_partkey, l_suppkey
 ORDER BY
     l_partkey, l_suppkey
 LIMIT 10;
-LOG:  join order: [ "lineitem" ][ reference join "supplier" ][ dual partition join "part_append" ]
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 1_1 for subquery SELECT l_partkey, l_suppkey FROM public.lineitem WHERE true
+DEBUG:  Plan 1 query after replacing subqueries and CTEs: SELECT lineitem.l_partkey, lineitem.l_suppkey, count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, lineitem_1.l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey, intermediate_result.l_suppkey FROM read_intermediate_result('1_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer, l_suppkey integer)) lineitem_1) lineitem, public.part_append, public.supplier WHERE ((lineitem.l_partkey OPERATOR(pg_catalog.=) part_append.p_partkey) AND (lineitem.l_suppkey OPERATOR(pg_catalog.<) supplier.s_suppkey)) GROUP BY lineitem.l_partkey, lineitem.l_suppkey ORDER BY lineitem.l_partkey, lineitem.l_suppkey LIMIT 10
 DEBUG:  push down of limit count: 10
  l_partkey | l_suppkey | count 
 -----------+-----------+-------
-       195 |       196 |   804
-       245 |       246 |   754
-       278 |       279 |   721
-       308 |       309 |   691
-       309 |       310 |  1380
-       350 |       351 |   649
-       358 |       359 |   641
-       574 |       575 |   425
-       641 |       642 |   358
-       654 |       655 |   345
-(10 rows)
+(0 rows)
 
 SELECT
     l_partkey, l_suppkey, count(*)
 FROM
     lineitem, part_append, supplier
 WHERE
     l_partkey = p_partkey
     AND int4eq(l_suppkey, s_suppkey)
 GROUP BY
     l_partkey, l_suppkey
 ORDER BY
     l_partkey, l_suppkey
 LIMIT 10;
-LOG:  join order: [ "lineitem" ][ reference join "supplier" ][ dual partition join "part_append" ]
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 2_1 for subquery SELECT l_partkey, l_suppkey FROM public.lineitem WHERE true
+DEBUG:  Plan 2 query after replacing subqueries and CTEs: SELECT lineitem.l_partkey, lineitem.l_suppkey, count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, lineitem_1.l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey, intermediate_result.l_suppkey FROM read_intermediate_result('2_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer, l_suppkey integer)) lineitem_1) lineitem, public.part_append, public.supplier WHERE ((lineitem.l_partkey OPERATOR(pg_catalog.=) part_append.p_partkey) AND int4eq(lineitem.l_suppkey, supplier.s_suppkey)) GROUP BY lineitem.l_partkey, lineitem.l_suppkey ORDER BY lineitem.l_partkey, lineitem.l_suppkey LIMIT 10
 DEBUG:  push down of limit count: 10
  l_partkey | l_suppkey | count 
 -----------+-----------+-------
-       195 |       196 |     1
-       245 |       246 |     1
-       278 |       279 |     1
-       308 |       309 |     1
-       309 |       310 |     2
-       350 |       351 |     1
-       358 |       359 |     1
-       574 |       575 |     1
-       641 |       642 |     1
-       654 |       655 |     1
-(10 rows)
+(0 rows)
 
 SELECT
     l_partkey, l_suppkey, count(*)
 FROM
     lineitem, part_append, supplier
 WHERE
     l_partkey = p_partkey
     AND NOT int4ne(l_suppkey, s_suppkey)
 GROUP BY
     l_partkey, l_suppkey
 ORDER BY
     l_partkey, l_suppkey
 LIMIT 10;
-LOG:  join order: [ "lineitem" ][ reference join "supplier" ][ dual partition join "part_append" ]
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 3_1 for subquery SELECT l_partkey, l_suppkey FROM public.lineitem WHERE true
+DEBUG:  Plan 3 query after replacing subqueries and CTEs: SELECT lineitem.l_partkey, lineitem.l_suppkey, count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, lineitem_1.l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey, intermediate_result.l_suppkey FROM read_intermediate_result('3_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer, l_suppkey integer)) lineitem_1) lineitem, public.part_append, public.supplier WHERE ((lineitem.l_partkey OPERATOR(pg_catalog.=) part_append.p_partkey) AND (NOT int4ne(lineitem.l_suppkey, supplier.s_suppkey))) GROUP BY lineitem.l_partkey, lineitem.l_suppkey ORDER BY lineitem.l_partkey, lineitem.l_suppkey LIMIT 10
 DEBUG:  push down of limit count: 10
  l_partkey | l_suppkey | count 
 -----------+-----------+-------
-       195 |       196 |     1
-       245 |       246 |     1
-       278 |       279 |     1
-       308 |       309 |     1
-       309 |       310 |     2
-       350 |       351 |     1
-       358 |       359 |     1
-       574 |       575 |     1
-       641 |       642 |     1
-       654 |       655 |     1
-(10 rows)
+(0 rows)
 
 SELECT
     l_partkey, l_suppkey, count(*)
 FROM
     lineitem, part_append, supplier
 WHERE
     l_partkey = p_partkey
 GROUP BY
     l_partkey, l_suppkey
 ORDER BY
     l_partkey, l_suppkey
 LIMIT 10;
-LOG:  join order: [ "lineitem" ][ dual partition join "part_append" ][ cartesian product reference join "supplier" ]
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 4_1 for subquery SELECT l_partkey, l_suppkey FROM public.lineitem WHERE true
+DEBUG:  Plan 4 query after replacing subqueries and CTEs: SELECT lineitem.l_partkey, lineitem.l_suppkey, count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, lineitem_1.l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey, intermediate_result.l_suppkey FROM read_intermediate_result('4_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer, l_suppkey integer)) lineitem_1) lineitem, public.part_append, public.supplier WHERE (lineitem.l_partkey OPERATOR(pg_catalog.=) part_append.p_partkey) GROUP BY lineitem.l_partkey, lineitem.l_suppkey ORDER BY lineitem.l_partkey, lineitem.l_suppkey LIMIT 10
 DEBUG:  push down of limit count: 10
  l_partkey | l_suppkey | count 
 -----------+-----------+-------
-        18 |      7519 |  1000
-        79 |      7580 |  1000
-        91 |      2592 |  1000
-       149 |      5150 |  1000
-       149 |      7650 |  1000
-       175 |      5176 |  1000
-       179 |      2680 |  1000
-       182 |      7683 |  1000
-       195 |       196 |  1000
-       204 |      7705 |  1000
-(10 rows)
+(0 rows)
 
 SELECT
     l_partkey, l_suppkey, count(*)
 FROM
     lineitem, part_append, supplier
 WHERE
     l_partkey = p_partkey
     AND (int4eq(l_suppkey, s_suppkey) OR l_suppkey = s_suppkey)
 GROUP BY
     l_partkey, l_suppkey
 ORDER BY
     l_partkey, l_suppkey
 LIMIT 10;
-LOG:  join order: [ "lineitem" ][ reference join "supplier" ][ dual partition join "part_append" ]
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 5_1 for subquery SELECT l_partkey, l_suppkey FROM public.lineitem WHERE true
+DEBUG:  Plan 5 query after replacing subqueries and CTEs: SELECT lineitem.l_partkey, lineitem.l_suppkey, count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, lineitem_1.l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey, intermediate_result.l_suppkey FROM read_intermediate_result('5_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer, l_suppkey integer)) lineitem_1) lineitem, public.part_append, public.supplier WHERE ((lineitem.l_partkey OPERATOR(pg_catalog.=) part_append.p_partkey) AND (int4eq(lineitem.l_suppkey, supplier.s_suppkey) OR (lineitem.l_suppkey OPERATOR(pg_catalog.=) supplier.s_suppkey))) GROUP BY lineitem.l_partkey, lineitem.l_suppkey ORDER BY lineitem.l_partkey, lineitem.l_suppkey LIMIT 10
 DEBUG:  push down of limit count: 10
  l_partkey | l_suppkey | count 
 -----------+-----------+-------
-       195 |       196 |     1
-       245 |       246 |     1
-       278 |       279 |     1
-       308 |       309 |     1
-       309 |       310 |     2
-       350 |       351 |     1
-       358 |       359 |     1
-       574 |       575 |     1
-       641 |       642 |     1
-       654 |       655 |     1
-(10 rows)
+(0 rows)
 
 SELECT
     l_partkey, l_suppkey, count(*)
 FROM
     lineitem, part_append, supplier
 WHERE
     l_partkey = p_partkey
     AND (int4eq(l_suppkey, s_suppkey) OR random() > 2)
 GROUP BY
     l_partkey, l_suppkey
 ORDER BY
     l_partkey, l_suppkey
 LIMIT 10;
-LOG:  join order: [ "lineitem" ][ reference join "supplier" ][ dual partition join "part_append" ]
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 6_1 for subquery SELECT l_partkey, l_suppkey FROM public.lineitem WHERE true
+DEBUG:  Plan 6 query after replacing subqueries and CTEs: SELECT lineitem.l_partkey, lineitem.l_suppkey, count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, lineitem_1.l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey, intermediate_result.l_suppkey FROM read_intermediate_result('6_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer, l_suppkey integer)) lineitem_1) lineitem, public.part_append, public.supplier WHERE ((lineitem.l_partkey OPERATOR(pg_catalog.=) part_append.p_partkey) AND (int4eq(lineitem.l_suppkey, supplier.s_suppkey) OR (random() OPERATOR(pg_catalog.>) (2)::double precision))) GROUP BY lineitem.l_partkey, lineitem.l_suppkey ORDER BY lineitem.l_partkey, lineitem.l_suppkey LIMIT 10
 DEBUG:  push down of limit count: 10
  l_partkey | l_suppkey | count 
 -----------+-----------+-------
-       195 |       196 |     1
-       245 |       246 |     1
-       278 |       279 |     1
-       308 |       309 |     1
-       309 |       310 |     2
-       350 |       351 |     1
-       358 |       359 |     1
-       574 |       575 |     1
-       641 |       642 |     1
-       654 |       655 |     1
-(10 rows)
+(0 rows)
 
 SELECT
     l_partkey, l_suppkey, count(*)
 FROM
     lineitem, part_append, supplier
 WHERE
     l_partkey = p_partkey
     AND (l_suppkey = 1 OR s_suppkey = 1)
 GROUP BY
     l_partkey, l_suppkey
 ORDER BY
     l_partkey, l_suppkey
 LIMIT 10;
-LOG:  join order: [ "lineitem" ][ reference join "supplier" ][ dual partition join "part_append" ]
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 7_1 for subquery SELECT l_partkey, l_suppkey FROM public.lineitem WHERE true
+DEBUG:  Plan 7 query after replacing subqueries and CTEs: SELECT lineitem.l_partkey, lineitem.l_suppkey, count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, lineitem_1.l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey, intermediate_result.l_suppkey FROM read_intermediate_result('7_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer, l_suppkey integer)) lineitem_1) lineitem, public.part_append, public.supplier WHERE ((lineitem.l_partkey OPERATOR(pg_catalog.=) part_append.p_partkey) AND ((lineitem.l_suppkey OPERATOR(pg_catalog.=) 1) OR (supplier.s_suppkey OPERATOR(pg_catalog.=) 1))) GROUP BY lineitem.l_partkey, lineitem.l_suppkey ORDER BY lineitem.l_partkey, lineitem.l_suppkey LIMIT 10
 DEBUG:  push down of limit count: 10
  l_partkey | l_suppkey | count 
 -----------+-----------+-------
-        18 |      7519 |     1
-        79 |      7580 |     1
-        91 |      2592 |     1
-       149 |      5150 |     1
-       149 |      7650 |     1
-       175 |      5176 |     1
-       179 |      2680 |     1
-       182 |      7683 |     1
-       195 |       196 |     1
-       204 |      7705 |     1
-(10 rows)
+(0 rows)
 
 SELECT
     l_partkey, l_suppkey, count(*)
 FROM
     lineitem, part_append, supplier
 WHERE
     l_partkey = p_partkey
     AND l_partkey + p_partkey = s_suppkey
 GROUP BY
     l_partkey, l_suppkey
 ORDER BY
     l_partkey, l_suppkey
 LIMIT 10;
-LOG:  join order: [ "lineitem" ][ dual partition join "part_append" ][ reference join "supplier" ]
+DEBUG:  Wrapping relation "lineitem" to a subquery
+DEBUG:  generating subplan 8_1 for subquery SELECT l_partkey, l_suppkey FROM public.lineitem WHERE true
+DEBUG:  Plan 8 query after replacing subqueries and CTEs: SELECT lineitem.l_partkey, lineitem.l_suppkey, count(*) AS count FROM (SELECT NULL::bigint AS l_orderkey, lineitem_1.l_partkey, lineitem_1.l_suppkey, NULL::integer AS l_linenumber, NULL::numeric(15,2) AS l_quantity, NULL::numeric(15,2) AS l_extendedprice, NULL::numeric(15,2) AS l_discount, NULL::numeric(15,2) AS l_tax, NULL::character(1) AS l_returnflag, NULL::character(1) AS l_linestatus, NULL::date AS l_shipdate, NULL::date AS l_commitdate, NULL::date AS l_receiptdate, NULL::character(25) AS l_shipinstruct, NULL::character(10) AS l_shipmode, NULL::character varying(44) AS l_comment FROM (SELECT intermediate_result.l_partkey, intermediate_result.l_suppkey FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(l_partkey integer, l_suppkey integer)) lineitem_1) lineitem, public.part_append, public.supplier WHERE ((lineitem.l_partkey OPERATOR(pg_catalog.=) part_append.p_partkey) AND ((lineitem.l_partkey OPERATOR(pg_catalog.+) part_append.p_partkey) OPERATOR(pg_catalog.=) supplier.s_suppkey)) GROUP BY lineitem.l_partkey, lineitem.l_suppkey ORDER BY lineitem.l_partkey, lineitem.l_suppkey LIMIT 10
 DEBUG:  push down of limit count: 10
  l_partkey | l_suppkey | count 
 -----------+-----------+-------
-        18 |      7519 |     1
-        79 |      7580 |     1
-        91 |      2592 |     1
-       149 |      5150 |     1
-       149 |      7650 |     1
-       175 |      5176 |     1
-       179 |      2680 |     1
-       182 |      7683 |     1
-       195 |       196 |     1
-       204 |      7705 |     1
-(10 rows)
+(0 rows)
 
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/with_prepare.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/with_prepare.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/with_prepare.out.modified	2022-11-09 13:38:07.729312479 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/with_prepare.out.modified	2022-11-09 13:38:07.739312479 +0300
@@ -210,25 +210,22 @@
 ORDER BY
 	user_id,
 	time
 LIMIT 10;
 --
 -- Test a prepared statement with unused argument
 --
 CREATE TYPE foo as (x int, y int);
 CREATE TABLE footest (x int, y int, z foo);
 SELECT create_distributed_table('footest','x');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO footest VALUES(1, 2, (3,4));
 -- Add a redundant parameter
 PREPARE prepared_test_9(foo,foo) AS
 WITH a AS (
 	SELECT * FROM footest WHERE z = $1 AND x = 1 OFFSET 0
 )
 SELECT * FROM a;
 EXECUTE prepared_test_1;
  user_id |              time               | value_1 | value_2 | value_3 | value_4 
 ---------+---------------------------------+---------+---------+---------+---------
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/with_basics.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/with_basics.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/with_basics.out.modified	2022-11-09 13:38:08.749312475 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/with_basics.out.modified	2022-11-09 13:38:08.759312475 +0300
@@ -781,25 +781,22 @@
 );
 INSERT INTO test_cte
 SELECT *
 FROM (VALUES ('1'), ('1'), ('2'), ('2'), ('3'), ('4'), ('5'), ('6'), ('7'), ('8')) AS foo;
 CREATE TABLE test_cte_distributed
 (
     user_id varchar
 );
 SELECT *
 FROM create_distributed_table('test_cte_distributed', 'user_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO test_cte_distributed
 SELECT *
 FROM (VALUES ('1'), ('3'), ('3'), ('5'), ('8')) AS foo;
 WITH distinct_undistribured AS (
     SELECT DISTINCT user_id
     FROM test_cte
 ),
      exsist_in_distributed AS (
          SELECT DISTINCT user_id
          FROM test_cte_distributed
@@ -833,38 +830,58 @@
 FROM distinct_undistribured
 LEFT JOIN
   (SELECT DISTINCT user_id
    FROM test_cte_distributed
    WHERE EXISTS
        (SELECT *
         FROM distinct_undistribured
         WHERE distinct_undistribured.user_id = test_cte_distributed.user_id)) exsist_in_distributed
   ON distinct_undistribured.user_id = exsist_in_distributed.user_id
 ORDER BY 2 DESC, 1 DESC;
-ERROR:  cannot pushdown the subquery
-DETAIL:  Complex subqueries, CTEs and local tables cannot be in the outer part of an outer join with a distributed table
+ user_id | user_id 
+---------+---------
+ 7       | 
+ 6       | 
+ 4       | 
+ 2       | 
+ 8       | 8
+ 5       | 5
+ 3       | 3
+ 1       | 1
+(8 rows)
+
 -- similar query as the above, but this time
 -- use NOT EXITS, which is pretty common struct
 WITH distinct_undistribured AS
   (SELECT DISTINCT user_id
    FROM test_cte)
 SELECT *
 FROM distinct_undistribured
 LEFT JOIN
   (SELECT DISTINCT user_id
    FROM test_cte_distributed
    WHERE NOT EXISTS
        (SELECT NULL
         FROM distinct_undistribured
         WHERE distinct_undistribured.user_id = test_cte_distributed.user_id)) exsist_in_distributed ON distinct_undistribured.user_id = exsist_in_distributed.user_id;
-ERROR:  cannot pushdown the subquery
-DETAIL:  Complex subqueries, CTEs and local tables cannot be in the outer part of an outer join with a distributed table
+ user_id | user_id 
+---------+---------
+ 2       | 
+ 4       | 
+ 7       | 
+ 3       | 
+ 6       | 
+ 5       | 
+ 1       | 
+ 8       | 
+(8 rows)
+
 -- same NOT EXISTS struct, but with CTE
 -- so should work
 WITH distinct_undistribured AS (
     SELECT DISTINCT user_id
     FROM test_cte
 ),
      not_exsist_in_distributed AS (
          SELECT DISTINCT user_id
          FROM test_cte_distributed
          WHERE NOT EXISTS(SELECT NULL
@@ -898,22 +915,25 @@
 SELECT count(*)
 FROM distinct_undistribured
 LEFT JOIN
   (SELECT *,
           random()
    FROM test_cte_distributed d1
    WHERE NOT EXISTS
        (SELECT NULL
         FROM distinct_undistribured d2
         WHERE d1.user_id = d2.user_id )) AS bar USING (user_id);
-ERROR:  cannot pushdown the subquery
-DETAIL:  Complex subqueries, CTEs and local tables cannot be in the outer part of an outer join with a distributed table
+ count 
+-------
+     8
+(1 row)
+
 -- should work fine with materialized ctes
 WITH distinct_undistribured AS MATERIALIZED (
     SELECT DISTINCT user_id
     FROM test_cte
 ),
      exsist_in_distributed AS MATERIALIZED (
          SELECT DISTINCT user_id
          FROM test_cte_distributed
          WHERE EXISTS(SELECT *
                       FROM distinct_undistribured
@@ -944,36 +964,56 @@
 FROM distinct_undistribured
 LEFT JOIN
   (SELECT DISTINCT user_id
    FROM test_cte_distributed
    WHERE EXISTS
        (SELECT *
         FROM distinct_undistribured
         WHERE distinct_undistribured.user_id = test_cte_distributed.user_id)) exsist_in_distributed
   ON distinct_undistribured.user_id = exsist_in_distributed.user_id
 ORDER BY 2 DESC, 1 DESC;
-ERROR:  cannot pushdown the subquery
-DETAIL:  Complex subqueries, CTEs and local tables cannot be in the outer part of an outer join with a distributed table
+ user_id | user_id 
+---------+---------
+ 7       | 
+ 6       | 
+ 4       | 
+ 2       | 
+ 8       | 8
+ 5       | 5
+ 3       | 3
+ 1       | 1
+(8 rows)
+
 WITH distinct_undistribured AS MATERIALIZED
   (SELECT DISTINCT user_id
    FROM test_cte)
 SELECT *
 FROM distinct_undistribured
 LEFT JOIN
   (SELECT DISTINCT user_id
    FROM test_cte_distributed
    WHERE NOT EXISTS
        (SELECT NULL
         FROM distinct_undistribured
         WHERE distinct_undistribured.user_id = test_cte_distributed.user_id)) exsist_in_distributed ON distinct_undistribured.user_id = exsist_in_distributed.user_id;
-ERROR:  cannot pushdown the subquery
-DETAIL:  Complex subqueries, CTEs and local tables cannot be in the outer part of an outer join with a distributed table
+ user_id | user_id 
+---------+---------
+ 2       | 
+ 4       | 
+ 7       | 
+ 3       | 
+ 6       | 
+ 5       | 
+ 1       | 
+ 8       | 
+(8 rows)
+
 -- NOT EXISTS struct, with cte inlining disabled
 WITH distinct_undistribured AS MATERIALIZED(
     SELECT DISTINCT user_id
     FROM test_cte
 ),
      not_exsist_in_distributed AS MATERIALIZED (
          SELECT DISTINCT user_id
          FROM test_cte_distributed
          WHERE NOT EXISTS(SELECT NULL
                       FROM distinct_undistribured
@@ -1006,22 +1046,25 @@
 SELECT count(*)
 FROM distinct_undistribured
 LEFT JOIN
   (SELECT *,
           random()
    FROM test_cte_distributed d1
    WHERE NOT EXISTS
        (SELECT NULL
         FROM distinct_undistribured d2
         WHERE d1.user_id = d2.user_id )) AS bar USING (user_id);
-ERROR:  cannot pushdown the subquery
-DETAIL:  Complex subqueries, CTEs and local tables cannot be in the outer part of an outer join with a distributed table
+ count 
+-------
+     8
+(1 row)
+
 -- some test  with failures
 WITH a AS MATERIALIZED (SELECT * FROM users_table LIMIT 10)
 	SELECT user_id/0 FROM users_table JOIN a USING (user_id);
 ERROR:  division by zero
 CONTEXT:  while executing command on localhost:57638
 DROP VIEW basic_view;
 DROP VIEW cte_view;
 DROP SCHEMA with_basics CASCADE;
 NOTICE:  drop cascades to 5 other objects
 DETAIL:  drop cascades to table users_table
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/with_modifying.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/with_modifying.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/with_modifying.out.modified	2022-11-09 13:38:09.129312474 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/with_modifying.out.modified	2022-11-09 13:38:09.139312474 +0300
@@ -1,37 +1,28 @@
 -- Tests for modifying CTEs and CTEs in modifications
 SET citus.next_shard_id TO 1502000;
 CREATE SCHEMA with_modifying;
 SET search_path TO with_modifying, public;
 CREATE TABLE with_modifying.local_table (id int, val int);
 CREATE TABLE with_modifying.modify_table (id int, val int);
 SELECT create_distributed_table('modify_table', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE with_modifying.users_table (LIKE public.users_table INCLUDING ALL);
 SELECT create_distributed_table('with_modifying.users_table', 'user_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO with_modifying.users_table SELECT * FROM public.users_table;
 CREATE TABLE with_modifying.summary_table (id int, counter int);
 SELECT create_distributed_table('summary_table', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE with_modifying.anchor_table (id int);
 SELECT create_reference_table('anchor_table');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 -- basic insert query in CTE
 WITH basic_insert AS (
 	INSERT INTO users_table VALUES (1), (2), (3) RETURNING *
@@ -291,82 +282,87 @@
 )
 INSERT INTO
 	summary_table
 SELECT id, SUM(counter) FROM (
 	(SELECT raw_data.id, COUNT(*) AS counter FROM raw_data, anchor_data
 		WHERE raw_data.id = anchor_data.id GROUP BY raw_data.id)
 	UNION ALL
 	(SELECT * FROM summary_data)) AS all_rows
 GROUP BY
 	id;
+ERROR:  relation modify_table is not distributed
 SELECT COUNT(*) FROM modify_table;
  count 
 -------
      0
 (1 row)
 
 SELECT * FROM summary_table ORDER BY id, counter;
  id | counter 
 ----+---------
   1 |       1
-  2 |      21
-  3 |      39
+  2 |       1
+  2 |      20
+  3 |       1
+  3 |      38
   4 |      24
   5 |      27
-(5 rows)
+(7 rows)
 
 WITH added_data AS (
 	INSERT INTO modify_table VALUES (1,2), (1,6), (2,4), (3,6) RETURNING *
 ),
 raw_data AS (
 	DELETE FROM modify_table WHERE id = 1 AND val = (SELECT MAX(val) FROM added_data) RETURNING *
 )
 INSERT INTO summary_table SELECT id, COUNT(*) AS counter FROM raw_data GROUP BY id;
 SELECT COUNT(*) FROM modify_table;
  count 
 -------
-     3
+     4
 (1 row)
 
 SELECT * FROM summary_table ORDER BY id, counter;
  id | counter 
 ----+---------
   1 |       1
-  1 |       1
-  2 |      21
-  3 |      39
+  2 |       1
+  2 |      20
+  3 |       1
+  3 |      38
   4 |      24
   5 |      27
-(6 rows)
+(7 rows)
 
 -- Merge rows in the summary_table
 WITH summary_data AS (
 	DELETE FROM summary_table RETURNING *
 )
 INSERT INTO summary_table SELECT id, SUM(counter) AS counter FROM summary_data GROUP BY id;
 SELECT * FROM summary_table ORDER BY id, counter;
  id | counter 
 ----+---------
-  1 |       2
+  1 |       1
   2 |      21
   3 |      39
   4 |      24
   5 |      27
 (5 rows)
 
 SELECT * FROM modify_table ORDER BY id, val;
  id | val 
 ----+-----
   1 |   2
+  1 |   6
   2 |   4
   3 |   6
-(3 rows)
+(4 rows)
 
 SELECT * FROM anchor_table ORDER BY id;
  id 
 ----
   1
   2
 (2 rows)
 
 INSERT INTO modify_table VALUES (11, 1), (12, 2), (13, 3);
 WITH select_data AS (
@@ -481,51 +477,53 @@
   1
   2
   3
   4
 (4 rows)
 
 ROLLBACK;
 SELECT * FROM summary_table ORDER BY id, counter;
  id | counter 
 ----+---------
-  1 |       2
+  1 |       1
   2 |      21
   3 |      39
   4 |      24
   5 |      27
 (5 rows)
 
 SELECT * FROM modify_table ORDER BY id, val;
  id | val 
 ----+-----
   1 |   2
+  1 |   6
   2 |   4
   3 |   6
-(3 rows)
+(4 rows)
 
 SELECT * FROM anchor_table ORDER BY id;
  id 
 ----
   1
   2
 (2 rows)
 
 -- Test delete with subqueries
 WITH deleted_rows AS (
 	DELETE FROM modify_table WHERE id IN (SELECT id FROM modify_table WHERE id = 1) RETURNING *
 )
 SELECT * FROM deleted_rows;
  id | val 
 ----+-----
   1 |   2
-(1 row)
+  1 |   6
+(2 rows)
 
 WITH deleted_rows AS (
 	DELETE FROM modify_table WHERE id IN (SELECT id FROM modify_table WHERE val = 4) RETURNING *
 )
 SELECT * FROM deleted_rows;
  id | val 
 ----+-----
   2 |   4
 (1 row)
 
@@ -557,87 +555,98 @@
 )
 SELECT * FROM deleted_rows;
  id | val 
 ----+-----
 (0 rows)
 
 WITH deleted_rows AS (
 	DELETE FROM modify_table WHERE ctid IN (SELECT ctid FROM modify_table WHERE id = 1) RETURNING *
 )
 SELECT * FROM deleted_rows;
-ERROR:  cannot perform distributed planning for the given modification
-DETAIL:  Recursively planned distributed modifications with ctid on where clause are not supported.
+ id | val 
+----+-----
+(0 rows)
+
 WITH select_rows AS (
 	SELECT ctid FROM modify_table WHERE id = 1
 ),
 deleted_rows AS (
 	DELETE FROM modify_table WHERE ctid IN (SELECT ctid FROM select_rows) RETURNING *
 )
 SELECT * FROM deleted_rows;
-ERROR:  cannot perform distributed planning for the given modification
-DETAIL:  Recursively planned distributed modifications with ctid on where clause are not supported.
+ id | val 
+----+-----
+(0 rows)
+
 WITH added_data AS (
 	INSERT INTO modify_table VALUES (1,2), (1,6) RETURNING *
 ),
 select_data AS (
 	SELECT * FROM added_data WHERE id = 1
 ),
 raw_data AS (
 	DELETE FROM modify_table WHERE id = 1 AND ctid IN (SELECT ctid FROM select_data) RETURNING val
 )
 SELECT * FROM raw_data ORDER BY val;
-ERROR:  cannot perform distributed planning for the given modification
-DETAIL:  Recursively planned distributed modifications with ctid on where clause are not supported.
+ val 
+-----
+(0 rows)
+
 -- We materialize because of https://github.com/citusdata/citus/issues/3189
 WITH added_data AS MATERIALIZED (
 	INSERT INTO modify_table VALUES (1, trunc(10 * random())), (1, trunc(random())) RETURNING *
 ),
 select_data AS MATERIALIZED (
 	SELECT val, now() FROM added_data WHERE id = 1
 ),
 raw_data AS MATERIALIZED (
 	DELETE FROM modify_table WHERE id = 1 AND val IN (SELECT val FROM select_data) RETURNING *
 )
 SELECT COUNT(*) FROM raw_data;
  count 
 -------
-     2
+     0
 (1 row)
 
 WITH added_data AS (
 	INSERT INTO modify_table VALUES (1, trunc(10 * random())), (1, trunc(random())) RETURNING *
 ),
 select_data AS (
 	SELECT val, '2011-01-01' FROM added_data WHERE id = 1
 ),
 raw_data AS (
 	DELETE FROM modify_table WHERE id = 1 AND val IN (SELECT val FROM select_data) RETURNING *
 )
 SELECT COUNT(*) FROM raw_data;
  count 
 -------
-     2
+     1
 (1 row)
 
 INSERT INTO modify_table VALUES (1,2), (1,6), (2, 3), (3, 5);
 WITH select_data AS (
 	SELECT * FROM modify_table
 ),
 raw_data AS (
 	DELETE FROM modify_table WHERE id IN (SELECT id FROM select_data WHERE val > 5) RETURNING id, val
 )
 SELECT * FROM raw_data ORDER BY val;
  id | val 
 ----+-----
+  1 |   0
+  1 |   1
+  1 |   2
   1 |   2
   1 |   6
-(2 rows)
+  1 |   6
+  1 |   9
+(7 rows)
 
 WITH select_data AS (
 	SELECT * FROM modify_table
 ),
 raw_data AS (
 	UPDATE modify_table SET val = 0 WHERE id IN (SELECT id FROM select_data WHERE val < 5) RETURNING id, val
 )
 SELECT * FROM raw_data ORDER BY val;
  id | val 
 ----+-----
@@ -676,31 +685,32 @@
 	FROM modify_table mt WHERE mt.id = lt.id
 	RETURNING lt.id, lt.val
 ) SELECT * FROM cte JOIN modify_table mt ON mt.id = cte.id ORDER BY 1,2;
  id | val | id | val 
 ----+-----+----+-----
 (0 rows)
 
 -- Make sure checks for volatile functions apply to CTEs too
 WITH cte AS (UPDATE modify_table SET val = random() WHERE id = 3 RETURNING *)
 SELECT * FROM cte JOIN modify_table mt ON mt.id = 3 AND mt.id = cte.id ORDER BY 1,2;
-ERROR:  functions used in UPDATE queries on distributed tables must not be VOLATILE
+ id | val | id | val 
+----+-----+----+-----
+  3 |   1 |  3 |   5
+(1 row)
+
 -- Two queries from HammerDB:
 -- 1
 CREATE TABLE with_modifying.stock (s_i_id numeric(6,0) NOT NULL, s_w_id numeric(4,0) NOT NULL, s_quantity numeric(6,0), s_dist_01 character(24)) WITH (fillfactor='50');
 ALTER TABLE with_modifying.stock ADD CONSTRAINT stock_i1 PRIMARY KEY (s_i_id, s_w_id);
 SELECT create_distributed_table('stock', 's_w_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO with_modifying.stock VALUES
 	(64833, 10, 3, 'test1'),
 	(64834, 10, 3, 'test2'),
 	(63867, 10, 3, 'test3');
 PREPARE su_after(INT[], SMALLINT[], SMALLINT[], NUMERIC(5,2)[], NUMERIC, NUMERIC, NUMERIC) AS
 	WITH stock_update AS (
 		UPDATE stock
 		SET s_quantity = ( CASE WHEN s_quantity < (item_stock.quantity + 10) THEN s_quantity + 91 ELSE s_quantity END) - item_stock.quantity
 		FROM UNNEST($1, $2, $3, $4) AS item_stock (item_id, supply_wid, quantity, price)
 		WHERE stock.s_i_id = item_stock.item_id
@@ -746,31 +756,25 @@
  {"test1                   "} | {46}      | {24.7958000000}
 (1 row)
 
 -- 2
 CREATE TABLE with_modifying.orders (o_id numeric NOT NULL, o_w_id numeric NOT NULL, o_d_id numeric NOT NULL, o_c_id numeric) WITH (fillfactor='50');
 CREATE UNIQUE INDEX orders_i2 ON with_modifying.orders USING btree (o_w_id, o_d_id, o_c_id, o_id) TABLESPACE pg_default;
 ALTER TABLE with_modifying.orders ADD CONSTRAINT orders_i1 PRIMARY KEY (o_w_id, o_d_id, o_id);
 CREATE TABLE with_modifying.order_line (ol_w_id numeric NOT NULL, ol_d_id numeric NOT NULL, ol_o_id numeric NOT NULL, ol_number numeric NOT NULL, ol_delivery_d timestamp without time zone, ol_amount numeric) WITH (fillfactor='50');
 ALTER TABLE with_modifying.order_line ADD CONSTRAINT order_line_i1 PRIMARY KEY (ol_w_id, ol_d_id, ol_o_id, ol_number);
 SELECT create_distributed_table('orders', 'o_w_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT create_distributed_table('order_line', 'ol_w_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO orders VALUES (1, 1, 1, 1), (2, 2, 2, 2), (3, 3, 3, 3);
 INSERT INTO order_line VALUES (1, 1, 1, 10), (2, 2, 2, 20), (3, 3, 3, 30);
 PREPARE olu(int,int[],int[]) AS
 	WITH order_line_update AS (
 		UPDATE order_line
 		SET ol_delivery_d = current_timestamp
 		FROM UNNEST($2, $3) AS ids(o_id, d_id)
 		WHERE ol_o_id = ids.o_id
 			AND ol_d_id = ids.d_id
 			AND ol_w_id = $1
@@ -881,34 +885,28 @@
  1991
  1992
 (2 rows)
 
 DELETE FROM with_modifying.anchor_table WHERE id IN (1990, 1991, 1992, 1995, 1996, 1997, 1998);
 -- Test with replication factor 2
 SET citus.shard_replication_factor to 2;
 DROP TABLE modify_table;
 CREATE TABLE with_modifying.modify_table (id int, val int);
 SELECT create_distributed_table('modify_table', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO with_modifying.modify_table SELECT user_id, value_1 FROM public.users_table;
 DROP TABLE summary_table;
 CREATE TABLE with_modifying.summary_table (id int, counter int);
 SELECT create_distributed_table('summary_table', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT COUNT(*) FROM modify_table;
  count 
 -------
    107
 (1 row)
 
 SELECT * FROM summary_table ORDER BY id, counter;
  id | counter 
 ----+---------
 (0 rows)
@@ -943,95 +941,61 @@
 -------
      1
 (1 row)
 
 ROLLBACK;
 -- similarly, make sure that the intermediate result uses a seperate connection
 WITH first_query AS (INSERT INTO modify_table (id) VALUES (10001)),
  	second_query AS (SELECT * FROM modify_table) SELECT count(*) FROM second_query;
  count 
 -------
-     1
+     0
 (1 row)
 
 SET client_min_messages TO debug2;
 -- pushed down without the insert
 WITH mb AS (UPDATE modify_table SET val = 3 WHERE id = 3 RETURNING NULL) INSERT INTO modify_table WITH ma AS (SELECT * FROM modify_table LIMIT 10) SELECT count(*) FROM mb;
-DEBUG:  LIMIT clauses are not allowed in distributed INSERT ... SELECT queries
-DEBUG:  Creating router plan
-DEBUG:  query has a single distribution column value: 3
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 -- not pushed down due to volatile
 WITH ma AS (SELECT count(*) FROM modify_table where id = 1), mu AS (WITH allref AS (SELECT random() a FROM modify_table limit 4) UPDATE modify_table SET val = 3 WHERE id = 1 AND val IN (SELECT a FROM allref) RETURNING id+1) SELECT count(*) FROM mu, ma;
-DEBUG:  CTE ma is going to be inlined via distributed planning
-DEBUG:  Router planner doesn't support VOLATILE functions in common table expressions.
-DEBUG:  generating subplan XXX_1 for CTE mu: WITH allref AS (SELECT random() AS a FROM with_modifying.modify_table modify_table_1 LIMIT 4) UPDATE with_modifying.modify_table SET val = 3 WHERE ((id OPERATOR(pg_catalog.=) 1) AND ((val)::double precision OPERATOR(pg_catalog.=) ANY (SELECT allref.a FROM allref))) RETURNING (id OPERATOR(pg_catalog.+) 1)
-DEBUG:  Router planner doesn't support VOLATILE functions in common table expressions.
-DEBUG:  generating subplan XXX_1 for CTE allref: SELECT random() AS a FROM with_modifying.modify_table LIMIT 4
-DEBUG:  Router planner cannot handle multi-shard select queries
-DEBUG:  push down of limit count: 4
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE with_modifying.modify_table SET val = 3 WHERE ((id OPERATOR(pg_catalog.=) 1) AND ((val)::double precision OPERATOR(pg_catalog.=) ANY (SELECT allref.a FROM (SELECT intermediate_result.a FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(a double precision)) allref))) RETURNING (id OPERATOR(pg_catalog.+) 1)
-DEBUG:  Creating router plan
-DEBUG:  query has a single distribution column value: 1
-DEBUG:  Distributed planning for a fast-path router query
-DEBUG:  Creating router plan
-DEBUG:  query has a single distribution column value: 1
-DEBUG:  generating subplan XXX_2 for subquery SELECT count(*) AS count FROM with_modifying.modify_table WHERE (id OPERATOR(pg_catalog.=) 1)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT intermediate_result."?column?" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("?column?" integer)) mu("?column?"), (SELECT intermediate_result.count FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(count bigint)) ma
-DEBUG:  Creating router plan
  count 
 -------
      0
 (1 row)
 
 WITH mu AS (WITH allref AS (SELECT random() a FROM anchor_table) UPDATE modify_table SET val = 3 WHERE id = 1 AND val IN (SELECT a FROM allref) RETURNING id+1) SELECT count(*) FROM mu;
 DEBUG:  Router planner doesn't support VOLATILE functions in common table expressions.
 DEBUG:  generating subplan 25_1 for CTE mu: WITH allref AS (SELECT random() AS a FROM with_modifying.anchor_table) UPDATE with_modifying.modify_table SET val = 3 WHERE ((id OPERATOR(pg_catalog.=) 1) AND ((val)::double precision OPERATOR(pg_catalog.=) ANY (SELECT allref.a FROM allref))) RETURNING (id OPERATOR(pg_catalog.+) 1)
 DEBUG:  Router planner doesn't support VOLATILE functions in common table expressions.
 DEBUG:  generating subplan 26_1 for CTE allref: SELECT random() AS a FROM with_modifying.anchor_table
 DEBUG:  Distributed planning for a fast-path router query
 DEBUG:  Creating router plan
 DEBUG:  Plan 26 query after replacing subqueries and CTEs: UPDATE with_modifying.modify_table SET val = 3 WHERE ((id OPERATOR(pg_catalog.=) 1) AND ((val)::double precision OPERATOR(pg_catalog.=) ANY (SELECT allref.a FROM (SELECT intermediate_result.a FROM read_intermediate_result('26_1'::text, 'binary'::citus_copy_format) intermediate_result(a double precision)) allref))) RETURNING (id OPERATOR(pg_catalog.+) 1)
 DEBUG:  Creating router plan
-DEBUG:  query has a single distribution column value: 1
 DEBUG:  Plan 25 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT intermediate_result."?column?" FROM read_intermediate_result('25_1'::text, 'binary'::citus_copy_format) intermediate_result("?column?" integer)) mu("?column?")
 DEBUG:  Creating router plan
  count 
 -------
      0
 (1 row)
 
 -- pushed down
 WITH mu AS (WITH allref AS (SELECT id a FROM anchor_table) UPDATE modify_table SET val = 3 WHERE id = 1 AND val IN (SELECT a FROM allref) RETURNING id+1) SELECT count(*) FROM mu;
-DEBUG:  Creating router plan
-DEBUG:  query has a single distribution column value: 1
- count
----------------------------------------------------------------------
-     0
-(1 row)
-
+ERROR:  relation modify_table is not distributed
 -- pushed down and stable function evaluated
 WITH mu AS (WITH allref AS (SELECT now() a FROM anchor_table) UPDATE modify_table SET val = 3 WHERE id = 1 AND now() IN (SELECT a FROM allref) RETURNING id+1) SELECT count(*) FROM mu;
-DEBUG:  Creating router plan
-DEBUG:  query has a single distribution column value: 1
- count
----------------------------------------------------------------------
-     0
-(1 row)
-
+ERROR:  relation modify_table is not distributed
 RESET client_min_messages;
 -- https://github.com/citusdata/citus/issues/3975
 WITH mb AS (INSERT INTO modify_table VALUES (3, 3) RETURNING NULL, NULL) SELECT * FROM modify_table WHERE id = 3;
  id | val 
 ----+-----
-  3 |   3
-(1 row)
+(0 rows)
 
 WITH mb AS (UPDATE modify_table SET val = 3 WHERE id = 3 RETURNING NULL) SELECT * FROM modify_table WHERE id = 3;
  id | val 
 ----+-----
   3 |   3
 (1 row)
 
 WITH mb AS (UPDATE modify_table SET val = 3 WHERE id = 3 RETURNING NULL) SELECT * FROM modify_table, mb WHERE id = 3;
  id | val | ?column? 
 ----+-----+----------
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/cte_prepared_modify.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/cte_prepared_modify.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/cte_prepared_modify.out.modified	2022-11-09 13:38:09.229312473 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/cte_prepared_modify.out.modified	2022-11-09 13:38:09.239312473 +0300
@@ -1,36 +1,22 @@
 CREATE SCHEMA cte_prepared_modify;
 SET search_path TO cte_prepared_modify, public;
 CREATE TABLE tt1(id int, value_1 int);
 INSERT INTO tt1 VALUES(1,2),(2,3),(3,4);
 SELECT create_distributed_table('tt1','id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$cte_prepared_modify.tt1$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE tt2(id int, value_1 int);
 INSERT INTO tt2 VALUES(3,3),(4,4),(5,5);
 SELECT create_distributed_table('tt2','id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$cte_prepared_modify.tt2$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- Test with prepared statements (parameter used by SET)
 PREPARE prepared_test(integer) AS
 WITH cte_1 AS(
   SELECT * FROM tt1 WHERE id >= 2
 )
 UPDATE tt2
 SET value_1 = $1
 FROM cte_1
 WHERE tt2.id = cte_1.id;
 -- Test with prepared statements (parameter used by WHERE on partition column)
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/cte_nested_modification.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/cte_nested_modification.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/cte_nested_modification.out.modified	2022-11-09 13:38:09.319312473 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/cte_nested_modification.out.modified	2022-11-09 13:38:09.319312473 +0300
@@ -1,36 +1,22 @@
 CREATE SCHEMA cte_nested_modifications;
 SET search_path TO cte_nested_modifications, public;
 CREATE TABLE tt1(id int, value_1 int);
 INSERT INTO tt1 VALUES(1,2),(2,3),(3,4);
 SELECT create_distributed_table('tt1','id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$cte_nested_modifications.tt1$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE tt2(id int, value_1 int);
 INSERT INTO tt2 VALUES(3,3),(4,4),(5,5);
 SELECT create_distributed_table('tt2','id');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$cte_nested_modifications.tt2$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE tt3(id int, json_val json);
 INSERT INTO tt3 VALUES(1, '{"prod_name":"name_1", "qty":"6"}'),
                       (2, '{"prod_name":"name_2", "qty":"4"}'),
                       (3, '{"prod_name":"name_3", "qty":"2"}');
 -- DELETE within CTE and use it from UPDATE
 BEGIN;
 WITH cte_1 AS (
     WITH cte_2 AS (
         SELECT id as cte2_id
         FROM tt1
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/with_join.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/with_join.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/with_join.out.modified	2022-11-09 13:38:12.899312458 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/with_join.out.modified	2022-11-09 13:38:12.899312458 +0300
@@ -266,31 +266,25 @@
           1 |        
 (2 rows)
 
 -- some more tests for more complex outer-joins
 -- with reference tables
 CREATE TABLE distributed_1 (col1 int, col2 int, distrib_col int);
 CREATE TABLE distributed_2 (col1 int, col2 int, distrib_col int);
 CREATE TABLE reference_1 (col1 int, col2 int);
 CREATE TABLE reference_2(col1 int, col2 int);
 SELECT create_distributed_table('distributed_1','distrib_col');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT create_distributed_table('distributed_2','distrib_col');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT create_reference_table('reference_1');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 SELECT create_reference_table('reference_2');
  create_reference_table 
 ------------------------
  
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/with_dml.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/with_dml.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/with_dml.out.modified	2022-11-09 13:38:13.079312457 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/with_dml.out.modified	2022-11-09 13:38:13.099312457 +0300
@@ -1,26 +1,20 @@
 CREATE SCHEMA with_dml;
 SET search_path TO 	with_dml, public;
 CREATE TABLE with_dml.distributed_table (tenant_id text PRIMARY KEY, dept int);
 SELECT create_distributed_table('distributed_table', 'tenant_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE with_dml.second_distributed_table (tenant_id text, dept int);
 SELECT create_distributed_table('second_distributed_table', 'tenant_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE with_dml.reference_table (id text, name text);
 SELECT create_reference_table('reference_table');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 INSERT INTO distributed_table SELECT i::text, i % 10 FROM generate_series (0, 100) i;
 INSERT INTO second_distributed_table SELECT i::text, i % 10 FROM generate_series (0, 100) i;
 INSERT INTO reference_table SELECT i::text, 'user_' || i FROM generate_series (0, 100) i;
@@ -61,63 +55,52 @@
 UPDATE
 	distributed_table
 SET
 	dept = dept + 1
 FROM
 	ids_to_delete, (SELECT tenant_id FROM distributed_table WHERE tenant_id::int < 60) as some_tenants
 WHERE
 	some_tenants.tenant_id = ids_to_delete.tenant_id
 	AND distributed_table.tenant_id = some_tenants.tenant_id
 	AND EXISTS (SELECT * FROM ids_to_delete);
-DEBUG:  generating subplan XXX_1 for CTE ids_to_delete: SELECT tenant_id FROM with_dml.distributed_table WHERE (dept OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE with_dml.distributed_table SET dept = (distributed_table.dept OPERATOR(pg_catalog.+) 1) FROM (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text)) ids_to_delete, (SELECT distributed_table_1.tenant_id FROM with_dml.distributed_table distributed_table_1 WHERE ((distributed_table_1.tenant_id)::integer OPERATOR(pg_catalog.<) 60)) some_tenants WHERE ((some_tenants.tenant_id OPERATOR(pg_catalog.=) ids_to_delete.tenant_id) AND (distributed_table.tenant_id OPERATOR(pg_catalog.=) some_tenants.tenant_id) AND (EXISTS (SELECT ids_to_delete_1.tenant_id FROM (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text)) ids_to_delete_1)))
 SET client_min_messages TO WARNING;
 -- this query falls back repartitioned insert/select since we've some hard
 -- errors in the INSERT ... SELECT pushdown which prevents to fallback to
 -- recursive planning
 SELECT * FROM
 coordinator_plan($Q$
 EXPLAIN (costs off)
 WITH ids_to_upsert AS
 (
 	SELECT tenant_id FROM distributed_table WHERE dept > 7
 )
 INSERT INTO distributed_table
        SELECT distributed_table.tenant_id FROM ids_to_upsert, distributed_table
        		WHERE  distributed_table.tenant_id = ids_to_upsert.tenant_id
        	ON CONFLICT (tenant_id) DO UPDATE SET dept = 8;
 $Q$) s
 WHERE s LIKE '%INSERT/SELECT method%';
  query_plan 
 ------------
-   INSERT/SELECT method: repartition
-(1 row)
+(0 rows)
 
 SET client_min_messages TO DEBUG1;
 -- the following query is very similar to the above one
 -- but this time the query is pulled to coordinator since
 -- we return before hitting any hard errors
 WITH ids_to_insert AS
 (
 	SELECT (tenant_id::int * 100)::text as tenant_id FROM distributed_table WHERE dept > 7
 )
 INSERT INTO distributed_table
        SELECT DISTINCT ids_to_insert.tenant_id FROM ids_to_insert, distributed_table
        		WHERE  distributed_table.tenant_id < ids_to_insert.tenant_id;
-DEBUG:  cannot perform distributed INSERT INTO ... SELECT because the partition columns in the source table and subquery do not match
-DETAIL:  Subquery contains an expression that is not a simple column reference in the same position as the target table's partition column.
-HINT:  Ensure the target table's partition column has a corresponding simple column reference to a distributed table's partition column in the subquery.
-DEBUG:  CTE ids_to_insert is going to be inlined via distributed planning
-DEBUG:  generating subplan XXX_1 for subquery SELECT (((tenant_id)::integer OPERATOR(pg_catalog.*) 100))::text AS tenant_id FROM with_dml.distributed_table WHERE (dept OPERATOR(pg_catalog.>) 7)
-DEBUG:  generating subplan XXX_2 for subquery SELECT DISTINCT ids_to_insert.tenant_id FROM (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text)) ids_to_insert, with_dml.distributed_table WHERE (distributed_table.tenant_id OPERATOR(pg_catalog.<) ids_to_insert.tenant_id)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT tenant_id FROM (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text)) citus_insert_select_subquery
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 -- not a very meaningful query
 -- but has two modifying CTEs along with another
 -- modify statement
 -- We need to force 1 connection per placement
 -- otherwise the coordinator insert select fails
 -- since COPY cannot be executed
 SET citus.force_max_query_parallelization TO on;
 -- We are reducing the log level here to avoid alternative test output
 -- in PG15 because of the change in the display of SQL-standard
 -- function's arguments in INSERT/SELECT in PG15.
@@ -149,38 +132,31 @@
 SET citus.force_max_query_parallelization TO off;
 SET client_min_messages TO DEBUG1;
 -- CTE inside the UPDATE statement
 UPDATE
 	second_distributed_table
 SET dept =
 		(WITH  vals AS (
 		SELECT DISTINCT tenant_id::int FROM distributed_table
 		) select * from vals where tenant_id = 8 )
 		WHERE dept = 8;
-DEBUG:  CTE vals is going to be inlined via distributed planning
-DEBUG:  generating subplan XXX_1 for subquery SELECT DISTINCT (tenant_id)::integer AS tenant_id FROM with_dml.distributed_table
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE with_dml.second_distributed_table SET dept = (SELECT vals.tenant_id FROM (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id integer)) vals WHERE (vals.tenant_id OPERATOR(pg_catalog.=) 8)) WHERE (dept OPERATOR(pg_catalog.=) 8)
 -- Subquery inside the UPDATE statement
 UPDATE
 	second_distributed_table
 SET dept =
 		(SELECT DISTINCT tenant_id::int FROM distributed_table WHERE tenant_id = '9')
 		WHERE dept = 8;
-DEBUG:  generating subplan XXX_1 for subquery SELECT DISTINCT (tenant_id)::integer AS tenant_id FROM with_dml.distributed_table WHERE (tenant_id OPERATOR(pg_catalog.=) '9'::text)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE with_dml.second_distributed_table SET dept = (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id integer)) WHERE (dept OPERATOR(pg_catalog.=) 8)
 -- delete all remaining tenants
 WITH ids_to_delete AS (
   SELECT tenant_id FROM distributed_table
 )
 DELETE FROM distributed_table WHERE tenant_id = ANY(SELECT tenant_id FROM ids_to_delete);
-DEBUG:  generating subplan XXX_1 for CTE ids_to_delete: SELECT tenant_id FROM with_dml.distributed_table
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: DELETE FROM with_dml.distributed_table WHERE (tenant_id OPERATOR(pg_catalog.=) ANY (SELECT ids_to_delete.tenant_id FROM (SELECT intermediate_result.tenant_id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(tenant_id text)) ids_to_delete))
 WITH ids_to_delete AS (
   SELECT id FROM reference_table
 )
 DELETE FROM reference_table WHERE id = ANY(SELECT id FROM ids_to_delete);
 RESET client_min_messages;
 DROP SCHEMA with_dml CASCADE;
 NOTICE:  drop cascades to 3 other objects
 DETAIL:  drop cascades to table distributed_table
 drop cascades to table second_distributed_table
 drop cascades to table reference_table
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_index_statements.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_index_statements.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_index_statements.out.modified	2022-11-09 13:38:15.899312446 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_index_statements.out.modified	2022-11-09 13:38:15.919312446 +0300
@@ -11,59 +11,40 @@
 SET search_path TO multi_index_statements;
 SET citus.next_shard_id TO 102080;
 CREATE TABLE index_test_range(a int, b int, c int);
 SELECT create_distributed_table('index_test_range', 'a', 'range');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('index_test_range');
- master_create_empty_shard
----------------------------------------------------------------------
-                    102080
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 SELECT master_create_empty_shard('index_test_range');
- master_create_empty_shard
----------------------------------------------------------------------
-                    102081
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 SET citus.shard_count TO 8;
 SET citus.shard_replication_factor TO 2;
 CREATE TABLE index_test_hash(a int, b int, c int, a_text text, b_text text);
 SELECT create_distributed_table('index_test_hash', 'a', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE index_test_append(a int, b int, c int);
 SELECT create_distributed_table('index_test_append', 'a', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('index_test_append');
- master_create_empty_shard
----------------------------------------------------------------------
-                    102090
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 SELECT master_create_empty_shard('index_test_append');
- master_create_empty_shard
----------------------------------------------------------------------
-                    102091
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 --
 -- CREATE INDEX
 --
 -- Verify that we can create different types of indexes
 CREATE INDEX lineitem_orderkey_index ON public.lineitem (l_orderkey);
 CREATE INDEX lineitem_partkey_desc_index ON public.lineitem (l_partkey DESC);
 CREATE INDEX lineitem_partial_index ON public.lineitem (l_shipdate)
 	WHERE l_shipdate < '1995-01-01';
 CREATE INDEX lineitem_colref_index ON public.lineitem (record_ne(lineitem.*, NULL));
 SET client_min_messages = ERROR; -- avoid version dependent warning about WAL
@@ -142,22 +123,23 @@
 CREATE TABLE local_table (id integer, name text);
 CREATE INDEX CONCURRENTLY ON local_table(id);
 -- Vefify we don't warn out on CLUSTER command for local tables
 CREATE INDEX CONCURRENTLY local_table_index ON local_table(id);
 CLUSTER local_table USING local_table_index;
 DROP TABLE local_table;
 -- Verify that we can run CLUSTER command
 CLUSTER index_test_hash USING index_test_hash_index_a;
 -- Verify that we ERROR on CLUSTER VERBOSE
 CLUSTER VERBOSE index_test_hash USING index_test_hash_index_a;
-ERROR:  cannot run CLUSTER command
-DETAIL:  VERBOSE option is currently unsupported for distributed tables.
+INFO:  clustering "multi_index_statements.index_test_hash" using sequential scan and sort
+INFO:  "index_test_hash": found 0 removable, 0 nonremovable row versions in 0 pages
+DETAIL:  0 dead row versions cannot be removed yet.
 -- Verify that we WARN on CLUSTER ALL
 CLUSTER;
 WARNING:  not propagating CLUSTER command to worker nodes
 HINT:  Provide a specific table in order to CLUSTER distributed tables.
 -- Verify that all indexes got created on the master node and one of the workers
 SELECT * FROM pg_indexes WHERE tablename = 'lineitem' or tablename like 'index_test_%' ORDER BY indexname;
        schemaname       |    tablename     |               indexname                | tablespace |                                                                                  indexdef                                                                                   
 ------------------------+------------------+----------------------------------------+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  multi_index_statements | index_test_hash  | index_test_hash_a_b_idx                |            | CREATE INDEX index_test_hash_a_b_idx ON multi_index_statements.index_test_hash USING btree (a) INCLUDE (b) WHERE (value_plus_one(c) > 10)
  multi_index_statements | index_test_hash  | index_test_hash_a_idx                  |            | CREATE INDEX index_test_hash_a_idx ON multi_index_statements.index_test_hash USING btree (a) WHERE (value_plus_one(c) > 10)
@@ -188,61 +170,59 @@
  public                 | lineitem         | lineitem_partial_index                 |            | CREATE INDEX lineitem_partial_index ON public.lineitem USING btree (l_shipdate) WHERE (l_shipdate < '01-01-1995'::date)
  public                 | lineitem         | lineitem_partkey_desc_index            |            | CREATE INDEX lineitem_partkey_desc_index ON public.lineitem USING btree (l_partkey DESC)
  public                 | lineitem         | lineitem_pkey                          |            | CREATE UNIQUE INDEX lineitem_pkey ON public.lineitem USING btree (l_orderkey, l_linenumber)
  public                 | lineitem         | lineitem_time_index                    |            | CREATE INDEX lineitem_time_index ON public.lineitem USING btree (l_shipdate)
 (30 rows)
 
 \c - - - :worker_1_port
 SELECT count(*) FROM pg_indexes WHERE tablename = (SELECT relname FROM pg_class WHERE relname LIKE 'lineitem_%' ORDER BY relname LIMIT 1);
  count 
 -------
-     9
+     0
 (1 row)
 
 SELECT count(*) FROM pg_indexes WHERE tablename LIKE 'index_test_hash_%';
  count 
 -------
-   144
+     0
 (1 row)
 
 SELECT count(*) FROM pg_indexes WHERE tablename LIKE 'index_test_range_%';
  count 
 -------
-     6
+     0
 (1 row)
 
 SELECT count(*) FROM pg_indexes WHERE tablename LIKE 'index_test_append_%';
  count 
 -------
      0
 (1 row)
 
 -- Verify that we actually run the CLUSTER COMMAND
 SELECT sum(indisclustered::integer) FROM pg_index WHERE indrelid::regclass::text SIMILAR TO '%\d';
  sum 
 -----
-   8
+   0
 (1 row)
 
 \c - - - :master_port
 SET search_path TO multi_index_statements, public;
 -- Verify that we error out on unsupported statement types
 CREATE INDEX try_index ON lineitem (l_orderkey) TABLESPACE newtablespace;
-ERROR:  specifying tablespaces with CREATE INDEX statements is currently unsupported
+ERROR:  tablespace "newtablespace" does not exist
 CREATE UNIQUE INDEX try_unique_range_index ON index_test_range(b);
 ERROR:  creating unique indexes on non-partition columns is currently unsupported
 CREATE UNIQUE INDEX try_unique_range_index_partial ON index_test_range(b) WHERE c IS NOT NULL;
 ERROR:  creating unique indexes on non-partition columns is currently unsupported
 CREATE UNIQUE INDEX try_unique_hash_index ON index_test_hash(b);
-ERROR:  creating unique indexes on non-partition columns is currently unsupported
 CREATE UNIQUE INDEX try_unique_hash_index_partial ON index_test_hash(b) WHERE c IS NOT NULL;
-ERROR:  creating unique indexes on non-partition columns is currently unsupported
 CREATE UNIQUE INDEX try_unique_append_index ON index_test_append(b);
 ERROR:  creating unique indexes on append-partitioned tables is currently unsupported
 CREATE UNIQUE INDEX try_unique_append_index ON index_test_append(a);
 ERROR:  creating unique indexes on append-partitioned tables is currently unsupported
 CREATE UNIQUE INDEX try_unique_append_index_a_b ON index_test_append(a,b);
 ERROR:  creating unique indexes on append-partitioned tables is currently unsupported
 -- Verify that we error out in case of postgres errors on supported statement
 -- types.
 CREATE INDEX lineitem_orderkey_index ON lineitem (l_orderkey);
 ERROR:  relation "lineitem_orderkey_index" already exists
@@ -261,27 +241,27 @@
 REINDEX INDEX lineitem_orderkey_index;
 REINDEX TABLE lineitem;
 REINDEX SCHEMA public;
 REINDEX DATABASE regression;
 REINDEX SYSTEM regression;
 --
 -- DROP INDEX
 --
 -- Verify that we can't drop multiple indexes in a single command
 DROP INDEX lineitem_orderkey_index, lineitem_partial_index;
-ERROR:  cannot drop multiple distributed objects in a single command
-HINT:  Try dropping each object in a separate DROP command.
 -- Verify that we can succesfully drop indexes
 DROP INDEX lineitem_orderkey_index;
+ERROR:  index "lineitem_orderkey_index" does not exist
 DROP INDEX lineitem_orderkey_index_new;
 DROP INDEX lineitem_partkey_desc_index;
 DROP INDEX lineitem_partial_index;
+ERROR:  index "lineitem_partial_index" does not exist
 DROP INDEX lineitem_colref_index;
 -- Verify that we handle if exists statements correctly
 DROP INDEX non_existent_index;
 ERROR:  index "non_existent_index" does not exist
 DROP INDEX IF EXISTS non_existent_index;
 NOTICE:  index "non_existent_index" does not exist, skipping
 DROP INDEX IF EXISTS lineitem_orderkey_hash_index;
 DROP INDEX lineitem_orderkey_hash_index;
 ERROR:  index "lineitem_orderkey_hash_index" does not exist
 DROP INDEX index_test_range_index_a;
@@ -353,47 +333,46 @@
 ----------+------------
 (0 rows)
 
 SELECT * FROM pg_indexes WHERE tablename SIMILAR TO 'index_test_%\d' ORDER BY indexname;
  schemaname | tablename | indexname | tablespace | indexdef 
 ------------+-----------+-----------+------------+----------
 (0 rows)
 
 -- create index that will conflict with master operations
 CREATE INDEX CONCURRENTLY ith_b_idx_102089 ON multi_index_statements.index_test_hash_102089(b);
+ERROR:  schema "multi_index_statements" does not exist
 \c - - - :master_port
 SET search_path TO multi_index_statements;
 -- should fail because worker index already exists
 CREATE INDEX CONCURRENTLY ith_b_idx ON index_test_hash(b);
-ERROR:  CONCURRENTLY-enabled index command failed
-DETAIL:  CONCURRENTLY-enabled index commands can fail partially, leaving behind an INVALID index.
-HINT:  Use DROP INDEX CONCURRENTLY IF EXISTS to remove the invalid index, then retry the original command.
 -- the failure results in an INVALID index
 SELECT indisvalid AS "Index Valid?" FROM pg_index WHERE indexrelid='ith_b_idx'::regclass;
  Index Valid? 
 --------------
- f
+ t
 (1 row)
 
 -- we can clean it up and recreate with an DROP IF EXISTS
 DROP INDEX CONCURRENTLY IF EXISTS ith_b_idx;
 CREATE INDEX CONCURRENTLY ith_b_idx ON index_test_hash(b);
 SELECT indisvalid AS "Index Valid?" FROM pg_index WHERE indexrelid='ith_b_idx'::regclass;
  Index Valid? 
 --------------
  t
 (1 row)
 
 \c - - - :worker_1_port
 SET search_path TO multi_index_statements;
 -- now drop shard index to test partial master DROP failure
 DROP INDEX CONCURRENTLY ith_b_idx_102089;
+ERROR:  index "ith_b_idx_102089" does not exist
 \c - - - :master_port
 SET search_path TO multi_index_statements;
 SET citus.next_shard_id TO 103080;
 SET citus.shard_count TO 32;
 SET citus.shard_replication_factor TO 1;
 -- the following tests are intended to show that
 -- Citus does not get into self-deadlocks because
 -- of long index names. So, make sure that we have
 -- enough remote connections to trigger the case
 SET citus.force_max_query_parallelization TO ON;
@@ -413,22 +392,20 @@
 
 -- should be able to create short named indexes in parallel
 -- as there are no partitions even if the index name is too long
 SET client_min_messages TO DEBUG1;
 CREATE INDEX ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1
     ON test_index_creation1 USING btree
     (tenant_id, timeperiod);
 NOTICE:  identifier "ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1" will be truncated to "ix_test_index_creation1_ix_test_index_creation1_ix_test_index_c"
 DEBUG:  identifier "ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1" will be truncated to "ix_test_index_creation1_ix_test_index_creation1_ix_test_index_c"
 DETAIL:  from localhost:57638
-DEBUG:  identifier "ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1" will be truncated to "ix_test_index_creation1_ix_test_index_creation1_ix_test_index_c"
-DETAIL:  from localhost:xxxxx
 RESET client_min_messages;
 CREATE TABLE test_index_creation1_p2020_09_26 PARTITION OF test_index_creation1 FOR VALUES FROM ('2020-09-26 00:00:00') TO ('2020-09-27 00:00:00');
 CREATE TABLE test_index_creation1_p2020_09_27 PARTITION OF test_index_creation1 FOR VALUES FROM ('2020-09-27 00:00:00') TO ('2020-09-28 00:00:00');
 -- should switch to sequential execution as the index name on the partition is
 -- longer than 63
 SET client_min_messages TO DEBUG1;
 CREATE INDEX ix_test_index_creation2
     ON test_index_creation1 USING btree
     (tenant_id, timeperiod);
 DEBUG:  the index name on the shards of the partition is too long, switching to sequential and local execution mode to prevent self deadlocks: test_index_creation1_p2020_09_26_10314_tenant_id_timeperiod_idx
@@ -498,21 +475,21 @@
 -------
      0
 (1 row)
 
 \c - - - :worker_1_port
 SET search_path TO multi_index_statements;
 -- show that we have parent index_* only on the parent shards not on the partition shards
 SELECT count(*) FROM pg_index WHERE indrelid::regclass::text LIKE 'test_index_creation1_%' AND indexrelid::regclass::text LIKE 'parent_index%';
  count 
 -------
-    16
+     0
 (1 row)
 
 SELECT count(*) FROM pg_index WHERE indrelid::regclass::text LIKE 'test_index_creation1_p2020%' AND indexrelid::regclass::text LIKE 'parent_index%';
  count 
 -------
      0
 (1 row)
 
 \c - - - :master_port
 SET search_path TO multi_index_statements;
@@ -527,21 +504,21 @@
 -------
      1
 (1 row)
 
 \c - - - :worker_1_port
 SET search_path TO multi_index_statements;
 -- show that child indices of partition shards also inherit from parent indices of parent shards
 SELECT count(*) FROM pg_inherits WHERE inhrelid::regclass::text LIKE 'child_index\_%' AND inhparent::regclass::text LIKE 'parent_index\_%';
  count 
 -------
-    16
+     0
 (1 row)
 
 \c - - - :master_port
 SET search_path TO multi_index_statements;
 -- verify error check for partitioned index
 ALTER INDEX parent_index SET TABLESPACE foo;
 ERROR:  alter index ... set tablespace ... is currently unsupported
 DETAIL:  Only RENAME TO, SET (), RESET (), ATTACH PARTITION and SET STATISTICS are supported.
 -- drop parent index and show that child index will also be dropped
 DROP INDEX parent_index;
@@ -589,25 +566,22 @@
 FROM
 generate_Series(1, 32) x;
                                                                                                                                                                                                 ?column?                                                                                                                                                                                                
 --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  CREATE TABLE distributed_table(col1 int, col2 int, col3 int, col4 int, col5 int, col6 int, col7 int, col8 int, col9 int, col10 int, col11 int, col12 int, col13 int, col14 int, col15 int, col16 int, col17 int, col18 int, col19 int, col20 int, col21 int, col22 int, col23 int, col24 int, col25 int, col26 int, col27 int, col28 int, col29 int, col30 int, col31 int, col32 int, last_column int)
 (1 row)
 
 \gexec
 CREATE TABLE distributed_table(col1 int, col2 int, col3 int, col4 int, col5 int, col6 int, col7 int, col8 int, col9 int, col10 int, col11 int, col12 int, col13 int, col14 int, col15 int, col16 int, col17 int, col18 int, col19 int, col20 int, col21 int, col22 int, col23 int, col24 int, col25 int, col26 int, col27 int, col28 int, col29 int, col30 int, col31 int, col32 int, last_column int)
 SELECT create_distributed_table('distributed_table', 'last_column');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- try to use all 33 columns to create the index
 -- show that we error out as postgres would do
 SELECT
 'CREATE INDEX ON distributed_table(' ||
 string_Agg('col' || x::text || ',', ' ') ||
 ' last_column)'
 FROM
 generate_Series(1, 32) x;
                                                                                                                                ?column?                                                                                                                                
 -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
@@ -633,40 +607,36 @@
 RETURNS VOID AS
 $BODY$
 BEGIN
     CREATE INDEX ON distributed_table(last_column);
 END;
 $BODY$ LANGUAGE plpgsql;
 CREATE TABLE test_for_func(
     a int
 );
 SELECT create_distributed_table('test_for_func', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- create a function that depends on a relation that depends on an extension
 CREATE OR REPLACE FUNCTION function_on_table_depends_on_extension (
     p_table_name text)
 RETURNS TABLE (LIKE pg_dist_partition)
 AS $$
 BEGIN
   RETURN QUERY
   SELECT * FROM pg_dist_partition WHERE logicalrelid::regclass::text = p_table_name;
 END;
 $$ LANGUAGE plpgsql;
 SELECT logicalrelid FROM function_on_table_depends_on_extension('test_for_func');
  logicalrelid 
 --------------
- test_for_func
-(1 row)
+(0 rows)
 
 -- create a function that depends on a relation that does not depend on an extension
 CREATE TABLE local_test(a int);
 CREATE OR REPLACE FUNCTION function_on_table_does_not_depend_on_extension (
     input int)
 RETURNS TABLE (LIKE local_test)
 AS $$
 BEGIN
   RETURN QUERY
   SELECT * FROM local_test WHERE a = input;
@@ -709,22 +679,17 @@
  distributed_table_last_column_idx3
  distributed_table_last_column_idx4
 (5 rows)
 
 SET citus.force_max_query_parallelization TO OFF;
 SET client_min_messages TO ERROR;
 DROP INDEX f1;
 DROP INDEX ix_test_index_creation2;
 DROP INDEX ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1_ix_test_index_creation1;
 DROP INDEX CONCURRENTLY ith_b_idx;
-ERROR:  index "ith_b_idx_102089" does not exist
 -- the failure results in an INVALID index
 SELECT indisvalid AS "Index Valid?" FROM pg_index WHERE indexrelid='ith_b_idx'::regclass;
- Index Valid?
----------------------------------------------------------------------
- f
-(1 row)
-
+ERROR:  relation "ith_b_idx" does not exist at character 68
 -- final clean up
 DROP INDEX CONCURRENTLY IF EXISTS ith_b_idx;
 DROP SCHEMA multi_index_statements CASCADE;
 DROP SCHEMA multi_index_statements_2 CASCADE;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_alter_table_statements.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_alter_table_statements.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_alter_table_statements.out.modified	2022-11-09 13:38:17.029312441 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_alter_table_statements.out.modified	2022-11-09 13:38:17.049312441 +0300
@@ -24,80 +24,54 @@
 	l_comment varchar(44) not null
 	)
   WITH ( fillfactor = 80 );
 SELECT create_distributed_table('lineitem_alter', 'l_orderkey', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('lineitem_alter') AS shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 \set lineitem_1_data_file :abs_srcdir '/data/lineitem.1.data'
 copy lineitem_alter FROM :'lineitem_1_data_file' with (delimiter '|', append_to_shard :shardid);
+ERROR:  syntax error at or near ":"
 -- verify that the storage options made it to the table definitions
 SELECT relname, reloptions FROM pg_class WHERE relname = 'lineitem_alter';
     relname     |   reloptions    
 ----------------+-----------------
  lineitem_alter | {fillfactor=80}
 (1 row)
 
 \c - - - :worker_1_port
 SELECT relname, reloptions FROM pg_class WHERE relname LIKE 'lineitem_alter%' ORDER BY relname;
  relname | reloptions 
 ---------+------------
- lineitem_alter_220000 | {fillfactor=80}
-(1 row)
+(0 rows)
 
 \c - - - :master_port
 -- Verify that we can add columns
 ALTER TABLE lineitem_alter ADD COLUMN float_column FLOAT;
 ALTER TABLE lineitem_alter ADD COLUMN date_column DATE;
 ALTER TABLE lineitem_alter ADD COLUMN int_column1 INTEGER DEFAULT 1;
 ALTER TABLE lineitem_alter ADD COLUMN int_column2 INTEGER DEFAULT 2;
 ALTER TABLE lineitem_alter ADD COLUMN null_column INTEGER;
 -- show changed schema on one worker
 \c - - - :worker_1_port
 SELECT attname, atttypid::regtype
 FROM
     (SELECT oid FROM pg_class WHERE relname LIKE 'lineitem_alter_%' ORDER BY relname LIMIT 1) pc
     JOIN pg_attribute ON (pc.oid = pg_attribute.attrelid)
 ORDER BY attnum;
  attname | atttypid 
 ---------+----------
- tableoid        | oid
- cmax            | cid
- xmax            | xid
- cmin            | cid
- xmin            | xid
- ctid            | tid
- l_orderkey      | bigint
- l_partkey       | integer
- l_suppkey       | integer
- l_linenumber    | integer
- l_quantity      | numeric
- l_extendedprice | numeric
- l_discount      | numeric
- l_tax           | numeric
- l_returnflag    | character
- l_linestatus    | character
- l_shipdate      | date
- l_commitdate    | date
- l_receiptdate   | date
- l_shipinstruct  | character
- l_shipmode      | character
- l_comment       | character varying
- float_column    | double precision
- date_column     | date
- int_column1     | integer
- int_column2     | integer
- null_column     | integer
-(27 rows)
+(0 rows)
 
 \c - - - :master_port
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid='public.lineitem_alter'::regclass;
      Column      |         Type          | Modifiers 
 -----------------+-----------------------+-----------
  l_orderkey      | bigint                | not null
  l_partkey       | integer               | not null
  l_suppkey       | integer               | not null
  l_linenumber    | integer               | not null
  l_quantity      | numeric(15,2)         | not null
@@ -115,48 +89,44 @@
  float_column    | double precision      | 
  date_column     | date                  | 
  int_column1     | integer               | default 1
  int_column2     | integer               | default 2
  null_column     | integer               | 
 (21 rows)
 
 SELECT float_column, count(*) FROM lineitem_alter GROUP BY float_column;
  float_column | count 
 --------------+-------
-              |  6000
-(1 row)
+(0 rows)
 
 SELECT int_column1, count(*) FROM lineitem_alter GROUP BY int_column1;
  int_column1 | count 
 -------------+-------
-           1 |  6000
-(1 row)
+(0 rows)
 
 -- Verify that SET|DROP DEFAULT works
 ALTER TABLE lineitem_alter ALTER COLUMN float_column SET DEFAULT 1;
 ALTER TABLE lineitem_alter ALTER COLUMN int_column1 DROP DEFAULT;
 -- COPY to verify that default values take effect
 SELECT master_create_empty_shard('lineitem_alter') as shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 copy lineitem_alter (l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode, l_comment) FROM :'lineitem_1_data_file' with (delimiter '|', append_to_shard :shardid);
+ERROR:  syntax error at or near ":"
 SELECT float_column, count(*) FROM lineitem_alter GROUP BY float_column;
  float_column | count 
 --------------+-------
-              |  6000
-            1 |  6000
-(2 rows)
+(0 rows)
 
 SELECT int_column1, count(*) FROM lineitem_alter GROUP BY int_column1;
  int_column1 | count 
 -------------+-------
-             |  6000
-           1 |  6000
-(2 rows)
+(0 rows)
 
 -- Verify that SET NOT NULL works
 ALTER TABLE lineitem_alter ALTER COLUMN int_column2 SET NOT NULL;
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid='public.lineitem_alter'::regclass;
      Column      |         Type          |     Modifiers      
 -----------------+-----------------------+--------------------
  l_orderkey      | bigint                | not null
  l_partkey       | integer               | not null
  l_suppkey       | integer               | not null
  l_linenumber    | integer               | not null
@@ -178,23 +148,23 @@
  int_column2     | integer               | not null default 2
  null_column     | integer               | 
 (21 rows)
 
 -- Drop default so that NULLs will be inserted for this column
 ALTER TABLE lineitem_alter ALTER COLUMN int_column2 DROP DEFAULT;
 -- COPY should fail because it will try to insert NULLs for a NOT NULL column
 -- Note, this operation will create a table on the workers but it won't be in the metadata
 BEGIN;
 SELECT master_create_empty_shard('lineitem_alter') as shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 copy lineitem_alter (l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode, l_comment) FROM :'lineitem_1_data_file' with (delimiter '|', append_to_shard :shardid);
-ERROR:  null value in column "int_column2" violates not-null constraint
-DETAIL:  Failing row contains (1, 155190, 7706, 1, 17.00, 21168.23, 0.04, 0.02, N, O, 1996-03-13, 1996-02-12, 1996-03-22, DELIVER IN PERSON        , TRUCK     , egular courts above the, 1, null, null, null, null).
+ERROR:  syntax error at or near ":"
 END;
 -- Verify that DROP NOT NULL works
 ALTER TABLE lineitem_alter ALTER COLUMN int_column2 DROP NOT NULL;
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid='public.lineitem_alter'::regclass;
      Column      |         Type          | Modifiers 
 -----------------+-----------------------+-----------
  l_orderkey      | bigint                | not null
  l_partkey       | integer               | not null
  l_suppkey       | integer               | not null
  l_linenumber    | integer               | not null
@@ -212,34 +182,34 @@
  l_comment       | character varying(44) | not null
  float_column    | double precision      | default 1
  date_column     | date                  | 
  int_column1     | integer               | 
  int_column2     | integer               | 
  null_column     | integer               | 
 (21 rows)
 
 -- COPY should succeed now
 SELECT master_create_empty_shard('lineitem_alter') as shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 copy lineitem_alter (l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode, l_comment) FROM :'lineitem_1_data_file' with (delimiter '|', append_to_shard :shardid);
+ERROR:  syntax error at or near ":"
 SELECT count(*) from lineitem_alter;
  count 
 -------
- 18000
+     0
 (1 row)
 
 -- Verify that SET DATA TYPE works
 SELECT int_column2, pg_typeof(int_column2), count(*) from lineitem_alter GROUP BY int_column2;
  int_column2 | pg_typeof | count 
 -------------+-----------+-------
-             | integer   |  6000
-           2 | integer   | 12000
-(2 rows)
+(0 rows)
 
 ALTER TABLE lineitem_alter ALTER COLUMN int_column2 SET DATA TYPE FLOAT;
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid='public.lineitem_alter'::regclass;
      Column      |         Type          | Modifiers 
 -----------------+-----------------------+-----------
  l_orderkey      | bigint                | not null
  l_partkey       | integer               | not null
  l_suppkey       | integer               | not null
  l_linenumber    | integer               | not null
  l_quantity      | numeric(15,2)         | not null
@@ -257,53 +227,51 @@
  float_column    | double precision      | default 1
  date_column     | date                  | 
  int_column1     | integer               | 
  int_column2     | double precision      | 
  null_column     | integer               | 
 (21 rows)
 
 SELECT int_column2, pg_typeof(int_column2), count(*) from lineitem_alter GROUP BY int_column2;
  int_column2 | pg_typeof | count 
 -------------+-----------+-------
-             | double precision |  6000
-           2 | double precision | 12000
-(2 rows)
+(0 rows)
 
 -- Verify that DROP COLUMN works
 ALTER TABLE lineitem_alter DROP COLUMN int_column1;
 ALTER TABLE lineitem_alter DROP COLUMN float_column;
 ALTER TABLE lineitem_alter DROP COLUMN date_column;
 -- Verify that RENAME COLUMN works
 ALTER TABLE lineitem_alter RENAME COLUMN l_orderkey TO l_orderkey_renamed;
 SELECT SUM(l_orderkey_renamed) FROM lineitem_alter;
  sum 
 -----
- 53620791
+    
 (1 row)
 
 -- Verify that IF EXISTS works as expected
 ALTER TABLE non_existent_table ADD COLUMN new_column INTEGER;
 ERROR:  relation "non_existent_table" does not exist
 ALTER TABLE IF EXISTS non_existent_table ADD COLUMN new_column INTEGER;
 NOTICE:  relation "non_existent_table" does not exist, skipping
 ALTER TABLE IF EXISTS lineitem_alter ALTER COLUMN int_column2 SET DATA TYPE INTEGER;
 ALTER TABLE lineitem_alter DROP COLUMN non_existent_column;
 ERROR:  column "non_existent_column" of relation "lineitem_alter" does not exist
 ALTER TABLE lineitem_alter DROP COLUMN IF EXISTS non_existent_column;
 NOTICE:  column "non_existent_column" of relation "lineitem_alter" does not exist, skipping
 ALTER TABLE lineitem_alter DROP COLUMN IF EXISTS int_column2;
 -- Verify with IF EXISTS for extant table
 ALTER TABLE IF EXISTS lineitem_alter RENAME COLUMN l_orderkey_renamed TO l_orderkey;
 SELECT SUM(l_orderkey) FROM lineitem_alter;
  sum 
 -----
- 53620791
+    
 (1 row)
 
 SELECT "Column", "Type", "Modifiers" FROM table_desc WHERE relid='public.lineitem_alter'::regclass;
      Column      |         Type          | Modifiers 
 -----------------+-----------------------+-----------
  l_orderkey      | bigint                | not null
  l_partkey       | integer               | not null
  l_suppkey       | integer               | not null
  l_linenumber    | integer               | not null
  l_quantity      | numeric(15,2)         | not null
@@ -387,22 +355,20 @@
 ALTER TABLE lineitem_alter DROP CONSTRAINT IF EXISTS non_existent_contraint;
 NOTICE:  constraint "non_existent_contraint" of relation "lineitem_alter" does not exist, skipping
 ALTER TABLE lineitem_alter SET WITHOUT OIDS;
 ERROR:  alter table command is currently unsupported
 DETAIL:  Only ADD|DROP COLUMN, SET|DROP NOT NULL, SET|DROP DEFAULT, ADD|DROP|VALIDATE CONSTRAINT, SET (), RESET (), ENABLE|DISABLE|NO FORCE|FORCE ROW LEVEL SECURITY, ATTACH|DETACH PARTITION and TYPE subcommands are supported.
 -- Verify that we error out in case of postgres errors on supported statement
 -- types
 ALTER TABLE lineitem_alter ADD COLUMN new_column non_existent_type;
 ERROR:  type "non_existent_type" does not exist
 ALTER TABLE lineitem_alter ALTER COLUMN null_column SET NOT NULL;
-ERROR:  column "null_column" contains null values
-CONTEXT:  while executing command on localhost:xxxxx
 ALTER TABLE lineitem_alter ALTER COLUMN l_partkey SET DEFAULT 'a';
 ERROR:  invalid input syntax for type integer: "a"
 -- Verify that we error out on RENAME CONSTRAINT statement
 ALTER TABLE lineitem_alter RENAME CONSTRAINT constraint_a TO constraint_b;
 ERROR:  renaming constraints belonging to distributed tables is currently unsupported
 -- Verify that IF EXISTS works as expected with RENAME statements
 ALTER TABLE non_existent_table RENAME TO non_existent_table_renamed;
 ERROR:  relation "non_existent_table" does not exist
 ALTER TABLE IF EXISTS non_existent_table RENAME TO non_existent_table_renamed;
 NOTICE:  relation "non_existent_table" does not exist, skipping
@@ -422,21 +388,21 @@
  l_discount      | numeric(15,2)         | not null
  l_tax           | numeric(15,2)         | not null
  l_returnflag    | character(1)          | not null
  l_linestatus    | character(1)          | not null
  l_shipdate      | date                  | not null
  l_commitdate    | date                  | not null
  l_receiptdate   | date                  | not null
  l_shipinstruct  | character(25)         | not null
  l_shipmode      | character(10)         | not null
  l_comment       | character varying(44) | not null
- null_column     | integer               |
+ null_column     | integer               | not null
 (17 rows)
 
 -- verify that non-propagated ddl commands are allowed inside a transaction block
 SET citus.enable_ddl_propagation to false;
 BEGIN;
 CREATE INDEX temp_index_1 ON lineitem_alter(l_linenumber);
 COMMIT;
 SELECT indexname, tablename FROM pg_indexes WHERE tablename = 'lineitem_alter';
   indexname   |   tablename    
 --------------+----------------
@@ -473,21 +439,21 @@
  l_discount      | numeric(15,2)         | not null
  l_tax           | numeric(15,2)         | not null
  l_returnflag    | character(1)          | not null
  l_linestatus    | character(1)          | not null
  l_shipdate      | date                  | not null
  l_commitdate    | date                  | not null
  l_receiptdate   | date                  | not null
  l_shipinstruct  | character(25)         | not null
  l_shipmode      | character(10)         | not null
  l_comment       | character varying(44) | not null
- null_column     | integer               |
+ null_column     | integer               | not null
  first           | integer               | 
 (18 rows)
 
 SELECT "Column", "Type", "Definition" FROM index_attrs WHERE
     relid = 'temp_index_2'::regclass;
    Column   |  Type  | Definition 
 ------------+--------+------------
  l_orderkey | bigint | l_orderkey
 (1 row)
 
@@ -530,172 +496,162 @@
 SELECT indexname, tablename FROM pg_indexes WHERE tablename = 'lineitem_alter';
   indexname   |   tablename    
 --------------+----------------
  temp_index_2 | lineitem_alter
 (1 row)
 
 DROP INDEX temp_index_2;
 -- Add column on only one worker...
 \c - - - :worker_2_port
 ALTER TABLE lineitem_alter_220000 ADD COLUMN first integer;
+ERROR:  relation "lineitem_alter_220000" does not exist
 \c - - - :master_port
 -- and try to add it in a multi-statement block, which fails
 BEGIN;
 CREATE INDEX temp_index_2 ON lineitem_alter(l_orderkey);
 ALTER TABLE lineitem_alter ADD COLUMN first integer;
-ERROR:  column "first" of relation "lineitem_alter_220000" already exists
-CONTEXT:  while executing command on localhost:xxxxx
 COMMIT;
 -- Nothing from the block should have committed
 SELECT indexname, tablename FROM pg_indexes WHERE tablename = 'lineitem_alter';
   indexname   |   tablename    
 --------------+----------------
-(0 rows)
+ temp_index_2 | lineitem_alter
+(1 row)
 
 -- Create single-shard table (to avoid deadlocks in the upcoming test hackery)
 CREATE TABLE single_shard_items (id integer NOT NULL, name text);
 SET citus.shard_count TO 1;
 SET citus.shard_replication_factor TO 2;
 SELECT create_distributed_table('single_shard_items', 'id', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- Verify that ALTER TABLE .. REPLICATION IDENTITY [USING INDEX]* .. works
 CREATE UNIQUE INDEX replica_idx on single_shard_items(id);
 SELECT relreplident FROM pg_class WHERE relname = 'single_shard_items';
  relreplident 
 --------------
  d
 (1 row)
 
 SELECT run_command_on_workers('SELECT relreplident FROM pg_class WHERE relname LIKE ''single_shard_items_%'' LIMIT 1;');
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,d)
- (localhost,57638,t,d)
-(2 rows)
+ (localhost,57638,t,"")
+(1 row)
 
 ALTER TABLE single_shard_items REPLICA IDENTITY nothing;
 SELECT relreplident FROM pg_class WHERE relname = 'single_shard_items';
  relreplident 
 --------------
  n
 (1 row)
 
 SELECT run_command_on_workers('SELECT relreplident FROM pg_class WHERE relname LIKE ''single_shard_items_%'' LIMIT 1;');
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,n)
- (localhost,57638,t,n)
-(2 rows)
+ (localhost,57638,t,"")
+(1 row)
 
 ALTER TABLE single_shard_items REPLICA IDENTITY full;
 SELECT relreplident FROM pg_class WHERE relname = 'single_shard_items';
  relreplident 
 --------------
  f
 (1 row)
 
 SELECT run_command_on_workers('SELECT relreplident FROM pg_class WHERE relname LIKE ''single_shard_items_%'' LIMIT 1;');
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,f)
- (localhost,57638,t,f)
-(2 rows)
+ (localhost,57638,t,"")
+(1 row)
 
 ALTER TABLE single_shard_items REPLICA IDENTITY USING INDEX replica_idx;
 SELECT relreplident FROM pg_class WHERE relname = 'single_shard_items';
  relreplident 
 --------------
  i
 (1 row)
 
 SELECT run_command_on_workers('SELECT relreplident FROM pg_class WHERE relname LIKE ''single_shard_items_%'' LIMIT 1;');
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,i)
- (localhost,57638,t,i)
-(2 rows)
+ (localhost,57638,t,"")
+(1 row)
 
 ALTER TABLE single_shard_items REPLICA IDENTITY default, REPLICA IDENTITY USING INDEX replica_idx, REPLICA IDENTITY nothing;
 SELECT relreplident FROM pg_class WHERE relname = 'single_shard_items';
  relreplident 
 --------------
  n
 (1 row)
 
 SELECT run_command_on_workers('SELECT relreplident FROM pg_class WHERE relname LIKE ''single_shard_items_%'' LIMIT 1;');
  run_command_on_workers 
 ------------------------
- (localhost,57637,t,n)
- (localhost,57638,t,n)
-(2 rows)
+ (localhost,57638,t,"")
+(1 row)
 
 ALTER TABLE single_shard_items ADD COLUMN test_col int, REPLICA IDENTITY full;
 DROP INDEX replica_idx;
 ALTER TABLE single_shard_items REPLICA IDENTITY default;
 -- Drop the column from the worker...
 \c - - - :worker_2_port
 ALTER TABLE lineitem_alter_220000 DROP COLUMN first;
+ERROR:  relation "lineitem_alter_220000" does not exist
 -- Create table to trigger at-xact-end (deferred) failure
 CREATE TABLE ddl_commands (command text UNIQUE DEFERRABLE INITIALLY DEFERRED);
 -- Use an event trigger to log all DDL event tags in it
 SET citus.enable_metadata_sync TO OFF;
 CREATE FUNCTION log_ddl_tag() RETURNS event_trigger AS $ldt$
 	BEGIN
 		INSERT INTO ddl_commands VALUES (tg_tag);
 	END;
 $ldt$ LANGUAGE plpgsql;
 RESET citus.enable_metadata_sync;
 CREATE EVENT TRIGGER log_ddl_tag ON ddl_command_end EXECUTE PROCEDURE log_ddl_tag();
 \c - - - :master_port
 -- The above trigger will cause failure at transaction end on one placement.
 -- Citus always uses 2PC. 2PC should handle this "best" (no divergence)
 BEGIN;
 CREATE INDEX single_index_2 ON single_shard_items(id);
 CREATE INDEX single_index_3 ON single_shard_items(name);
 COMMIT;
-ERROR:  duplicate key value violates unique constraint "ddl_commands_command_key"
-DETAIL:  Key (command)=(CREATE INDEX) already exists.
-CONTEXT:  while executing command on localhost:xxxxx
 -- Nothing from the block should have committed
 SELECT indexname, tablename FROM pg_indexes WHERE tablename = 'single_shard_items' ORDER BY 1;
    indexname    |     tablename      
 ----------------+--------------------
-(0 rows)
+ single_index_2 | single_shard_items
+ single_index_3 | single_shard_items
+(2 rows)
 
 \c - - - :worker_2_port
 DROP EVENT TRIGGER log_ddl_tag;
 DROP FUNCTION log_ddl_tag();
 DROP TABLE ddl_commands;
 \c - - - :master_port
 -- Distributed SELECTs may appear after ALTER
 BEGIN;
 CREATE INDEX temp_index_2 ON lineitem_alter(l_orderkey);
+ERROR:  relation "temp_index_2" already exists
 SELECT count(*) FROM lineitem_alter;
- count
----------------------------------------------------------------------
- 18000
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 ROLLBACK;
 -- and before
 BEGIN;
 SELECT count(*) FROM lineitem_alter;
  count 
 -------
- 18000
+     0
 (1 row)
 
 CREATE INDEX temp_index_2 ON lineitem_alter(l_orderkey);
+ERROR:  relation "temp_index_2" already exists
 COMMIT;
 SELECT indexname, tablename FROM pg_indexes WHERE tablename = 'lineitem_alter';
   indexname   |   tablename    
 --------------+----------------
  temp_index_2 | lineitem_alter
 (1 row)
 
 DROP INDEX temp_index_2;
 -- verify that distributed ddl commands are allowed without transaction block as well
 -- Reminder: Now Citus always uses 2PC
@@ -709,77 +665,43 @@
 DROP INDEX temp_index_3;
 SELECT indexname, tablename FROM pg_indexes WHERE tablename = 'lineitem_alter';
  indexname | tablename 
 -----------+-----------
 (0 rows)
 
 -- verify that not any of shard placements are marked as failed when a query failure occurs
 CREATE TABLE test_ab (a int, b int);
 SET citus.shard_count TO 8;
 SELECT create_distributed_table('test_ab', 'a', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO test_ab VALUES (2, 10);
 INSERT INTO test_ab VALUES (2, 11);
 CREATE UNIQUE INDEX temp_unique_index_1 ON test_ab(a);
-ERROR:  could not create unique index "temp_unique_index_1_220011"
+ERROR:  could not create unique index "temp_unique_index_1"
 DETAIL:  Key (a)=(2) is duplicated.
-CONTEXT:  while executing command on localhost:xxxxx
 SELECT shardid FROM pg_dist_shard_placement NATURAL JOIN pg_dist_shard
 WHERE logicalrelid='test_ab'::regclass AND shardstate=3;
  shardid 
 ---------
 (0 rows)
 
 -- Check that the schema on the worker still looks reasonable
 \c - - - :worker_1_port
 SELECT attname, atttypid::regtype
 FROM
     (SELECT oid FROM pg_class WHERE relname LIKE 'lineitem_alter_%' ORDER BY relname LIMIT 1) pc
     JOIN pg_attribute ON (pc.oid = pg_attribute.attrelid)
 ORDER BY attnum;
  attname | atttypid 
 ---------+----------
- tableoid                      | oid
- cmax                          | cid
- xmax                          | xid
- cmin                          | cid
- xmin                          | xid
- ctid                          | tid
- l_orderkey                    | bigint
- l_partkey                     | integer
- l_suppkey                     | integer
- l_linenumber                  | integer
- l_quantity                    | numeric
- l_extendedprice               | numeric
- l_discount                    | numeric
- l_tax                         | numeric
- l_returnflag                  | character
- l_linestatus                  | character
- l_shipdate                    | date
- l_commitdate                  | date
- l_receiptdate                 | date
- l_shipinstruct                | character
- l_shipmode                    | character
- l_comment                     | character varying
- ........pg.dropped.17........ | -
- ........pg.dropped.18........ | -
- ........pg.dropped.19........ | -
- ........pg.dropped.20........ | -
- null_column                   | integer
- ........pg.dropped.22........ | -
- ........pg.dropped.23........ | -
- ........pg.dropped.24........ | -
-(30 rows)
+(0 rows)
 
 \c - - - :master_port
 -- verify that we can rename distributed tables
 SHOW citus.enable_ddl_propagation;
  citus.enable_ddl_propagation 
 ------------------------------
  on
 (1 row)
 
 ALTER TABLE lineitem_alter RENAME TO lineitem_renamed;
@@ -788,93 +710,78 @@
      relname      
 ------------------
  lineitem_renamed
 (1 row)
 
 -- show rename worked on one worker, too
 \c - - - :worker_1_port
 SELECT relname FROM pg_class WHERE relname LIKE 'lineitem_renamed%'  ORDER BY relname;
  relname 
 ---------
- lineitem_renamed_220000
- lineitem_renamed_220001
- lineitem_renamed_220003
-(3 rows)
+(0 rows)
 
 \c - - - :master_port
 -- revert it to original name
 ALTER TABLE lineitem_renamed RENAME TO lineitem_alter;
 -- show rename worked on one worker, too
 \c - - - :worker_1_port
 SELECT relname FROM pg_class WHERE relname LIKE 'lineitem_alter%' AND relname <> 'lineitem_alter_220002' /* failed copy trails */ ORDER BY relname;
  relname 
 ---------
- lineitem_alter_220000
- lineitem_alter_220001
- lineitem_alter_220003
-(3 rows)
+(0 rows)
 
 \c - - - :master_port
 -- verify that we can set and reset storage parameters
 ALTER TABLE lineitem_alter SET(fillfactor=40);
 SELECT relname, reloptions FROM pg_class WHERE relname = 'lineitem_alter';
     relname     |   reloptions    
 ----------------+-----------------
  lineitem_alter | {fillfactor=40}
 (1 row)
 
 \c - - - :worker_1_port
 SELECT relname, reloptions FROM pg_class WHERE relname LIKE 'lineitem_alter%' AND relname <> 'lineitem_alter_220002' /* failed copy trails */ ORDER BY relname;
  relname | reloptions 
 ---------+------------
- lineitem_alter_220000 | {fillfactor=40}
- lineitem_alter_220001 | {fillfactor=40}
- lineitem_alter_220003 | {fillfactor=40}
-(3 rows)
+(0 rows)
 
 \c - - - :master_port
 ALTER TABLE lineitem_alter RESET(fillfactor);
 SELECT relname, reloptions FROM pg_class WHERE relname = 'lineitem_alter';
     relname     | reloptions 
 ----------------+------------
  lineitem_alter | 
 (1 row)
 
 \c - - - :worker_1_port
 SELECT relname, reloptions FROM pg_class WHERE relname LIKE 'lineitem_alter%'  AND relname <> 'lineitem_alter_220002' /* failed copy trails */ ORDER BY relname;
  relname | reloptions 
 ---------+------------
- lineitem_alter_220000 |
- lineitem_alter_220001 |
- lineitem_alter_220003 |
-(3 rows)
+(0 rows)
 
 \c - - - :master_port
 -- verify that we can rename indexes on distributed tables
 CREATE INDEX temp_index_1 ON lineitem_alter(l_linenumber);
 ALTER INDEX temp_index_1 RENAME TO idx_lineitem_linenumber;
 -- verify rename is performed
 SELECT relname FROM pg_class WHERE relname = 'idx_lineitem_linenumber';
          relname         
 -------------------------
  idx_lineitem_linenumber
 (1 row)
 
 -- show rename worked on one worker, too
 \c - - - :worker_1_port
 SELECT relname FROM pg_class WHERE relname LIKE 'idx_lineitem_linenumber%' ORDER BY relname;
  relname 
 ---------
- idx_lineitem_linenumber_220000
- idx_lineitem_linenumber_220001
- idx_lineitem_linenumber_220003
-(3 rows)
+(0 rows)
 
 \c - - - :master_port
 -- now get rid of the index
 DROP INDEX idx_lineitem_linenumber;
 -- verify that we don't intercept DDL commands if propagation is turned off
 SET citus.enable_ddl_propagation to false;
 -- table rename statement can be performed on the coordinator only now
 ALTER TABLE lineitem_alter RENAME TO lineitem_renamed;
 -- verify rename is performed
 SELECT relname FROM pg_class WHERE relname = 'lineitem_alter' or relname = 'lineitem_renamed';
@@ -883,35 +790,35 @@
  lineitem_renamed
 (1 row)
 
 -- revert it to original name
 ALTER TABLE lineitem_renamed RENAME TO lineitem_alter;
 -- this column is added to master table and not workers
 ALTER TABLE lineitem_alter ADD COLUMN column_only_added_to_master int;
 -- verify newly added column is not present in a worker shard
 \c - - - :worker_1_port
 SELECT column_only_added_to_master FROM lineitem_alter_220000 LIMIT 0;
-ERROR:  column "column_only_added_to_master" does not exist
+ERROR:  relation "lineitem_alter_220000" does not exist
 \c - - - :master_port
 -- ddl propagation flag is reset to default, disable it again
 SET citus.enable_ddl_propagation to false;
 -- following query succeeds since it accesses an previously existing column
 SELECT l_orderkey FROM lineitem_alter LIMIT 0;
  l_orderkey 
 ------------
 (0 rows)
 
 -- make master and workers have the same schema again
 ALTER TABLE lineitem_alter DROP COLUMN column_only_added_to_master;
 -- now this should succeed
 SELECT * FROM lineitem_alter LIMIT 0;
- l_orderkey | l_partkey | l_suppkey | l_linenumber | l_quantity | l_extendedprice | l_discount | l_tax | l_returnflag | l_linestatus | l_shipdate | l_commitdate | l_receiptdate | l_shipinstruct | l_shipmode | l_comment | null_column
+ l_orderkey | l_partkey | l_suppkey | l_linenumber | l_quantity | l_extendedprice | l_discount | l_tax | l_returnflag | l_linestatus | l_shipdate | l_commitdate | l_receiptdate | l_shipinstruct | l_shipmode | l_comment | null_column | first 
 ------------+-----------+-----------+--------------+------------+-----------------+------------+-------+--------------+--------------+------------+--------------+---------------+----------------+------------+-----------+-------------+-------
 (0 rows)
 
 -- previously unsupported statements are accepted by postgresql now
 ALTER TABLE lineitem_alter ALTER COLUMN l_orderkey SET STATISTICS 100;
 ALTER TABLE lineitem_alter DROP CONSTRAINT IF EXISTS non_existent_contraint;
 NOTICE:  constraint "non_existent_contraint" of relation "lineitem_alter" does not exist, skipping
 ALTER TABLE lineitem_alter SET WITHOUT OIDS;
 -- distribution column still cannot be dropped.
 ALTER TABLE lineitem_alter DROP COLUMN l_orderkey;
@@ -931,25 +838,22 @@
  indexname | tablename 
 -----------+-----------
 (0 rows)
 
 \c - - - :master_port
 -- verify alter table and drop sequence in the same transaction does not cause deadlock
 SET citus.shard_count TO 4;
 SET citus.shard_replication_factor TO 2;
 CREATE TABLE sequence_deadlock_test (a serial, b serial);
 SELECT create_distributed_table('sequence_deadlock_test', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 BEGIN;
 ALTER TABLE sequence_deadlock_test ADD COLUMN c int;
 -- suppress notice message caused by DROP ... CASCADE to prevent pg version difference
 SET client_min_messages TO 'WARNING';
 DROP SEQUENCE sequence_deadlock_test_b_seq CASCADE;
 RESET client_min_messages;
 END;
 DROP TABLE sequence_deadlock_test;
 -- verify enable/disable trigger all works
 SET citus.shard_replication_factor TO 1;
@@ -970,44 +874,45 @@
 CREATE FUNCTION update_value() RETURNS trigger AS $up$
     BEGIN
 		NEW.value := 'trigger enabled';
 		RETURN NEW;
     END;
 $up$ LANGUAGE plpgsql;
 RESET citus.enable_metadata_sync;
 CREATE TRIGGER update_value
 BEFORE INSERT ON trigger_table_220017
 FOR EACH ROW EXECUTE PROCEDURE update_value();
+ERROR:  relation "trigger_table_220017" does not exist
 \c - - - :master_port
 INSERT INTO trigger_table VALUES (1, 'trigger disabled');
 SELECT value, count(*) FROM trigger_table GROUP BY value ORDER BY value;
       value       | count 
 ------------------+-------
- trigger enabled |     1
+ trigger disabled |     1
 (1 row)
 
 ALTER TABLE trigger_table DISABLE TRIGGER ALL;
 ERROR:  triggers are not supported on distributed tables
 INSERT INTO trigger_table VALUES (1, 'trigger disabled');
 SELECT value, count(*) FROM trigger_table GROUP BY value ORDER BY value;
       value       | count 
 ------------------+-------
- trigger enabled |     2
+ trigger disabled |     2
 (1 row)
 
 ALTER TABLE trigger_table ENABLE TRIGGER ALL;
 ERROR:  triggers are not supported on distributed tables
 INSERT INTO trigger_table VALUES (1, 'trigger disabled');
 SELECT value, count(*) FROM trigger_table GROUP BY value ORDER BY value;
       value       | count 
 ------------------+-------
- trigger enabled |     3
+ trigger disabled |     3
 (1 row)
 
 DROP TABLE trigger_table;
 -- test ALTER TABLE ALL IN TABLESPACE
 -- we expect that it will warn out
 \set tablespace_location :abs_srcdir '/data'
 CREATE TABLESPACE super_fast_ssd LOCATION :'tablespace_location';
 ALTER TABLE ALL IN TABLESPACE pg_default SET TABLESPACE super_fast_ssd;
 WARNING:  not propagating ALTER TABLE ALL IN TABLESPACE commands to worker nodes
 HINT:  Connect to worker nodes directly to manually move all tables.
@@ -1026,201 +931,164 @@
 -- should not be able to drop the table as non table owner
 DROP TABLE lineitem_alter;
 ERROR:  must be owner of table lineitem_alter
 \c - postgres - :master_port
 ALTER TABLE lineitem_alter OWNER TO alter_table_owner;
 \c - alter_table_owner - :master_port
 -- should be able to query the table as table owner
 SELECT count(*) FROM lineitem_alter;
  count 
 -------
- 18000
+     0
 (1 row)
 
 -- should be able to drop the table as table owner
 DROP TABLE lineitem_alter;
 -- check that nothing's left over on workers, other than the leftover shard created
 -- during the unsuccessful COPY
 \c - postgres - :worker_1_port
 SELECT relname FROM pg_class WHERE relname LIKE 'lineitem_alter%';
  relname 
 ---------
- lineitem_alter_220002
-(1 row)
+(0 rows)
 
 \c - - - :master_port
 -- drop the roles created
 REVOKE ALL ON SCHEMA PUBLIC FROM alter_table_owner;
 DROP ROLE alter_table_owner;
 -- Test alter table with drop table in the same transaction
 BEGIN;
 CREATE TABLE test_table_1(id int);
 SELECT create_distributed_table('test_table_1','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ALTER TABLE test_table_1 ADD CONSTRAINT u_key UNIQUE(id);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 DROP TABLE test_table_1;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 END;
 -- There should be no test_table_1 shard on workers
 \c - - - :worker_1_port
 SELECT relname FROM pg_class WHERE relname LIKE 'test_table_1%';
  relname 
 ---------
 (0 rows)
 
 \c - - - :master_port
 -- verify logged info is propagated to workers when distributing the table
 CREATE TABLE logged_test(id int);
 ALTER TABLE logged_test SET UNLOGGED;
 SELECT create_distributed_table('logged_test', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 \c - - - :worker_1_port
 SELECT relname, CASE relpersistence WHEN 'u' THEN 'unlogged' WHEN 'p' then 'logged' ELSE 'unknown' END AS logged_info FROM pg_class WHERE relname ~ 'logged_test_' ORDER BY relname;
  relname | logged_info 
 ---------+-------------
- logged_test_220022 | unlogged
- logged_test_220023 | unlogged
- logged_test_220024 | unlogged
- logged_test_220025 | unlogged
-(4 rows)
+(0 rows)
 
 \c - - - :master_port
 -- verify SET LOGGED/UNLOGGED works after distributing the table
 ALTER TABLE logged_test SET LOGGED;
 SELECT relname, CASE relpersistence WHEN 'u' THEN 'unlogged' WHEN 'p' then 'logged' ELSE 'unknown' END AS logged_info FROM pg_class WHERE relname ~ 'logged_test*' ORDER BY relname;
    relname   | logged_info 
 -------------+-------------
  logged_test | logged
 (1 row)
 
 \c - - - :worker_1_port
 SELECT relname, CASE relpersistence WHEN 'u' THEN 'unlogged' WHEN 'p' then 'logged' ELSE 'unknown' END AS logged_info FROM pg_class WHERE relname ~ 'logged_test_' ORDER BY relname;
  relname | logged_info 
 ---------+-------------
- logged_test_220022 | logged
- logged_test_220023 | logged
- logged_test_220024 | logged
- logged_test_220025 | logged
-(4 rows)
+(0 rows)
 
 \c - - - :master_port
 ALTER TABLE logged_test SET UNLOGGED;
 SELECT relname, CASE relpersistence WHEN 'u' THEN 'unlogged' WHEN 'p' then 'logged' ELSE 'unknown' END AS logged_info FROM pg_class WHERE relname ~ 'logged_test*' ORDER BY relname;
    relname   | logged_info 
 -------------+-------------
  logged_test | unlogged
 (1 row)
 
 \c - - - :worker_1_port
 SELECT relname, CASE relpersistence WHEN 'u' THEN 'unlogged' WHEN 'p' then 'logged' ELSE 'unknown' END AS logged_info FROM pg_class WHERE relname ~ 'logged_test_' ORDER BY relname;
  relname | logged_info 
 ---------+-------------
- logged_test_220022 | unlogged
- logged_test_220023 | unlogged
- logged_test_220024 | unlogged
- logged_test_220025 | unlogged
-(4 rows)
+(0 rows)
 
 \c - - - :master_port
 DROP TABLE logged_test;
 -- Test WITH options on a normal simple hash-distributed table
 CREATE TABLE hash_dist(id bigint primary key, f1 text) WITH (fillfactor=40);
 SELECT create_distributed_table('hash_dist','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- verify that the storage options made it to the table definitions
 SELECT relname, reloptions FROM pg_class WHERE relname = 'hash_dist';
   relname  |   reloptions    
 -----------+-----------------
  hash_dist | {fillfactor=40}
 (1 row)
 
 \c - - - :worker_1_port
 SELECT relname, reloptions FROM pg_class WHERE relkind = 'r' AND relname LIKE 'hash_dist_%' ORDER BY relname;
  relname | reloptions 
 ---------+------------
- hash_dist_220026 | {fillfactor=40}
- hash_dist_220027 | {fillfactor=40}
- hash_dist_220028 | {fillfactor=40}
- hash_dist_220029 | {fillfactor=40}
-(4 rows)
+(0 rows)
 
 \c - - - :master_port
 -- verify that we can set and reset index storage parameters
 ALTER INDEX hash_dist_pkey SET(fillfactor=40);
 SELECT relname, reloptions FROM pg_class WHERE relname = 'hash_dist_pkey';
     relname     |   reloptions    
 ----------------+-----------------
  hash_dist_pkey | {fillfactor=40}
 (1 row)
 
 \c - - - :worker_1_port
 SELECT relname, reloptions FROM pg_class WHERE relname LIKE 'hash_dist_pkey_%' ORDER BY relname;
  relname | reloptions 
 ---------+------------
- hash_dist_pkey_220026 | {fillfactor=40}
- hash_dist_pkey_220027 | {fillfactor=40}
- hash_dist_pkey_220028 | {fillfactor=40}
- hash_dist_pkey_220029 | {fillfactor=40}
-(4 rows)
+(0 rows)
 
 \c - - - :master_port
 ALTER INDEX hash_dist_pkey RESET(fillfactor);
 SELECT relname, reloptions FROM pg_class WHERE relname = 'hash_dist_pkey';
     relname     | reloptions 
 ----------------+------------
  hash_dist_pkey | 
 (1 row)
 
 \c - - - :worker_1_port
 SELECT relname, reloptions FROM pg_class WHERE relname LIKE 'hash_dist_pkey_%' ORDER BY relname;
  relname | reloptions 
 ---------+------------
- hash_dist_pkey_220026 |
- hash_dist_pkey_220027 |
- hash_dist_pkey_220028 |
- hash_dist_pkey_220029 |
-(4 rows)
+(0 rows)
 
 \c - - - :master_port
 -- verify error message on ALTER INDEX, SET TABLESPACE is unsupported
 ALTER INDEX hash_dist_pkey SET TABLESPACE foo;
-ERROR:  alter index ... set tablespace ... is currently unsupported
-DETAIL:  Only RENAME TO, SET (), RESET (), ATTACH PARTITION and SET STATISTICS are supported.
+ERROR:  tablespace "foo" does not exist
 -- verify that we can add indexes with new storage options
 CREATE UNIQUE INDEX another_index ON hash_dist(id) WITH (fillfactor=50);
 -- show the index and its storage options on coordinator, then workers
 SELECT relname, reloptions FROM pg_class WHERE relname = 'another_index';
     relname    |   reloptions    
 ---------------+-----------------
  another_index | {fillfactor=50}
 (1 row)
 
 \c - - - :worker_1_port
 SELECT relname, reloptions FROM pg_class WHERE relname LIKE 'another_index_%' ORDER BY relname;
  relname | reloptions 
 ---------+------------
- another_index_220026 | {fillfactor=50}
- another_index_220027 | {fillfactor=50}
- another_index_220028 | {fillfactor=50}
- another_index_220029 | {fillfactor=50}
-(4 rows)
+(0 rows)
 
 \c - - - :master_port
 -- get rid of the index
 DROP INDEX another_index;
 -- check if we fail properly when a column with un-supported constraint is added
 -- UNIQUE, PRIMARY KEY on non-distribution column is not supported
 -- CHECK, FOREIGN KEY, UNIQE, PRIMARY KEY cannot be added together with ADD COLUMN
 SET citus.shard_replication_factor TO 1;
 CREATE TABLE test_table_1(id int);
 SELECT create_distributed_table('test_table_1', 'id');
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_alter_table_add_constraints.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_alter_table_add_constraints.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_alter_table_add_constraints.out.modified	2022-11-09 13:38:17.529312439 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_alter_table_add_constraints.out.modified	2022-11-09 13:38:17.539312439 +0300
@@ -6,46 +6,40 @@
 SET citus.shard_count TO 32;
 ALTER SEQUENCE pg_catalog.pg_dist_shardid_seq RESTART 1450000;
 ALTER SEQUENCE pg_catalog.pg_dist_placement_placementid_seq RESTART 1450000;
 -- Check "PRIMARY KEY CONSTRAINT"
 CREATE TABLE products (
     product_no integer,
     name text,
     price numeric
 );
 SELECT create_distributed_table('products', 'product_no');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- Can only add primary key constraint on distribution column (or group of columns
 -- including distribution column)
 -- Command below should error out since 'name' is not a distribution column
 ALTER TABLE products ADD CONSTRAINT p_key PRIMARY KEY(name);
-ERROR:  cannot create constraint on "products"
-DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
 ALTER TABLE products ADD CONSTRAINT p_key PRIMARY KEY(product_no);
+ERROR:  multiple primary keys for table "products" are not allowed
 INSERT INTO products VALUES(1, 'product_1', 1);
 -- Should error out, since we are trying to add a new row having a value on p_key column
 -- conflicting with the existing row.
 INSERT INTO products VALUES(1, 'product_1', 1);
-ERROR:  duplicate key value violates unique constraint "p_key_1450001"
-DETAIL:  Key (product_no)=(1) already exists.
-CONTEXT:  while executing command on localhost:xxxxx
+ERROR:  duplicate key value violates unique constraint "p_key"
+DETAIL:  Key (name)=(product_1) already exists.
 ALTER TABLE products DROP CONSTRAINT p_key;
 INSERT INTO products VALUES(1, 'product_1', 1);
 -- Can not create constraint since it conflicts with the existing data
 ALTER TABLE products ADD CONSTRAINT p_key PRIMARY KEY(product_no);
-ERROR:  could not create unique index "p_key_1450001"
+ERROR:  could not create unique index "p_key"
 DETAIL:  Key (product_no)=(1) is duplicated.
-CONTEXT:  while executing command on localhost:xxxxx
 DROP TABLE products;
 -- Check "PRIMARY KEY CONSTRAINT" with reference table
 CREATE TABLE products_ref (
     product_no integer,
     name text,
     price numeric
 );
 SELECT create_reference_table('products_ref');
  create_reference_table 
 ------------------------
@@ -53,262 +47,265 @@
 (1 row)
 
 -- Can add PRIMARY KEY to any column
 ALTER TABLE products_ref ADD CONSTRAINT p_key PRIMARY KEY(name);
 ALTER TABLE products_ref DROP CONSTRAINT p_key;
 ALTER TABLE products_ref ADD CONSTRAINT p_key PRIMARY KEY(product_no);
 INSERT INTO products_ref VALUES(1, 'product_1', 1);
 -- Should error out, since we are trying to add new row having a value on p_key column
 -- conflicting with the existing row.
 INSERT INTO products_ref VALUES(1, 'product_1', 1);
-ERROR:  duplicate key value violates unique constraint "p_key_1450032"
+ERROR:  duplicate key value violates unique constraint "p_key_1450000"
 DETAIL:  Key (product_no)=(1) already exists.
 CONTEXT:  while executing command on localhost:57638
 DROP TABLE products_ref;
 -- Check "PRIMARY KEY CONSTRAINT" on append table
 CREATE TABLE products_append (
     product_no integer,
     name text,
     price numeric
 );
 SELECT create_distributed_table('products_append', 'product_no', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('products_append') AS shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 -- Can only add primary key constraint on distribution column (or group
 -- of columns including distribution column)
 -- Command below should error out since 'name' is not a distribution column
 ALTER TABLE products_append ADD CONSTRAINT p_key_name PRIMARY KEY(name);
 WARNING:  table "products_append" has a UNIQUE or EXCLUDE constraint
 DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
 HINT:  Consider using hash partitioning.
 ERROR:  cannot create constraint on "products_append"
 DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
 ALTER TABLE products_append ADD CONSTRAINT p_key PRIMARY KEY(product_no);
 WARNING:  table "products_append" has a UNIQUE or EXCLUDE constraint
 DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
 HINT:  Consider using hash partitioning.
 --- Error out since first and third rows have the same product_no
 COPY products_append FROM STDIN WITH (DELIMITER ',', append_to_shard :shardid);
-ERROR:  duplicate key value violates unique constraint "p_key_1450033"
-DETAIL:  Key (product_no)=(1) already exists.
+ERROR:  syntax error at or near ":"
+1, Product_1, 10
+2, Product_2, 15
+1, Product_3, 8
+\.
+invalid command \.
 DROP TABLE products_append;
+ERROR:  syntax error at or near "1"
 -- Check "UNIQUE CONSTRAINT"
 CREATE TABLE unique_test_table(id int, name varchar(20));
 SELECT create_distributed_table('unique_test_table', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- Can only add unique constraint on distribution column (or group
 -- of columns including distribution column)
 -- Command below should error out since 'name' is not a distribution column
 ALTER TABLE unique_test_table ADD CONSTRAINT unn_name UNIQUE(name);
-ERROR:  cannot create constraint on "unique_test_table"
-DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
 ALTER TABLE unique_test_table ADD CONSTRAINT unn_id UNIQUE(id);
 -- Error out, since table can not have two rows with same id.
 INSERT INTO unique_test_table VALUES(1, 'Ahmet');
 INSERT INTO unique_test_table VALUES(1, 'Mehmet');
-ERROR:  duplicate key value violates unique constraint "unn_id_1450035"
+ERROR:  duplicate key value violates unique constraint "unn_id"
 DETAIL:  Key (id)=(1) already exists.
-CONTEXT:  while executing command on localhost:xxxxx
 ALTER TABLE unique_test_table DROP CONSTRAINT unn_id;
 -- Insert row which will conflict with the next unique constraint command
 INSERT INTO unique_test_table VALUES(1, 'Mehmet');
 -- Can not create constraint since it conflicts with the existing data
 ALTER TABLE unique_test_table ADD CONSTRAINT unn_id UNIQUE(id);
-ERROR:  could not create unique index "unn_id_1450035"
+ERROR:  could not create unique index "unn_id"
 DETAIL:  Key (id)=(1) is duplicated.
-CONTEXT:  while executing command on localhost:xxxxx
 -- Can create unique constraint over multiple columns which must include
 -- distribution column
 ALTER TABLE unique_test_table ADD CONSTRAINT unn_id_name UNIQUE(id, name);
 -- Error out, since tables can not have two rows with same id and name.
 INSERT INTO unique_test_table VALUES(1, 'Mehmet');
-ERROR:  duplicate key value violates unique constraint "unn_id_name_1450035"
-DETAIL:  Key (id, name)=(1, Mehmet) already exists.
-CONTEXT:  while executing command on localhost:xxxxx
+ERROR:  duplicate key value violates unique constraint "unn_name"
+DETAIL:  Key (name)=(Mehmet) already exists.
 DROP TABLE unique_test_table;
 -- Check "UNIQUE CONSTRAINT" with reference table
 CREATE TABLE unique_test_table_ref(id int, name varchar(20));
 SELECT create_reference_table('unique_test_table_ref');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 -- We can add unique constraint on any column with reference tables
 ALTER TABLE unique_test_table_ref ADD CONSTRAINT unn_name UNIQUE(name);
 ALTER TABLE unique_test_table_ref ADD CONSTRAINT unn_id UNIQUE(id);
 -- Error out. Since the table can not have two rows with the same id.
 INSERT INTO unique_test_table_ref VALUES(1, 'Ahmet');
 INSERT INTO unique_test_table_ref VALUES(1, 'Mehmet');
-ERROR:  duplicate key value violates unique constraint "unn_id_1450066"
+ERROR:  duplicate key value violates unique constraint "unn_id_1450002"
 DETAIL:  Key (id)=(1) already exists.
 CONTEXT:  while executing command on localhost:57638
 -- We can add unique constraint with multiple columns
 ALTER TABLE unique_test_table_ref DROP CONSTRAINT unn_id;
 ALTER TABLE unique_test_table_ref ADD CONSTRAINT unn_id_name UNIQUE(id,name);
 -- Error out, since two rows can not have the same id or name.
 INSERT INTO unique_test_table_ref VALUES(1, 'Mehmet');
 DROP TABLE unique_test_table_ref;
 -- Check "UNIQUE CONSTRAINT" with append table
 CREATE TABLE unique_test_table_append(id int, name varchar(20));
 SELECT create_distributed_table('unique_test_table_append', 'id', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('unique_test_table_append') AS shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 -- Can only add unique constraint on distribution column (or group
 -- of columns including distribution column)
 -- Command below should error out since 'name' is not a distribution column
 ALTER TABLE unique_test_table_append ADD CONSTRAINT unn_name UNIQUE(name);
 WARNING:  table "unique_test_table_append" has a UNIQUE or EXCLUDE constraint
 DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
 HINT:  Consider using hash partitioning.
 ERROR:  cannot create constraint on "unique_test_table_append"
 DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
 ALTER TABLE unique_test_table_append ADD CONSTRAINT unn_id UNIQUE(id);
 WARNING:  table "unique_test_table_append" has a UNIQUE or EXCLUDE constraint
 DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
 HINT:  Consider using hash partitioning.
 -- Error out. Table can not have two rows with the same id.
 COPY unique_test_table_append FROM STDIN WITH (DELIMITER ',', append_to_shard :shardid);
-ERROR:  duplicate key value violates unique constraint "unn_id_1450067"
-DETAIL:  Key (id)=(X) already exists.
+ERROR:  syntax error at or near ":"
+1, Product_1
+2, Product_2
+1, Product_3
+\.
+invalid command \.
 DROP TABLE unique_test_table_append;
+ERROR:  syntax error at or near "1"
 -- Check "CHECK CONSTRAINT"
 CREATE TABLE products (
     product_no integer,
     name text,
     price numeric,
     discounted_price numeric
 );
 SELECT create_distributed_table('products', 'product_no');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- Can add column and table check constraints
 ALTER TABLE products ADD CONSTRAINT p_check CHECK(price > 0);
 ALTER TABLE products ADD CONSTRAINT p_multi_check CHECK(price > discounted_price);
 -- First and third queries will error out, because of conflicts with p_check and
 -- p_multi_check, respectively.
 INSERT INTO products VALUES(1, 'product_1', -1, -2);
-ERROR:  new row for relation "products_1450069" violates check constraint "p_check_1450069"
+ERROR:  new row for relation "products" violates check constraint "p_check"
 DETAIL:  Failing row contains (1, product_1, -1, -2).
-CONTEXT:  while executing command on localhost:xxxxx
 INSERT INTO products VALUES(1, 'product_1', 5, 3);
 INSERT INTO products VALUES(1, 'product_1', 2, 3);
-ERROR:  new row for relation "products_1450069" violates check constraint "p_multi_check_1450069"
+ERROR:  new row for relation "products" violates check constraint "p_multi_check"
 DETAIL:  Failing row contains (1, product_1, 2, 3).
-CONTEXT:  while executing command on localhost:xxxxx
 DROP TABLE products;
 -- Check "CHECK CONSTRAINT" with reference table
 CREATE TABLE products_ref (
     product_no integer,
     name text,
     price numeric,
     discounted_price numeric
 );
 SELECT create_reference_table('products_ref');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 -- Can add column and table check constraints
 ALTER TABLE products_ref ADD CONSTRAINT p_check CHECK(price > 0);
 ALTER TABLE products_ref ADD CONSTRAINT p_multi_check CHECK(price > discounted_price);
 -- First and third queries will error out, because of conflicts with p_check and
 -- p_multi_check, respectively.
 INSERT INTO products_ref VALUES(1, 'product_1', -1, -2);
-ERROR:  new row for relation "products_ref_1450100" violates check constraint "p_check_1450100"
+ERROR:  new row for relation "products_ref_1450004" violates check constraint "p_check_1450004"
 DETAIL:  Failing row contains (1, product_1, -1, -2).
 CONTEXT:  while executing command on localhost:57638
 INSERT INTO products_ref VALUES(1, 'product_1', 5, 3);
 INSERT INTO products_ref VALUES(1, 'product_1', 2, 3);
-ERROR:  new row for relation "products_ref_1450100" violates check constraint "p_multi_check_1450100"
+ERROR:  new row for relation "products_ref_1450004" violates check constraint "p_multi_check_1450004"
 DETAIL:  Failing row contains (1, product_1, 2, 3).
 CONTEXT:  while executing command on localhost:57638
 DROP TABLE products_ref;
 -- Check "CHECK CONSTRAINT" with append table
 CREATE TABLE products_append (
     product_no int,
     name varchar(20),
     price int,
     discounted_price int
 );
+ERROR:  relation "products_append" already exists
 SELECT create_distributed_table('products_append', 'product_no', 'append');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  table "products_append" is already distributed
 SELECT master_create_empty_shard('products_append') AS shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 -- Can add column and table check constraints
 ALTER TABLE products_append ADD CONSTRAINT p_check CHECK(price > 0);
+WARNING:  table "products_append" has a UNIQUE or EXCLUDE constraint
+DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
+HINT:  Consider using hash partitioning.
 ALTER TABLE products_append ADD CONSTRAINT p_multi_check CHECK(price > discounted_price);
+ERROR:  column "discounted_price" does not exist
 -- Error out,since the third row conflicting with the p_multi_check
 COPY products_append FROM STDIN WITH (DELIMITER ',', append_to_shard :shardid);
-ERROR:  new row for relation "products_append_1450101" violates check constraint "p_multi_check_1450101"
-DETAIL:  Failing row contains (1,  Product_3, 8, 10).
+ERROR:  syntax error at or near ":"
+1, Product_1, 10, 5
+2, Product_2, 15, 8
+1, Product_3, 8, 10
+\.
+invalid command \.
 DROP TABLE products_append;
+ERROR:  syntax error at or near "1"
 -- Check "EXCLUSION CONSTRAINT"
 CREATE TABLE products (
     product_no integer,
     name text,
     price numeric
 );
 SELECT create_distributed_table('products', 'product_no');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- Can only add exclusion constraint on distribution column (or group of columns
 -- including distribution column)
 -- Command below should error out since 'name' is not a distribution column
 ALTER TABLE products ADD CONSTRAINT exc_name EXCLUDE USING btree (name with =);
-ERROR:  cannot create constraint on "products"
-DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
 -- check that we can disable the constraint check for EXCLUDE
 BEGIN;
 SET LOCAL citus.allow_unsafe_constraints TO on;
 ALTER TABLE products ADD CONSTRAINT exc_name EXCLUDE USING btree (name with =);
+ERROR:  relation "exc_name" already exists
 -- not enforced across shards
 INSERT INTO products  VALUES (1,'boat',10.0);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 INSERT INTO products  VALUES (2,'boat',11.0);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- enforced within the shard
 INSERT INTO products  VALUES (1,'boat',12.0);
-ERROR:  conflicting key value violates exclusion constraint "exc_name_1450103"
-DETAIL:  Key (name)=(boat) conflicts with existing key (name)=(boat).
-CONTEXT:  while executing command on localhost:xxxxx
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 ROLLBACK;
 -- We can add composite exclusion
 ALTER TABLE products ADD CONSTRAINT exc_pno_name EXCLUDE USING btree (product_no with =, name with =);
 -- 4th command will error out since it conflicts with exc_pno_name constraint
 INSERT INTO products VALUES(1,'product_1', 5);
 INSERT INTO products VALUES(1,'product_2', 10);
 INSERT INTO products VALUES(2,'product_2', 5);
+ERROR:  conflicting key value violates exclusion constraint "exc_name"
+DETAIL:  Key (name)=(product_2) conflicts with existing key (name)=(product_2).
 INSERT INTO products VALUES(2,'product_2', 5);
-ERROR:  conflicting key value violates exclusion constraint "exc_pno_name_1450126"
-DETAIL:  Key (product_no, name)=(2, product_2) conflicts with existing key (product_no, name)=(2, product_2).
-CONTEXT:  while executing command on localhost:xxxxx
+ERROR:  conflicting key value violates exclusion constraint "exc_name"
+DETAIL:  Key (name)=(product_2) conflicts with existing key (name)=(product_2).
 DROP TABLE products;
 -- Check "EXCLUSION CONSTRAINT" with reference table
 CREATE TABLE products_ref (
     product_no integer,
     name text,
     price numeric
 );
 SELECT create_reference_table('products_ref');
  create_reference_table 
 ------------------------
@@ -316,75 +313,79 @@
 (1 row)
 
 -- We can add exclusion constraint on any column
 ALTER TABLE products_ref ADD CONSTRAINT exc_name EXCLUDE USING btree (name with =);
 -- We can add composite exclusion because none of pair of rows are conflicting
 ALTER TABLE products_ref ADD CONSTRAINT exc_pno_name EXCLUDE USING btree (product_no with =, name with =);
 -- Third insertion will error out, since it has the same name with second insertion
 INSERT INTO products_ref VALUES(1,'product_1', 5);
 INSERT INTO products_ref VALUES(1,'product_2', 10);
 INSERT INTO products_ref VALUES(2,'product_2', 5);
-ERROR:  conflicting key value violates exclusion constraint "exc_name_1450134"
+ERROR:  conflicting key value violates exclusion constraint "exc_name_1450006"
 DETAIL:  Key (name)=(product_2) conflicts with existing key (name)=(product_2).
 CONTEXT:  while executing command on localhost:57638
 DROP TABLE products_ref;
 -- Check "EXCLUSION CONSTRAINT" with append table
 CREATE TABLE products_append (
     product_no integer,
     name text,
     price numeric
 );
+ERROR:  relation "products_append" already exists
 SELECT create_distributed_table('products_append', 'product_no','append');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  table "products_append" is already distributed
 SELECT master_create_empty_shard('products_append') AS shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 -- Can only add exclusion constraint on distribution column (or group of column
 -- including distribution column)
 -- Command below should error out since 'name' is not a distribution column
 ALTER TABLE products_append ADD CONSTRAINT exc_name EXCLUDE USING btree (name with =);
 WARNING:  table "products_append" has a UNIQUE or EXCLUDE constraint
 DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
 HINT:  Consider using hash partitioning.
+WARNING:  table "products_append" has a UNIQUE or EXCLUDE constraint
+DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
+HINT:  Consider using hash partitioning.
 ERROR:  cannot create constraint on "products_append"
 DETAIL:  Distributed relations cannot have UNIQUE, EXCLUDE, or PRIMARY KEY constraints that do not include the partition column (with an equality operator if EXCLUDE).
 ALTER TABLE products_append ADD CONSTRAINT exc_pno_name EXCLUDE USING btree (product_no with =, name with =);
 WARNING:  table "products_append" has a UNIQUE or EXCLUDE constraint
 DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
 HINT:  Consider using hash partitioning.
+WARNING:  table "products_append" has a UNIQUE or EXCLUDE constraint
+DETAIL:  UNIQUE constraints, EXCLUDE constraints, and PRIMARY KEYs on append-partitioned tables cannot be enforced.
+HINT:  Consider using hash partitioning.
 -- Error out since first and third can not pass the exclusion check.
 COPY products_append FROM STDIN WITH (DELIMITER ',', append_to_shard :shardid);
-ERROR:  conflicting key value violates exclusion constraint "exc_pno_name_1450135"
-DETAIL:  Key (product_no, name)=(1,  Product_1) conflicts with existing key (product_no, name)=(1,  Product_1).
+ERROR:  syntax error at or near ":"
+1, Product_1, 10
+1, Product_2, 15
+1, Product_1, 8
+\.
+invalid command \.
 DROP TABLE products_append;
+ERROR:  syntax error at or near "1"
 -- Check "NOT NULL"
 CREATE TABLE products (
     product_no integer,
     name text,
     price numeric
 );
 SELECT create_distributed_table('products', 'product_no');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ALTER TABLE products ALTER COLUMN name SET NOT NULL;
 -- Insertions will error out since both product_no and name can not have NULL value
 INSERT INTO products VALUES(1,NULL,5);
 ERROR:  null value in column "name" of relation "products" violates not-null constraint
 DETAIL:  Failing row contains (1, null, 5).
-CONTEXT:  while executing command on localhost:xxxxx
 INSERT INTO products VALUES(NULL,'product_1', 5);
-ERROR:  cannot perform an INSERT with NULL in the partition column
 DROP TABLE products;
 -- Check "NOT NULL" with reference table
 CREATE TABLE products_ref (
     product_no integer,
     name text,
     price numeric
 );
 SELECT create_reference_table('products_ref');
  create_reference_table 
 ------------------------
@@ -398,293 +399,259 @@
 DETAIL:  Failing row contains (1, null, 5).
 CONTEXT:  while executing command on localhost:57638
 INSERT INTO products_ref VALUES(NULL,'product_1', 5);
 DROP TABLE products_ref;
 -- Check "NOT NULL" with append table
 CREATE TABLE products_append (
     product_no integer,
     name text,
     price numeric
 );
+ERROR:  relation "products_append" already exists
 SELECT create_distributed_table('products_append', 'product_no', 'append');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  table "products_append" is already distributed
 SELECT master_create_empty_shard('products_append') AS shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 ALTER TABLE products_append ALTER COLUMN name SET NOT NULL;
 -- Error out since name and product_no columns can not handle NULL value.
 COPY products_append FROM STDIN WITH (DELIMITER ',', append_to_shard :shardid);
+ERROR:  syntax error at or near ":"
+1, \N, 10
+invalid command \N,
+\N, Product_2, 15
+invalid command \N,
+1, Product_1, 8
+\.
+invalid command \.
 DROP TABLE products_append;
+ERROR:  syntax error at or near "1"
 -- Tests for ADD CONSTRAINT is not only subcommand
 CREATE TABLE products (
     product_no integer,
     name text,
     price numeric
 );
 SELECT create_distributed_table('products', 'product_no');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- Should error out since add constraint is not the single subcommand
 ALTER TABLE products ADD CONSTRAINT unn_1 UNIQUE(product_no, price), ADD CONSTRAINT unn_2 UNIQUE(product_no, name);
-ERROR:  cannot execute ADD CONSTRAINT command with other subcommands
-HINT:  You can issue each subcommand separately
 -- Tests for constraints without name
 -- Commands below should error out since constraints do not have the name
 ALTER TABLE products ADD UNIQUE(product_no);
-ERROR:  cannot create constraint without a name on a distributed table
 ALTER TABLE products ADD PRIMARY KEY(product_no);
-ERROR:  cannot create constraint without a name on a distributed table
 ALTER TABLE products ADD CHECK(product_no <> 0);
-ERROR:  cannot create constraint without a name on a distributed table
 ALTER TABLE products ADD EXCLUDE USING btree (product_no with =);
-ERROR:  cannot create constraint without a name on a distributed table
 -- ... with names, we can add/drop the constraints just fine
 ALTER TABLE products ADD CONSTRAINT nonzero_product_no CHECK(product_no <> 0);
 ALTER TABLE products ADD CONSTRAINT uniq_product_no EXCLUDE USING btree (product_no with =);
 ALTER TABLE products DROP CONSTRAINT nonzero_product_no;
 ALTER TABLE products DROP CONSTRAINT uniq_product_no;
 DROP TABLE products;
 -- Tests with transactions
 CREATE TABLE products (
     product_no integer,
     name text,
     price numeric,
     discounted_price numeric
 );
 SELECT create_distributed_table('products', 'product_no');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 BEGIN;
 INSERT INTO products VALUES(1,'product_1', 5);
 -- DDL should pick the right connections after a single INSERT
 ALTER TABLE products ADD CONSTRAINT unn_pno UNIQUE(product_no);
 ROLLBACK;
 -- check that we can disable the constraint check for CREATE UNIQUE INDEX
 BEGIN;
 SET LOCAL citus.allow_unsafe_constraints TO on;
 CREATE UNIQUE INDEX ON products (name, price);
 -- not enforced across shards
 INSERT INTO products  VALUES (1,'boat',10.0);
 INSERT INTO products  VALUES (2,'boat',11.0);
 -- enforced within the shard
 INSERT INTO products  VALUES (1,'boat',10.0);
-ERROR:  duplicate key value violates unique constraint "products_name_price_idx_1450203"
+ERROR:  duplicate key value violates unique constraint "products_name_price_idx"
 DETAIL:  Key (name, price)=(boat, 10.0) already exists.
-CONTEXT:  while executing command on localhost:xxxxx
 ROLLBACK;
 -- check that we can disable the constraint check for CREATE UNIQUE INDEX CONCURRENTLY
 SET citus.allow_unsafe_constraints TO on;
 CREATE UNIQUE INDEX CONCURRENTLY product_idx ON products (name, price);
 -- not enforced across shards
 INSERT INTO products  VALUES (1,'boat',10.0);
 INSERT INTO products  VALUES (2,'boat',11.0);
 -- enforced within the shard
 INSERT INTO products  VALUES (1,'boat',10.0);
-ERROR:  duplicate key value violates unique constraint "product_idx_1450203"
+ERROR:  duplicate key value violates unique constraint "product_idx"
 DETAIL:  Key (name, price)=(boat, 10.0) already exists.
-CONTEXT:  while executing command on localhost:xxxxx
 DROP INDEX product_idx;
 TRUNCATE products;
 RESET citus.allow_unsafe_constraints;
 -- check that we can disable the constraint check for ADD CONSTRAINT .. PRIMARY KEY
 BEGIN;
 SET LOCAL citus.allow_unsafe_constraints TO on;
 ALTER TABLE products ADD CONSTRAINT products_pk PRIMARY KEY (name, price);
 -- not enforced across shards
 INSERT INTO products  VALUES (1,'boat',10.0);
 INSERT INTO products  VALUES (2,'boat',11.0);
 -- enforced within the shard
 INSERT INTO products  VALUES (1,'boat',10.0);
-ERROR:  duplicate key value violates unique constraint "products_pk_1450203"
+ERROR:  duplicate key value violates unique constraint "products_pk"
 DETAIL:  Key (name, price)=(boat, 10.0) already exists.
-CONTEXT:  while executing command on localhost:xxxxx
 ROLLBACK;
 BEGIN;
 -- Add constraints
 ALTER TABLE products ADD CONSTRAINT unn_pno UNIQUE(product_no);
 ALTER TABLE products ADD CONSTRAINT check_price CHECK(price > discounted_price);
 ALTER TABLE products ALTER COLUMN product_no SET NOT NULL;
 ALTER TABLE products ADD CONSTRAINT p_key_product PRIMARY KEY(product_no);
 INSERT INTO products VALUES(1,'product_1', 10, 8);
 ROLLBACK;
 -- There should be no constraint on master and worker(s)
 SELECT "Constraint", "Definition" FROM table_checks WHERE relid='products'::regclass;
  Constraint | Definition 
 ------------+------------
 (0 rows)
 
 \c - - :public_worker_1_host :worker_1_port
 SELECT "Constraint", "Definition" FROM table_checks WHERE relid='public.products_1450202'::regclass;
- Constraint | Definition
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  relation "public.products_1450202" does not exist
 \c - - :master_host :master_port
 -- Tests to check the effect of rollback
 BEGIN;
 -- Add constraints (which will be rollbacked)
 ALTER TABLE products ADD CONSTRAINT unn_pno UNIQUE(product_no);
 ALTER TABLE products ADD CONSTRAINT check_price CHECK(price > discounted_price);
 ALTER TABLE products ADD CONSTRAINT p_key_product PRIMARY KEY(product_no);
 ROLLBACK;
 -- There should be no constraint on master and worker(s)
 SELECT "Constraint", "Definition" FROM table_checks WHERE relid='products'::regclass;
  Constraint | Definition 
 ------------+------------
 (0 rows)
 
 \c - - :public_worker_1_host :worker_1_port
 SELECT "Constraint", "Definition" FROM table_checks WHERE relid='public.products_1450202'::regclass;
- Constraint | Definition
----------------------------------------------------------------------
-(0 rows)
-
+ERROR:  relation "public.products_1450202" does not exist
 \c - - :master_host :master_port
 DROP TABLE products;
 SET citus.shard_count to 2;
 -- Test if the ALTER TABLE %s ADD %s PRIMARY KEY %s works
 CREATE SCHEMA sc1;
 CREATE TABLE sc1.alter_add_prim_key(x int, y int);
 CREATE UNIQUE INDEX CONCURRENTLY alter_pk_idx ON sc1.alter_add_prim_key(x);
 ALTER TABLE sc1.alter_add_prim_key ADD CONSTRAINT alter_pk_idx PRIMARY KEY USING INDEX alter_pk_idx;
 SELECT create_distributed_table('sc1.alter_add_prim_key', 'x');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT (run_command_on_workers($$
     SELECT
         kc.constraint_name
     FROM
         information_schema.table_constraints tc join information_schema.key_column_usage kc on (kc.table_name = tc.table_name and kc.table_schema = tc.table_schema and kc.constraint_name = tc.constraint_name)
     WHERE
         kc.table_schema = 'sc1' and tc.constraint_type = 'PRIMARY KEY' and kc.table_name LIKE 'alter_add_prim_key_%'
     ORDER BY
     1
     LIMIT
         1;
     $$)).*
 ORDER BY
     1,2,3,4;
  nodename  | nodeport | success | result 
 -----------+----------+---------+--------
- localhost |    57637 | t       | alter_pk_idx_1450234
- localhost |    57638 | t       | alter_pk_idx_1450234
-(2 rows)
+ localhost |    57638 | t       | 
+(1 row)
 
 CREATE SCHEMA sc2;
 CREATE TABLE sc2.alter_add_prim_key(x int, y int);
 SET search_path TO 'sc2';
 SELECT create_distributed_table('alter_add_prim_key', 'x');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE UNIQUE INDEX CONCURRENTLY alter_pk_idx ON alter_add_prim_key(x);
 ALTER TABLE alter_add_prim_key ADD CONSTRAINT alter_pk_idx PRIMARY KEY USING INDEX alter_pk_idx;
 SELECT (run_command_on_workers($$
     SELECT
         kc.constraint_name
     FROM
         information_schema.table_constraints tc join information_schema.key_column_usage kc on (kc.table_name = tc.table_name and kc.table_schema = tc.table_schema and kc.constraint_name = tc.constraint_name)
     WHERE
         kc.table_schema = 'sc2' and tc.constraint_type = 'PRIMARY KEY' and kc.table_name LIKE 'alter_add_prim_key_%'
     ORDER BY
     1
     LIMIT
         1;
     $$)).*
 ORDER BY
     1,2,3,4;
  nodename  | nodeport | success | result 
 -----------+----------+---------+--------
- localhost |    57637 | t       | alter_pk_idx_1450236
- localhost |    57638 | t       | alter_pk_idx_1450236
-(2 rows)
+ localhost |    57638 | t       | 
+(1 row)
 
 -- We are running almost the same test with a slight change on the constraint name because if the constraint has a different name than the index, Postgres renames the index.
 CREATE SCHEMA sc3;
 CREATE TABLE sc3.alter_add_prim_key(x int);
 INSERT INTO sc3.alter_add_prim_key(x) SELECT generate_series(1,100);
 SET search_path TO 'sc3';
 SELECT create_distributed_table('alter_add_prim_key', 'x');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$sc3.alter_add_prim_key$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE UNIQUE INDEX CONCURRENTLY alter_pk_idx ON alter_add_prim_key(x);
 ALTER TABLE alter_add_prim_key ADD CONSTRAINT a_constraint PRIMARY KEY USING INDEX alter_pk_idx;
 NOTICE:  ALTER TABLE / ADD CONSTRAINT USING INDEX will rename index "alter_pk_idx" to "a_constraint"
 SELECT (run_command_on_workers($$
     SELECT
         kc.constraint_name
     FROM
         information_schema.table_constraints tc join information_schema.key_column_usage kc on (kc.table_name = tc.table_name and kc.table_schema = tc.table_schema and kc.constraint_name = tc.constraint_name)
     WHERE
         kc.table_schema = 'sc3' and tc.constraint_type = 'PRIMARY KEY' and kc.table_name LIKE 'alter_add_prim_key_%'
     ORDER BY
     1
     LIMIT
         1;
     $$)).*
 ORDER BY
     1,2,3,4;
  nodename  | nodeport | success | result 
 -----------+----------+---------+--------
- localhost |    57637 | t       | a_constraint_1450238
- localhost |    57638 | t       | a_constraint_1450238
-(2 rows)
+ localhost |    57638 | t       | 
+(1 row)
 
 ALTER TABLE alter_add_prim_key DROP CONSTRAINT a_constraint;
 SELECT (run_command_on_workers($$
     SELECT
         kc.constraint_name
     FROM
         information_schema.table_constraints tc join information_schema.key_column_usage kc on (kc.table_name = tc.table_name and kc.table_schema = tc.table_schema and kc.constraint_name = tc.constraint_name)
     WHERE
         kc.table_schema = 'sc3' and tc.constraint_type = 'PRIMARY KEY' and kc.table_name LIKE 'alter_add_prim_key_%'
     ORDER BY
     1
     LIMIT
         1;
     $$)).*
 ORDER BY
     1,2,3,4;
  nodename  | nodeport | success | result 
 -----------+----------+---------+--------
- localhost |    57637 | t       |
  localhost |    57638 | t       | 
-(2 rows)
+(1 row)
 
 CREATE TABLE alter_add_unique(x int, y int);
 CREATE UNIQUE INDEX CONCURRENTLY alter_unique_idx ON alter_add_unique(x);
 SELECT create_distributed_table('alter_add_unique', 'x');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ALTER TABLE alter_add_unique ADD CONSTRAINT unique_constraint_test UNIQUE USING INDEX alter_unique_idx;
 NOTICE:  ALTER TABLE / ADD CONSTRAINT USING INDEX will rename index "alter_unique_idx" to "unique_constraint_test"
 ALTER TABLE alter_add_unique DROP CONSTRAINT unique_constraint_test;
 SET search_path TO 'public';
 DROP SCHEMA sc1 CASCADE;
 NOTICE:  drop cascades to table sc1.alter_add_prim_key
 DROP SCHEMA sc2 CASCADE;
 NOTICE:  drop cascades to table sc2.alter_add_prim_key
 DROP SCHEMA sc3 CASCADE;
 NOTICE:  drop cascades to 2 other objects
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/data_types.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/data_types.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/data_types.out.modified	2022-11-09 13:38:17.779312438 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/data_types.out.modified	2022-11-09 13:38:17.789312438 +0300
@@ -16,25 +16,22 @@
 	col11 bit varying(10)[], col12 bit varying(10)[][], col13 bit varying(10)[][][],
 	col14 bytea, col15 bytea[], col16 bytea[][], col17 bytea[][][],
 	col18 boolean, col19 boolean[], col20 boolean[][], col21 boolean[][][],
 	col22 inet, col23 inet[], col24 inet[][], col25 inet[][][],
 	col26 macaddr, col27 macaddr[], col28 macaddr[][], col29 macaddr[][][],
 	col30 numeric, col32 numeric[], col33 numeric[][], col34 numeric[][][],
 	col35 jsonb, col36 jsonb[], col37 jsonb[][], col38 jsonb[][][]
 );
 CREATE TABLE  data_types_table_local AS SELECT * FROM data_types_table;
 SELECT create_distributed_table('data_types_table', 'dist_key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO data_types_table (dist_key,col1, col2, col3, col4, col5, col6, col70, col7, col8, col9, col10, col11, col12, col13, col14, col15, col16, col17, col18, col19, col20, col21, col22, col23, col24, col25, col26, col27, col28, col29, col30, col32, col33, col34, col35, col36, col37, col38)
 VALUES (1,ARRAY[1], ARRAY[ARRAY[0,0,0]], ARRAY[ARRAY[ARRAY[0,0,0]]], ARRAY['1'], ARRAY[ARRAY['0','0','0']], ARRAY[ARRAY[ARRAY['0','0','0']]], '1', ARRAY[b'1'], ARRAY[ARRAY[b'0',b'0',b'0']], ARRAY[ARRAY[ARRAY[b'0',b'0',b'0']]], '11101',ARRAY[b'1'], ARRAY[ARRAY[b'01',b'01',b'01']], ARRAY[ARRAY[ARRAY[b'011',b'110',b'0000']]], '\xb4a8e04c0b', ARRAY['\xb4a8e04c0b'::BYTEA], ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA, '\xb4a8e04c0b'::BYTEA, '\xb4a8e04c0b'::BYTEA]], ARRAY[ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]]], '1', ARRAY[TRUE], ARRAY[ARRAY[1::boolean,TRUE,FALSE]], ARRAY[ARRAY[ARRAY[1::boolean,TRUE,FALSE]]], INET '192.168.1/24', ARRAY[INET '192.168.1.1'], ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']], ARRAY[ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']]],MACADDR '08:00:2b:01:02:03', ARRAY[MACADDR '08:00:2b:01:02:03'], ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']], ARRAY[ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']]], 690, ARRAY[1.1], ARRAY[ARRAY[0,0.111,0.15]], ARRAY[ARRAY[ARRAY[0,0,0]]], test_jsonb(), ARRAY[test_jsonb()], ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]], ARRAY[ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]]]),
        (2,ARRAY[1,2,3], ARRAY[ARRAY[1,2,3], ARRAY[5,6,7]], ARRAY[ARRAY[ARRAY[1,2,3]], ARRAY[ARRAY[5,6,7]], ARRAY[ARRAY[1,2,3]], ARRAY[ARRAY[5,6,7]]], ARRAY['1','2','3'], ARRAY[ARRAY['1','2','3'], ARRAY['5','6','7']], ARRAY[ARRAY[ARRAY['1','2','3']], ARRAY[ARRAY['5','6','7']], ARRAY[ARRAY['1','2','3']], ARRAY[ARRAY['5','6','7']]], '0', ARRAY[b'1',b'0',b'0'], ARRAY[ARRAY[b'1',b'1',b'0'], ARRAY[b'0',b'0',b'1']], ARRAY[ARRAY[ARRAY[b'1',b'1',b'1']], ARRAY[ARRAY[b'1','0','0']], ARRAY[ARRAY[b'1','1','1']], ARRAY[ARRAY[b'0','0','0']]], '00010', ARRAY[b'11',b'10',b'01'], ARRAY[ARRAY[b'11',b'010',b'101'], ARRAY[b'101',b'01111',b'1000001']], ARRAY[ARRAY[ARRAY[b'10000',b'111111',b'1101010101']], ARRAY[ARRAY[b'1101010','0','1']], ARRAY[ARRAY[b'1','1','11111111']], ARRAY[ARRAY[b'0000000','0','0']]], '\xb4a8e04c0b', ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA], ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA], ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]], ARRAY[ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]], ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]], ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]], ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]]], 'true', ARRAY[1::boolean,TRUE,FALSE], ARRAY[ARRAY[1::boolean,TRUE,FALSE], ARRAY[1::boolean,TRUE,FALSE]], ARRAY[ARRAY[ARRAY[1::boolean,TRUE,FALSE]], ARRAY[ARRAY[1::boolean,TRUE,FALSE]], ARRAY[ARRAY[1::boolean,TRUE,FALSE]], ARRAY[ARRAY[1::boolean,TRUE,FALSE]]],'0.0.0.0/32', ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24'], ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']], ARRAY[ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']], ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']], ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']], ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']]], '0800.2b01.0203', ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203'], ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']], ARRAY[ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']], ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']], ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']], ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']]], 0.99, ARRAY[1.1,2.22,3.33], ARRAY[ARRAY[1.55,2.66,3.88], ARRAY[11.5,10101.6,7111.1]], ARRAY[ARRAY[ARRAY[1,2,3]], ARRAY[ARRAY[5,6,7]], ARRAY[ARRAY[1.1,2.1,3]], ARRAY[ARRAY[5.0,6.0,7.0]]],test_jsonb(), ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()], ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()], ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]], ARRAY[ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]], ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]], ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]], ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]]]);
 -- insert the same data to the local node as well
 INSERT INTO data_types_table_local (dist_key,col1, col2, col3, col4, col5, col6, col70, col7, col8, col9, col10, col11, col12, col13, col14, col15, col16, col17, col18, col19, col20, col21, col22, col23, col24, col25, col26, col27, col28, col29, col30, col32, col33, col34, col35, col36, col37, col38)
 VALUES (1,ARRAY[1], ARRAY[ARRAY[0,0,0]], ARRAY[ARRAY[ARRAY[0,0,0]]], ARRAY['1'], ARRAY[ARRAY['0','0','0']], ARRAY[ARRAY[ARRAY['0','0','0']]], '1', ARRAY[b'1'], ARRAY[ARRAY[b'0',b'0',b'0']], ARRAY[ARRAY[ARRAY[b'0',b'0',b'0']]], '11101',ARRAY[b'1'], ARRAY[ARRAY[b'01',b'01',b'01']], ARRAY[ARRAY[ARRAY[b'011',b'110',b'0000']]], '\xb4a8e04c0b', ARRAY['\xb4a8e04c0b'::BYTEA], ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA, '\xb4a8e04c0b'::BYTEA, '\xb4a8e04c0b'::BYTEA]], ARRAY[ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]]], '1', ARRAY[TRUE], ARRAY[ARRAY[1::boolean,TRUE,FALSE]], ARRAY[ARRAY[ARRAY[1::boolean,TRUE,FALSE]]], INET '192.168.1/24', ARRAY[INET '192.168.1.1'], ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']], ARRAY[ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']]],MACADDR '08:00:2b:01:02:03', ARRAY[MACADDR '08:00:2b:01:02:03'], ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']], ARRAY[ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']]], 690, ARRAY[1.1], ARRAY[ARRAY[0,0.111,0.15]], ARRAY[ARRAY[ARRAY[0,0,0]]], test_jsonb(), ARRAY[test_jsonb()], ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]], ARRAY[ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]]]),
        (2,ARRAY[1,2,3], ARRAY[ARRAY[1,2,3], ARRAY[5,6,7]], ARRAY[ARRAY[ARRAY[1,2,3]], ARRAY[ARRAY[5,6,7]], ARRAY[ARRAY[1,2,3]], ARRAY[ARRAY[5,6,7]]], ARRAY['1','2','3'], ARRAY[ARRAY['1','2','3'], ARRAY['5','6','7']], ARRAY[ARRAY[ARRAY['1','2','3']], ARRAY[ARRAY['5','6','7']], ARRAY[ARRAY['1','2','3']], ARRAY[ARRAY['5','6','7']]], '0', ARRAY[b'1',b'0',b'0'], ARRAY[ARRAY[b'1',b'1',b'0'], ARRAY[b'0',b'0',b'1']], ARRAY[ARRAY[ARRAY[b'1',b'1',b'1']], ARRAY[ARRAY[b'1','0','0']], ARRAY[ARRAY[b'1','1','1']], ARRAY[ARRAY[b'0','0','0']]], '00010', ARRAY[b'11',b'10',b'01'], ARRAY[ARRAY[b'11',b'010',b'101'], ARRAY[b'101',b'01111',b'1000001']], ARRAY[ARRAY[ARRAY[b'10000',b'111111',b'1101010101']], ARRAY[ARRAY[b'1101010','0','1']], ARRAY[ARRAY[b'1','1','11111111']], ARRAY[ARRAY[b'0000000','0','0']]], '\xb4a8e04c0b', ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA], ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA], ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]], ARRAY[ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]], ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]], ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]], ARRAY[ARRAY['\xb4a8e04c0b'::BYTEA,'\x18a232a678'::BYTEA,'\x38b2697632'::BYTEA]]], 'true', ARRAY[1::boolean,TRUE,FALSE], ARRAY[ARRAY[1::boolean,TRUE,FALSE], ARRAY[1::boolean,TRUE,FALSE]], ARRAY[ARRAY[ARRAY[1::boolean,TRUE,FALSE]], ARRAY[ARRAY[1::boolean,TRUE,FALSE]], ARRAY[ARRAY[1::boolean,TRUE,FALSE]], ARRAY[ARRAY[1::boolean,TRUE,FALSE]]],'0.0.0.0/32', ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24'], ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']], ARRAY[ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']], ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']], ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']], ARRAY[ARRAY[INET '0.0.0.0', '0.0.0.0/32', '::ffff:fff0:1', '192.168.1/24']]], '0800.2b01.0203', ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203'], ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']], ARRAY[ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']], ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']], ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']], ARRAY[ARRAY[MACADDR '08002b-010203', MACADDR '08002b-010203', '08002b010203']]], 0.99, ARRAY[1.1,2.22,3.33], ARRAY[ARRAY[1.55,2.66,3.88], ARRAY[11.5,10101.6,7111.1]], ARRAY[ARRAY[ARRAY[1,2,3]], ARRAY[ARRAY[5,6,7]], ARRAY[ARRAY[1.1,2.1,3]], ARRAY[ARRAY[5.0,6.0,7.0]]],test_jsonb(), ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()], ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()], ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]], ARRAY[ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]], ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]], ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]], ARRAY[ARRAY[test_jsonb(),test_jsonb(),test_jsonb(),test_jsonb()]]]);
 -- different query/planning executiom types
 -- compare results with Postgres
 SELECT * FROM data_types_table
@@ -186,15 +183,14 @@
 -- test type names that start with underscore
 CREATE TYPE underscore_type_1 AS (a INT);
 CREATE TYPE _underscore_type_1 AS (a INT);
 CREATE TYPE underscore_type_2 AS ENUM ('a');
 CREATE TYPE _underscore_type_2 AS ENUM ('a');
 SELECT result FROM run_command_on_all_nodes('SELECT count(*) FROM pg_type WHERE typname LIKE ''%underscore\_type%''');
  result 
 --------
  8
  8
- 8
-(3 rows)
+(2 rows)
 
 SET client_min_messages TO ERROR;
 DROP SCHEMA data_types CASCADE;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/sequential_modifications.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/sequential_modifications.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/sequential_modifications.out.modified	2022-11-09 13:38:18.449312435 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/sequential_modifications.out.modified	2022-11-09 13:38:18.459312435 +0300
@@ -64,92 +64,89 @@
 -- disbable 2PC recovery since our tests will check that
 ALTER SYSTEM SET citus.recover_2pc_interval TO -1;
 SELECT pg_reload_conf();
  pg_reload_conf 
 ----------------
  t
 (1 row)
 
 CREATE TABLE test_table(a int, b int);
 SELECT create_distributed_table('test_table', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- not useful if not in transaction
 SELECT set_local_multi_shard_modify_mode_to_sequential();
  set_local_multi_shard_modify_mode_to_sequential 
 -------------------------------------------------
  
 (1 row)
 
 -- we should see #worker transactions
 -- when sequential mode is used
 SET citus.multi_shard_modify_mode TO 'sequential';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 ALTER TABLE test_table ADD CONSTRAINT a_check CHECK(a > 0);
 SELECT distributed_2PCs_are_equal_to_worker_count();
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
- t
+ f
 (1 row)
 
 -- we should see placement count # transactions
 -- when parallel mode is used
 SET citus.multi_shard_modify_mode TO 'parallel';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 ALTER TABLE test_table ADD CONSTRAINT b_check CHECK(b > 0);
 SELECT distributed_2PCs_are_equal_to_placement_count();
  distributed_2pcs_are_equal_to_placement_count 
 -----------------------------------------------
- t
+ f
 (1 row)
 
 -- even if 1PC used, we use 2PC as we modify replicated tables
 -- see distributed TXs in the pg_dist_transaction
 SET citus.multi_shard_modify_mode TO 'sequential';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 ALTER TABLE test_table ADD CONSTRAINT c_check CHECK(a > 0);
 SELECT no_distributed_2PCs();
  no_distributed_2pcs 
 ---------------------
- f
+ t
 (1 row)
 
 SET citus.multi_shard_modify_mode TO 'parallel';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 ALTER TABLE test_table ADD CONSTRAINT d_check CHECK(a > 0);
 SELECT no_distributed_2PCs();
  no_distributed_2pcs 
 ---------------------
- f
+ t
 (1 row)
 
 CREATE TABLE ref_test(a int);
 SELECT create_reference_table('ref_test');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 -- reference tables should always use 2PC
@@ -180,81 +177,78 @@
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
  t
 (1 row)
 
 -- tables with replication factor > 1 should also obey
 -- multi_shard_modify_mode
 SET citus.shard_replication_factor TO 2;
 CREATE TABLE test_table_rep_2 (a int);
 SELECT create_distributed_table('test_table_rep_2', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- even if 1PC used, we use 2PC with rep > 1
 SET citus.multi_shard_modify_mode TO 'sequential';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 CREATE INDEX test_table_rep_2_i_1 ON test_table_rep_2(a);
 SELECT no_distributed_2PCs();
  no_distributed_2pcs 
 ---------------------
- f
+ t
 (1 row)
 
 SET citus.multi_shard_modify_mode TO 'parallel';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 CREATE INDEX test_table_rep_2_i_2 ON test_table_rep_2(a);
 SELECT no_distributed_2PCs();
  no_distributed_2pcs 
 ---------------------
- f
+ t
 (1 row)
 
 -- 2PC should always use 2PC with rep > 1
 SET citus.multi_shard_modify_mode TO 'sequential';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 CREATE INDEX test_table_rep_2_i_3 ON test_table_rep_2(a);
 SELECT distributed_2PCs_are_equal_to_worker_count();
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
- t
+ f
 (1 row)
 
 SET citus.multi_shard_modify_mode TO 'parallel';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 CREATE INDEX test_table_rep_2_i_4 ON test_table_rep_2(a);
 SELECT distributed_2PCs_are_equal_to_placement_count();
  distributed_2pcs_are_equal_to_placement_count 
 -----------------------------------------------
- t
+ f
 (1 row)
 
 -- CREATE INDEX CONCURRENTLY should work fine with rep > 1
 -- with both 2PC and different parallel modes
 SET citus.multi_shard_modify_mode TO 'sequential';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
@@ -279,135 +273,122 @@
 SELECT no_distributed_2PCs();
  no_distributed_2pcs 
 ---------------------
  t
 (1 row)
 
 -- test TRUNCATE on sequential and parallel modes
 CREATE TABLE test_seq_truncate (a int);
 INSERT INTO test_seq_truncate SELECT i FROM generate_series(0, 100) i;
 SELECT create_distributed_table('test_seq_truncate', 'a');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$test_seq_ddl.test_seq_truncate$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- with parallel modification mode, we should see #shards records
 SET citus.multi_shard_modify_mode TO 'parallel';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 TRUNCATE test_seq_truncate;
 SELECT distributed_2PCs_are_equal_to_placement_count();
  distributed_2pcs_are_equal_to_placement_count 
 -----------------------------------------------
- t
+ f
 (1 row)
 
 -- with sequential modification mode, we should see #primary worker records
 SET citus.multi_shard_modify_mode TO 'sequential';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 TRUNCATE test_seq_truncate;
 SELECT distributed_2PCs_are_equal_to_worker_count();
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
- t
+ f
 (1 row)
 
 -- truncate with rep > 1 should work both in parallel and seq. modes
 CREATE TABLE test_seq_truncate_rep_2 (a int);
 SET citus.shard_replication_factor TO 2;
 SELECT create_distributed_table('test_seq_truncate_rep_2', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO test_seq_truncate_rep_2 SELECT i FROM generate_series(0, 100) i;
 SET citus.multi_shard_modify_mode TO 'sequential';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 TRUNCATE test_seq_truncate_rep_2;
 SELECT distributed_2PCs_are_equal_to_worker_count();
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
- t
+ f
 (1 row)
 
 SET citus.multi_shard_modify_mode TO 'parallel';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 TRUNCATE test_seq_truncate_rep_2;
 SELECT distributed_2PCs_are_equal_to_placement_count();
  distributed_2pcs_are_equal_to_placement_count 
 -----------------------------------------------
- t
+ f
 (1 row)
 
 CREATE TABLE multi_shard_modify_test (
         t_key integer not null,
         t_name varchar(25) not null,
         t_value integer not null);
 SELECT create_distributed_table('multi_shard_modify_test', 't_key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- with parallel modification mode, we should see #shards records
 SET citus.multi_shard_modify_mode TO 'parallel';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 DELETE FROM multi_shard_modify_test;
 SELECT distributed_2PCs_are_equal_to_placement_count();
  distributed_2pcs_are_equal_to_placement_count 
 -----------------------------------------------
- t
+ f
 (1 row)
 
 -- with sequential modification mode, we should see #primary worker records
 SET citus.multi_shard_modify_mode TO 'sequential';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 DELETE FROM multi_shard_modify_test;
 SELECT distributed_2PCs_are_equal_to_worker_count();
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
- t
+ f
 (1 row)
 
 -- one more realistic test with sequential inserts and truncate in the same tx
 INSERT INTO multi_shard_modify_test SELECT i, i::text, i FROM generate_series(0,100) i;
 BEGIN;
     INSERT INTO multi_shard_modify_test VALUES (1,'1',1), (2,'2',2), (3,'3',3), (4,'4',4);
     -- now switch to sequential mode to enable a successful TRUNCATE
     SELECT set_local_multi_shard_modify_mode_to_sequential();
  set_local_multi_shard_modify_mode_to_sequential 
 -------------------------------------------------
@@ -429,35 +410,35 @@
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 INSERT INTO multi_shard_modify_test SELECT * FROM multi_shard_modify_test;
 SELECT distributed_2PCs_are_equal_to_worker_count();
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
- t
+ f
 (1 row)
 
 SET citus.multi_shard_modify_mode TO 'parallel';
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 INSERT INTO multi_shard_modify_test SELECT * FROM multi_shard_modify_test;
 SELECT distributed_2PCs_are_equal_to_placement_count();
  distributed_2pcs_are_equal_to_placement_count 
 -----------------------------------------------
- t
+ f
 (1 row)
 
 -- one more realistic test with sequential inserts and INSERT .. SELECT in the same tx
 INSERT INTO multi_shard_modify_test SELECT i, i::text, i FROM generate_series(0,100) i;
 BEGIN;
     INSERT INTO multi_shard_modify_test VALUES (1,'1',1), (2,'2',2), (3,'3',3), (4,'4',4);
     -- now switch to sequential mode to enable a successful INSERT .. SELECT
     SELECT set_local_multi_shard_modify_mode_to_sequential();
  set_local_multi_shard_modify_mode_to_sequential 
 -------------------------------------------------
@@ -486,198 +467,167 @@
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 -- Check if multi_shard_update works properly after create_distributed_table in sequential mode
 CREATE TABLE test_seq_multi_shard_update(a int, b int);
 BEGIN;
     SET LOCAL citus.multi_shard_modify_mode TO 'sequential';
     SELECT create_distributed_table('test_seq_multi_shard_update', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
     INSERT INTO test_seq_multi_shard_update VALUES (0, 0), (1, 0), (2, 0), (3, 0), (4, 0);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
     DELETE FROM test_seq_multi_shard_update WHERE b < 2;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 SELECT distributed_2PCs_are_equal_to_worker_count();
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
- t
+ f
 (1 row)
 
 DROP TABLE test_seq_multi_shard_update;
 -- Check if truncate works properly after create_distributed_table in sequential mode
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 CREATE TABLE test_seq_truncate_after_create(a int, b int);
 BEGIN;
     SET LOCAL citus.multi_shard_modify_mode TO 'sequential';
     SELECT create_distributed_table('test_seq_truncate_after_create', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
     INSERT INTO test_seq_truncate_after_create VALUES (0, 0), (1, 0), (2, 0), (3, 0), (4, 0);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
     TRUNCATE test_seq_truncate_after_create;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 SELECT distributed_2PCs_are_equal_to_worker_count();
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
- t
+ f
 (1 row)
 
 DROP TABLE test_seq_truncate_after_create;
 -- Check if drop table works properly after create_distributed_table in sequential mode
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 CREATE TABLE test_seq_drop_table(a int, b int);
 BEGIN;
     SET LOCAL citus.multi_shard_modify_mode TO 'sequential';
     SELECT create_distributed_table('test_seq_drop_table', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
     DROP TABLE test_seq_drop_table;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 SELECT distributed_2PCs_are_equal_to_worker_count();
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
- t
+ f
 (1 row)
 
 -- Check if copy errors out properly after create_distributed_table in sequential mode
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 CREATE TABLE test_seq_copy(a int, b int);
 BEGIN;
     SET LOCAL citus.multi_shard_modify_mode TO 'sequential';
     SELECT create_distributed_table('test_seq_copy', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
     \COPY test_seq_copy FROM STDIN DELIMITER AS ',';
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
+1,1
+2,2
+3,3
+\.
+invalid command \.
 ROLLBACK;
+ERROR:  syntax error at or near "1"
 SELECT distributed_2PCs_are_equal_to_worker_count();
- distributed_2pcs_are_equal_to_worker_count
----------------------------------------------------------------------
- f
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 DROP TABLE test_seq_copy;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- Check if DDL + CREATE INDEX works properly after create_distributed_table in sequential mode
 SELECT recover_prepared_transactions();
- recover_prepared_transactions
----------------------------------------------------------------------
-                             0
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 CREATE TABLE test_seq_ddl_index(a int, b int);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 BEGIN;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
     SET LOCAL citus.multi_shard_modify_mode TO 'sequential';
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
     SELECT create_distributed_table('test_seq_ddl_index', 'a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
     INSERT INTO test_seq_ddl_index VALUES (0, 0), (1, 0), (2, 0), (3, 0), (4, 0);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
     ALTER TABLE test_seq_ddl_index ADD COLUMN c int;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
     CREATE INDEX idx ON test_seq_ddl_index(c);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 SELECT distributed_2PCs_are_equal_to_worker_count();
  distributed_2pcs_are_equal_to_worker_count 
 --------------------------------------------
- t
+ f
 (1 row)
 
 DROP TABLE test_seq_ddl_index;
+ERROR:  table "test_seq_ddl_index" does not exist
 -- create_distributed_table should works on relations with data in sequential mode in and out transaction block
 CREATE TABLE test_create_seq_table (a int);
 INSERT INTO test_create_seq_table VALUES (1);
 SET citus.multi_shard_modify_mode TO 'sequential';
 SELECT create_distributed_table('test_create_seq_table' ,'a');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$test_seq_ddl.test_create_seq_table$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT undistribute_table('test_create_seq_table');
-NOTICE:  creating a new table for test_seq_ddl.test_create_seq_table
-NOTICE:  moving the data of test_seq_ddl.test_create_seq_table
-NOTICE:  dropping the old test_seq_ddl.test_create_seq_table
-NOTICE:  renaming the new table to test_seq_ddl.test_create_seq_table
- undistribute_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  cannot undistribute table because the table is not distributed
 RESET citus.multi_shard_modify_mode;
 BEGIN;
     SET LOCAL citus.multi_shard_modify_mode TO 'sequential';
     select create_distributed_table('test_create_seq_table' ,'a');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$test_seq_ddl.test_create_seq_table$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ROLLBACK;
 -- trigger switch-over when using single connection per worker
 BEGIN;
 SET citus.next_shard_id TO 16900;
 SET LOCAL citus.shard_count TO 4;
 SET LOCAL citus.multi_shard_modify_mode TO 'sequential';
 CREATE UNLOGGED TABLE trigger_switchover(a int, b int, c int, d int, e int, f int, g int, h int);
 INSERT INTO trigger_switchover
   SELECT s AS a, s AS b, s AS c, s AS d, s AS e, s AS f, s AS g, s AS h FROM generate_series(1,250000) s;
 SELECT create_distributed_table('trigger_switchover','a');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$test_seq_ddl.trigger_switchover$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ABORT;
 SET search_path TO 'public';
 DROP SCHEMA test_seq_ddl CASCADE;
-NOTICE:  drop cascades to 11 other objects
+NOTICE:  drop cascades to 13 other objects
 DETAIL:  drop cascades to function test_seq_ddl.distributed_2pcs_are_equal_to_worker_count()
 drop cascades to function test_seq_ddl.distributed_2pcs_are_equal_to_placement_count()
 drop cascades to function test_seq_ddl.no_distributed_2pcs()
 drop cascades to function test_seq_ddl.set_local_multi_shard_modify_mode_to_sequential()
 drop cascades to table test_seq_ddl.test_table
 drop cascades to table test_seq_ddl.ref_test
 drop cascades to table test_seq_ddl.test_table_rep_2
 drop cascades to table test_seq_ddl.test_seq_truncate
 drop cascades to table test_seq_ddl.test_seq_truncate_rep_2
 drop cascades to table test_seq_ddl.multi_shard_modify_test
+drop cascades to table test_seq_ddl.test_seq_drop_table
+drop cascades to table test_seq_ddl.test_seq_copy
 drop cascades to table test_seq_ddl.test_create_seq_table
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_outer_join.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_outer_join.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_outer_join.out.modified	2022-11-09 13:38:18.809312434 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_outer_join.out.modified	2022-11-09 13:38:18.829312434 +0300
@@ -6,42 +6,36 @@
 	l_custkey integer not null,
 	l_name varchar(25) not null,
 	l_address varchar(40) not null,
 	l_nationkey integer not null,
 	l_phone char(15) not null,
 	l_acctbal decimal(15,2) not null,
 	l_mktsegment char(10) not null,
 	l_comment varchar(117) not null
 );
 SELECT create_distributed_table('multi_outer_join_left', 'l_custkey', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE multi_outer_join_right
 (
 	r_custkey integer not null,
 	r_name varchar(25) not null,
 	r_address varchar(40) not null,
 	r_nationkey integer not null,
 	r_phone char(15) not null,
 	r_acctbal decimal(15,2) not null,
 	r_mktsegment char(10) not null,
 	r_comment varchar(117) not null
 );
 SELECT create_distributed_table('multi_outer_join_right', 'r_custkey', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE multi_outer_join_right_reference
 (
 	r_custkey integer not null,
 	r_name varchar(25) not null,
 	r_address varchar(40) not null,
 	r_nationkey integer not null,
 	r_phone char(15) not null,
 	r_acctbal decimal(15,2) not null,
 	r_mktsegment char(10) not null,
 	r_comment varchar(117) not null
@@ -57,25 +51,22 @@
 	t_custkey integer not null,
 	t_name varchar(25) not null,
 	t_address varchar(40) not null,
 	t_nationkey integer not null,
 	t_phone char(15) not null,
 	t_acctbal decimal(15,2) not null,
 	t_mktsegment char(10) not null,
 	t_comment varchar(117) not null
 );
 SELECT create_distributed_table('multi_outer_join_third', 't_custkey', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE multi_outer_join_third_reference
 (
 	t_custkey integer not null,
 	t_name varchar(25) not null,
 	t_address varchar(40) not null,
 	t_nationkey integer not null,
 	t_phone char(15) not null,
 	t_acctbal decimal(15,2) not null,
 	t_mktsegment char(10) not null,
 	t_comment varchar(117) not null
@@ -269,21 +260,25 @@
  min | max 
 -----+-----
   11 |  30
 (1 row)
 
 -- Since we cannot broadcast or re-partition, joining on a different key should error out
 SELECT
 	count(*)
 FROM
 	multi_outer_join_left a LEFT JOIN multi_outer_join_right b ON (l_nationkey = r_nationkey);
-ERROR:  complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
+ count 
+-------
+    32
+(1 row)
+
 -- Anti-join should return customers for which there is no row in the right table
 SELECT
 	min(l_custkey), max(l_custkey)
 FROM
 	multi_outer_join_left a LEFT JOIN multi_outer_join_right b ON (l_custkey = r_custkey)
 WHERE
 	r_custkey IS NULL;
  min | max 
 -----+-----
   23 |  29
@@ -407,22 +402,54 @@
 (17 rows)
 
 -- Right join with single shard right most table should error out
 SELECT
 	l_custkey, r_custkey, t_custkey
 FROM
 	multi_outer_join_left l1
 	LEFT JOIN multi_outer_join_right r1 ON (l1.l_custkey = r1.r_custkey)
 	RIGHT JOIN multi_outer_join_third_reference t1 ON (r1.r_custkey  = t1.t_custkey)
 ORDER BY l_custkey, r_custkey, t_custkey;
-ERROR:  cannot pushdown the subquery
-DETAIL:  There exist a reference table in the outer part of the outer join
+ l_custkey | r_custkey | t_custkey 
+-----------+-----------+-----------
+        11 |        11 |        11
+        12 |        12 |        12
+        14 |        14 |        14
+        16 |        16 |        16
+        17 |        17 |        17
+        18 |        18 |        18
+        20 |        20 |        20
+        21 |        21 |        21
+        22 |        22 |        22
+        24 |        24 |        24
+        26 |        26 |        26
+        27 |        27 |        27
+        28 |        28 |        28
+        30 |        30 |        30
+           |           |         1
+           |           |         2
+           |           |         3
+           |           |         4
+           |           |         5
+           |           |         6
+           |           |         7
+           |           |         8
+           |           |         9
+           |           |        10
+           |           |        13
+           |           |        15
+           |           |        19
+           |           |        23
+           |           |        25
+           |           |        29
+(30 rows)
+
 -- Right join with single shard left most table should work
 SELECT
 	t_custkey, r_custkey, l_custkey
 FROM
 	multi_outer_join_third_reference t1
 	RIGHT JOIN multi_outer_join_right r1 ON (t1.t_custkey = r1.r_custkey)
 	LEFT JOIN multi_outer_join_left l1 ON (r1.r_custkey  = l1.l_custkey)
 ORDER BY t_custkey, r_custkey, l_custkey;
  t_custkey | r_custkey | l_custkey 
 -----------+-----------+-----------
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_select_distinct.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_select_distinct.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_select_distinct.out.modified	2022-11-09 13:38:21.339312424 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_select_distinct.out.modified	2022-11-09 13:38:21.369312423 +0300
@@ -8,27 +8,43 @@
 SELECT DISTINCT l_orderkey, now() FROM lineitem_hash_part LIMIT 0;
  l_orderkey | now 
 ------------+-----
 (0 rows)
 
 SELECT DISTINCT l_orderkey, avg(l_linenumber)
 FROM lineitem_hash_part
 GROUP BY l_orderkey
 HAVING avg(l_linenumber) = (select avg(distinct l_linenumber))
 LIMIT 10;
-ERROR:  Subqueries in HAVING cannot refer to outer query
+ l_orderkey |          avg           
+------------+------------------------
+          1 |     3.5000000000000000
+          2 | 1.00000000000000000000
+          3 |     3.5000000000000000
+          4 | 1.00000000000000000000
+          5 |     2.0000000000000000
+          6 | 1.00000000000000000000
+          7 |     4.0000000000000000
+         32 |     3.5000000000000000
+         33 |     2.5000000000000000
+         34 |     2.0000000000000000
+(10 rows)
+
 SELECT DISTINCT l_orderkey
 FROM lineitem_hash_part
 GROUP BY l_orderkey
 HAVING (select avg(distinct l_linenumber) = l_orderkey)
 LIMIT 10;
-ERROR:  Subqueries in HAVING cannot refer to outer query
+ l_orderkey 
+------------
+(0 rows)
+
 SELECT DISTINCT l_partkey, 1 + (random() * 0)::int FROM lineitem_hash_part ORDER BY 1 DESC LIMIT 3;
  l_partkey | ?column? 
 -----------+----------
     199973 |        1
     199946 |        1
     199943 |        1
 (3 rows)
 
 -- const expressions are supported
 SELECT DISTINCT l_orderkey, 1+1 FROM lineitem_hash_part ORDER BY 1 LIMIT 5;
@@ -214,61 +230,52 @@
 -- explain the query to see actual plan
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT l_orderkey, count(*)
 		FROM lineitem_hash_part
 		WHERE l_orderkey < 200
 		GROUP BY 1
 		HAVING count(*) > 5
 		ORDER BY 2 DESC, 1;
                     QUERY PLAN                    
 --------------------------------------------------
- Sort
-   Sort Key: remote_scan.count DESC, remote_scan.l_orderkey
-   ->  HashAggregate
-         Group Key: remote_scan.count, remote_scan.l_orderkey
-         ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-               Tasks Shown: One of 4
-               ->  Task
-                     Node: host=localhost port=xxxxx dbname=regression
+ Unique
+   ->  Sort
+         Sort Key: (count(*)) DESC, l_orderkey
          ->  HashAggregate
                Group Key: l_orderkey
                Filter: (count(*) > 5)
-                           ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
+               ->  Seq Scan on lineitem_hash_part
                      Filter: (l_orderkey < 200)
-(14 rows)
+(8 rows)
 
 -- check the plan if the hash aggreate is disabled
 SET enable_hashagg TO off;
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT l_orderkey, count(*)
 		FROM lineitem_hash_part
 		WHERE l_orderkey < 200
 		GROUP BY 1
 		HAVING count(*) > 5
 		ORDER BY 2 DESC, 1;
                        QUERY PLAN                       
 --------------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: remote_scan.count DESC, remote_scan.l_orderkey
-         ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-               Tasks Shown: One of 4
-               ->  Task
-                     Node: host=localhost port=xxxxx dbname=regression
-                     ->  HashAggregate
+         Sort Key: (count(*)) DESC, l_orderkey
+         ->  GroupAggregate
                Group Key: l_orderkey
                Filter: (count(*) > 5)
-                           ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
+               ->  Sort
+                     Sort Key: l_orderkey
+                     ->  Seq Scan on lineitem_hash_part
                            Filter: (l_orderkey < 200)
-(13 rows)
+(10 rows)
 
 SET enable_hashagg TO on;
 -- distinct on aggregate of group by columns, we try to check whether we handle
 -- queries which does not have any group by column in distinct columns properly.
 SELECT DISTINCT count(*)
 	FROM lineitem_hash_part
 	GROUP BY l_suppkey, l_linenumber
 	ORDER BY 1;
  count 
 -------
@@ -283,59 +290,45 @@
 -- the uniqueness of the result.
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT count(*)
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY 1;
                     QUERY PLAN                    
 --------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: (COALESCE((pg_catalog.sum(remote_scan.count))::bigint, '0'::bigint))
-         ->  HashAggregate
-               Group Key: remote_scan.worker_column_2, remote_scan.worker_column_3
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-                     Tasks Shown: One of 4
-                     ->  Task
-                           Node: host=localhost port=xxxxx dbname=regression
+         Sort Key: (count(*))
          ->  HashAggregate
                Group Key: l_suppkey, l_linenumber
-                                 ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(13 rows)
+               ->  Seq Scan on lineitem_hash_part
+(6 rows)
 
 -- check the plan if the hash aggreate is disabled. We expect to see sort+unique
 -- instead of aggregate plan node to handle distinct.
 SET enable_hashagg TO off;
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT count(*)
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY 1;
                        QUERY PLAN                       
 --------------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: (COALESCE((pg_catalog.sum(remote_scan.count))::bigint, '0'::bigint))
+         Sort Key: (count(*))
          ->  GroupAggregate
-               Group Key: remote_scan.worker_column_2, remote_scan.worker_column_3
-               ->  Sort
-                     Sort Key: remote_scan.worker_column_2, remote_scan.worker_column_3
-                     ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-                           Tasks Shown: One of 4
-                           ->  Task
-                                 Node: host=localhost port=xxxxx dbname=regression
-                                 ->  HashAggregate
                Group Key: l_suppkey, l_linenumber
-                                       ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(15 rows)
+               ->  Sort
+                     Sort Key: l_suppkey, l_linenumber
+                     ->  Seq Scan on lineitem_hash_part
+(8 rows)
 
 SET enable_hashagg TO on;
 -- Now we have only part of group clause columns in distinct, yet it is still not
 -- enough to use Group By columns to guarantee uniqueness of result list.
 SELECT DISTINCT l_suppkey, count(*)
 	FROM lineitem_hash_part
 	GROUP BY l_suppkey, l_linenumber
 	ORDER BY 1
 	LIMIT 10;
  l_suppkey | count 
@@ -355,63 +348,50 @@
 -- explain the query to see actual plan. Similar to the explain of the query above.
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT l_suppkey, count(*)
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY 1
 		LIMIT 10;
                        QUERY PLAN                       
 --------------------------------------------------------
  Limit
-   ->  Unique
    ->  Sort
-               Sort Key: remote_scan.l_suppkey, (COALESCE((pg_catalog.sum(remote_scan.count))::bigint, '0'::bigint))
+         Sort Key: l_suppkey
          ->  HashAggregate
-                     Group Key: remote_scan.l_suppkey, remote_scan.worker_column_3
-                     ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-                           Tasks Shown: One of 4
-                           ->  Task
-                                 Node: host=localhost port=xxxxx dbname=regression
+               Group Key: l_suppkey, count(*)
                ->  HashAggregate
                      Group Key: l_suppkey, l_linenumber
-                                       ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(14 rows)
+                     ->  Seq Scan on lineitem_hash_part
+(8 rows)
 
 -- check the plan if the hash aggreate is disabled. Similar to the explain of
 -- the query above.
 SET enable_hashagg TO off;
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT l_suppkey, count(*)
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY 1
 		LIMIT 10;
                           QUERY PLAN                          
 --------------------------------------------------------------
  Limit
    ->  Unique
          ->  Sort
-               Sort Key: remote_scan.l_suppkey, (COALESCE((pg_catalog.sum(remote_scan.count))::bigint, '0'::bigint))
+               Sort Key: l_suppkey, (count(*))
                ->  GroupAggregate
-                     Group Key: remote_scan.l_suppkey, remote_scan.worker_column_3
-                     ->  Sort
-                           Sort Key: remote_scan.l_suppkey, remote_scan.worker_column_3
-                           ->  Custom Scan (Citus Adaptive)
-                                 Task Count: 4
-                                 Tasks Shown: One of 4
-                                 ->  Task
-                                       Node: host=localhost port=xxxxx dbname=regression
-                                       ->  HashAggregate
                      Group Key: l_suppkey, l_linenumber
-                                             ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(16 rows)
+                     ->  Sort
+                           Sort Key: l_suppkey, l_linenumber
+                           ->  Seq Scan on lineitem_hash_part
+(9 rows)
 
 SET enable_hashagg TO on;
 -- Similar to the above query, not with count but avg. Only difference with the
 -- above query is that, we create run two aggregate functions in workers.
 SELECT DISTINCT l_suppkey, avg(l_partkey)
 	FROM lineitem_hash_part
 	GROUP BY l_suppkey, l_linenumber
 	ORDER BY 1,2
 	LIMIT 10;
  l_suppkey |          avg           
@@ -432,63 +412,50 @@
 -- Only aggregate functions will be changed.
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT l_suppkey, avg(l_partkey)
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY 1,2
 		LIMIT 10;
                        QUERY PLAN                       
 --------------------------------------------------------
  Limit
-   ->  Unique
    ->  Sort
-               Sort Key: remote_scan.l_suppkey, ((pg_catalog.sum(remote_scan.avg) / pg_catalog.sum(remote_scan.avg_1)))
+         Sort Key: l_suppkey, (avg(l_partkey))
          ->  HashAggregate
-                     Group Key: remote_scan.l_suppkey, remote_scan.worker_column_4
-                     ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-                           Tasks Shown: One of 4
-                           ->  Task
-                                 Node: host=localhost port=xxxxx dbname=regression
+               Group Key: l_suppkey, avg(l_partkey)
                ->  HashAggregate
                      Group Key: l_suppkey, l_linenumber
-                                       ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(14 rows)
+                     ->  Seq Scan on lineitem_hash_part
+(8 rows)
 
 -- check the plan if the hash aggreate is disabled. This explain errors out due
 -- to a bug right now, expectation must be corrected after fixing it.
 SET enable_hashagg TO off;
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT l_suppkey, avg(l_partkey)
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY 1,2
 		LIMIT 10;
                           QUERY PLAN                          
 --------------------------------------------------------------
  Limit
    ->  Unique
          ->  Sort
-               Sort Key: remote_scan.l_suppkey, ((pg_catalog.sum(remote_scan.avg) / pg_catalog.sum(remote_scan.avg_1)))
+               Sort Key: l_suppkey, (avg(l_partkey))
                ->  GroupAggregate
-                     Group Key: remote_scan.l_suppkey, remote_scan.worker_column_4
-                     ->  Sort
-                           Sort Key: remote_scan.l_suppkey, remote_scan.worker_column_4
-                           ->  Custom Scan (Citus Adaptive)
-                                 Task Count: 4
-                                 Tasks Shown: One of 4
-                                 ->  Task
-                                       Node: host=localhost port=xxxxx dbname=regression
-                                       ->  HashAggregate
                      Group Key: l_suppkey, l_linenumber
-                                             ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(16 rows)
+                     ->  Sort
+                           Sort Key: l_suppkey, l_linenumber
+                           ->  Seq Scan on lineitem_hash_part
+(9 rows)
 
 SET enable_hashagg TO on;
 -- Similar to the above query but with distinct on
 SELECT DISTINCT ON (l_suppkey) avg(l_partkey)
 	FROM lineitem_hash_part
 	GROUP BY l_suppkey, l_linenumber
 	ORDER BY l_suppkey,1
 	LIMIT 10;
           avg           
 ------------------------
@@ -510,61 +477,47 @@
 	SELECT DISTINCT ON (l_suppkey) avg(l_partkey)
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY l_suppkey,1
 		LIMIT 10;
                        QUERY PLAN                       
 --------------------------------------------------------
  Limit
    ->  Unique
          ->  Sort
-               Sort Key: remote_scan.worker_column_3, ((pg_catalog.sum(remote_scan.avg) / pg_catalog.sum(remote_scan.avg_1)))
-               ->  HashAggregate
-                     Group Key: remote_scan.worker_column_3, remote_scan.worker_column_4
-                     ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-                           Tasks Shown: One of 4
-                           ->  Task
-                                 Node: host=localhost port=xxxxx dbname=regression
+               Sort Key: l_suppkey, (avg(l_partkey))
                ->  HashAggregate
                      Group Key: l_suppkey, l_linenumber
-                                       ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(14 rows)
+                     ->  Seq Scan on lineitem_hash_part
+(7 rows)
 
 -- check the plan if the hash aggreate is disabled. We expect to see sort+unique to
 -- handle distinct on.
 SET enable_hashagg TO off;
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT ON (l_suppkey) avg(l_partkey)
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY l_suppkey,1
 		LIMIT 10;
                           QUERY PLAN                          
 --------------------------------------------------------------
  Limit
    ->  Unique
          ->  Sort
-               Sort Key: remote_scan.worker_column_3, ((pg_catalog.sum(remote_scan.avg) / pg_catalog.sum(remote_scan.avg_1)))
+               Sort Key: l_suppkey, (avg(l_partkey))
                ->  GroupAggregate
-                     Group Key: remote_scan.worker_column_3, remote_scan.worker_column_4
-                     ->  Sort
-                           Sort Key: remote_scan.worker_column_3, remote_scan.worker_column_4
-                           ->  Custom Scan (Citus Adaptive)
-                                 Task Count: 4
-                                 Tasks Shown: One of 4
-                                 ->  Task
-                                       Node: host=localhost port=xxxxx dbname=regression
-                                       ->  HashAggregate
                      Group Key: l_suppkey, l_linenumber
-                                             ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(16 rows)
+                     ->  Sort
+                           Sort Key: l_suppkey, l_linenumber
+                           ->  Seq Scan on lineitem_hash_part
+(9 rows)
 
 SET enable_hashagg TO on;
 -- distinct with expression and aggregation
 SELECT DISTINCT avg(ceil(l_partkey / 2))
 	FROM lineitem_hash_part
 	GROUP BY l_suppkey, l_linenumber
 	ORDER BY 1
 	LIMIT 10;
  avg 
 -----
@@ -583,63 +536,50 @@
 -- explain the query to see actual plan
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT avg(ceil(l_partkey / 2))
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY 1
 		LIMIT 10;
                                QUERY PLAN                                
 -------------------------------------------------------------------------
  Limit
-   ->  Unique
    ->  Sort
-               Sort Key: ((sum(remote_scan.avg) / (pg_catalog.sum(remote_scan.avg_1))::double precision))
+         Sort Key: (avg(ceil(((l_partkey / 2))::double precision)))
          ->  HashAggregate
-                     Group Key: remote_scan.worker_column_3, remote_scan.worker_column_4
-                     ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-                           Tasks Shown: One of 4
-                           ->  Task
-                                 Node: host=localhost port=xxxxx dbname=regression
+               Group Key: avg(ceil(((l_partkey / 2))::double precision))
                ->  HashAggregate
                      Group Key: l_suppkey, l_linenumber
-                                       ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(14 rows)
+                     ->  Seq Scan on lineitem_hash_part
+(8 rows)
 
 -- check the plan if the hash aggreate is disabled. This explain errors out due
 -- to a bug right now, expectation must be corrected after fixing it.
 SET enable_hashagg TO off;
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT avg(ceil(l_partkey / 2))
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY 1
 		LIMIT 10;
                                 QUERY PLAN                                
 --------------------------------------------------------------------------
  Limit
    ->  Unique
          ->  Sort
-               Sort Key: ((sum(remote_scan.avg) / (pg_catalog.sum(remote_scan.avg_1))::double precision))
+               Sort Key: (avg(ceil(((l_partkey / 2))::double precision)))
                ->  GroupAggregate
-                     Group Key: remote_scan.worker_column_3, remote_scan.worker_column_4
-                     ->  Sort
-                           Sort Key: remote_scan.worker_column_3, remote_scan.worker_column_4
-                           ->  Custom Scan (Citus Adaptive)
-                                 Task Count: 4
-                                 Tasks Shown: One of 4
-                                 ->  Task
-                                       Node: host=localhost port=xxxxx dbname=regression
-                                       ->  HashAggregate
                      Group Key: l_suppkey, l_linenumber
-                                             ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(16 rows)
+                     ->  Sort
+                           Sort Key: l_suppkey, l_linenumber
+                           ->  Seq Scan on lineitem_hash_part
+(9 rows)
 
 SET enable_hashagg TO on;
 -- expression among aggregations.
 SELECT DISTINCT sum(l_suppkey) + count(l_partkey) AS dis
 	FROM lineitem_hash_part
 	GROUP BY l_suppkey, l_linenumber
 	ORDER BY 1
 	LIMIT 10;
  dis 
 -----
@@ -658,63 +598,50 @@
 -- explain the query to see actual plan
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT sum(l_suppkey) + count(l_partkey) AS dis
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY 1
 		LIMIT 10;
                           QUERY PLAN                          
 --------------------------------------------------------------
  Limit
-   ->  Unique
    ->  Sort
-               Sort Key: (((pg_catalog.sum(remote_scan.dis))::bigint + COALESCE((pg_catalog.sum(remote_scan.dis_1))::bigint, '0'::bigint)))
+         Sort Key: ((sum(l_suppkey) + count(l_partkey)))
          ->  HashAggregate
-                     Group Key: remote_scan.worker_column_3, remote_scan.worker_column_4
-                     ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-                           Tasks Shown: One of 4
-                           ->  Task
-                                 Node: host=localhost port=xxxxx dbname=regression
+               Group Key: (sum(l_suppkey) + count(l_partkey))
                ->  HashAggregate
                      Group Key: l_suppkey, l_linenumber
-                                       ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(14 rows)
+                     ->  Seq Scan on lineitem_hash_part
+(8 rows)
 
 -- check the plan if the hash aggreate is disabled. This explain errors out due
 -- to a bug right now, expectation must be corrected after fixing it.
 SET enable_hashagg TO off;
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT sum(l_suppkey) + count(l_partkey) AS dis
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey, l_linenumber
 		ORDER BY 1
 		LIMIT 10;
                           QUERY PLAN                           
 ---------------------------------------------------------------
  Limit
    ->  Unique
          ->  Sort
-               Sort Key: (((pg_catalog.sum(remote_scan.dis))::bigint + COALESCE((pg_catalog.sum(remote_scan.dis_1))::bigint, '0'::bigint)))
+               Sort Key: ((sum(l_suppkey) + count(l_partkey)))
                ->  GroupAggregate
-                     Group Key: remote_scan.worker_column_3, remote_scan.worker_column_4
-                     ->  Sort
-                           Sort Key: remote_scan.worker_column_3, remote_scan.worker_column_4
-                           ->  Custom Scan (Citus Adaptive)
-                                 Task Count: 4
-                                 Tasks Shown: One of 4
-                                 ->  Task
-                                       Node: host=localhost port=xxxxx dbname=regression
-                                       ->  HashAggregate
                      Group Key: l_suppkey, l_linenumber
-                                             ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(16 rows)
+                     ->  Sort
+                           Sort Key: l_suppkey, l_linenumber
+                           ->  Seq Scan on lineitem_hash_part
+(9 rows)
 
 SET enable_hashagg TO on;
 -- distinct on all columns, note Group By columns guarantees uniqueness of the
 -- result list.
 SELECT DISTINCT *
 	FROM lineitem_hash_part
 	GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16
 	ORDER BY 1,2
 	LIMIT 10;
  l_orderkey | l_partkey | l_suppkey | l_linenumber | l_quantity | l_extendedprice | l_discount | l_tax | l_returnflag | l_linestatus | l_shipdate | l_commitdate | l_receiptdate |      l_shipinstruct       | l_shipmode |                 l_comment                  
@@ -737,48 +664,49 @@
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT *
 		FROM lineitem_hash_part
 		GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16
 		ORDER BY 1,2
 		LIMIT 10;
 $Q$);
                                                                                                              coordinator_plan                                                                                                             
 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  Limit
+   ->  Unique
+         ->  Group
+               Group Key: l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode, l_comment
                ->  Sort
-         Sort Key: remote_scan.l_orderkey, remote_scan.l_partkey
-         ->  HashAggregate
-               Group Key: remote_scan.l_orderkey, remote_scan.l_partkey, remote_scan.l_suppkey, remote_scan.l_linenumber, remote_scan.l_quantity, remote_scan.l_extendedprice, remote_scan.l_discount, remote_scan.l_tax, remote_scan.l_returnflag, remote_scan.l_linestatus, remote_scan.l_shipdate, remote_scan.l_commitdate, remote_scan.l_receiptdate, remote_scan.l_shipinstruct, remote_scan.l_shipmode, remote_scan.l_comment
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
+                     Sort Key: l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode, l_comment
+                     ->  Seq Scan on lineitem_hash_part
 (7 rows)
 
 -- check the plan if the hash aggreate is disabled. We expect to see only one
 -- aggregation node since group by columns guarantees the uniqueness.
 SET enable_hashagg TO off;
 SELECT coordinator_plan($Q$
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT *
 		FROM lineitem_hash_part
 		GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16
 		ORDER BY 1,2
 		LIMIT 10;
 $Q$);
                                                                                                              coordinator_plan                                                                                                             
 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  Limit
    ->  Unique
+         ->  Group
+               Group Key: l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode, l_comment
                ->  Sort
-               Sort Key: remote_scan.l_orderkey, remote_scan.l_partkey, remote_scan.l_suppkey, remote_scan.l_linenumber, remote_scan.l_quantity, remote_scan.l_extendedprice, remote_scan.l_discount, remote_scan.l_tax, remote_scan.l_returnflag, remote_scan.l_linestatus, remote_scan.l_shipdate, remote_scan.l_commitdate, remote_scan.l_receiptdate, remote_scan.l_shipinstruct, remote_scan.l_shipmode, remote_scan.l_comment
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-(6 rows)
+                     Sort Key: l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdate, l_commitdate, l_receiptdate, l_shipinstruct, l_shipmode, l_comment
+                     ->  Seq Scan on lineitem_hash_part
+(7 rows)
 
 SET enable_hashagg TO on;
 -- distinct on count distinct
 SELECT DISTINCT count(DISTINCT l_partkey), count(DISTINCT l_shipmode)
 	FROM lineitem_hash_part
 	GROUP BY l_orderkey
 	ORDER BY 1,2;
  count | count 
 -------+-------
      1 |     1
@@ -810,60 +738,49 @@
 
 -- explain the query to see actual plan. We expect to see aggregation plan for
 -- the outer distinct.
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT count(DISTINCT l_partkey), count(DISTINCT l_shipmode)
 		FROM lineitem_hash_part
 		GROUP BY l_orderkey
 		ORDER BY 1,2;
                                  QUERY PLAN                                  
 -----------------------------------------------------------------------------
- Sort
-   Sort Key: remote_scan.count, remote_scan.count_1
-   ->  HashAggregate
-         Group Key: remote_scan.count, remote_scan.count_1
-         ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-               Tasks Shown: One of 4
-               ->  Task
-                     Node: host=localhost port=xxxxx dbname=regression
+ Unique
+   ->  Sort
+         Sort Key: (count(DISTINCT l_partkey)), (count(DISTINCT l_shipmode))
          ->  GroupAggregate
                Group Key: l_orderkey
                ->  Sort
                      Sort Key: l_orderkey
-                                 ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(14 rows)
+                     ->  Seq Scan on lineitem_hash_part
+(8 rows)
 
 -- check the plan if the hash aggreate is disabled. We expect to see sort + unique
 -- plans for the outer distinct.
 SET enable_hashagg TO off;
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT count(DISTINCT l_partkey), count(DISTINCT l_shipmode)
 		FROM lineitem_hash_part
 		GROUP BY l_orderkey
 		ORDER BY 1,2;
                                  QUERY PLAN                                  
 -----------------------------------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: remote_scan.count, remote_scan.count_1
-         ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-               Tasks Shown: One of 4
-               ->  Task
-                     Node: host=localhost port=xxxxx dbname=regression
+         Sort Key: (count(DISTINCT l_partkey)), (count(DISTINCT l_shipmode))
          ->  GroupAggregate
                Group Key: l_orderkey
                ->  Sort
                      Sort Key: l_orderkey
-                                 ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(13 rows)
+                     ->  Seq Scan on lineitem_hash_part
+(8 rows)
 
 SET enable_hashagg TO on;
 -- distinct on aggregation with filter and expression
 SELECT DISTINCT ceil(count(case when l_partkey > 100000 THEN 1 ELSE 0 END) / 2) AS count
 	FROM lineitem_hash_part
 	GROUP BY l_suppkey
 	ORDER BY 1;
  count 
 -------
      0
@@ -876,99 +793,89 @@
 -- explain the query to see actual plan
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT ceil(count(case when l_partkey > 100000 THEN 1 ELSE 0 END) / 2) AS count
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey
 		ORDER BY 1;
                                                  QUERY PLAN                                                  
 -------------------------------------------------------------------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: (ceil(((COALESCE((pg_catalog.sum(remote_scan.count))::bigint, '0'::bigint) / 2))::double precision))
-         ->  HashAggregate
-               Group Key: remote_scan.worker_column_2
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-                     Tasks Shown: One of 4
-                     ->  Task
-                           Node: host=localhost port=xxxxx dbname=regression
+         Sort Key: (ceil(((count(CASE WHEN (l_partkey > 100000) THEN 1 ELSE 0 END) / 2))::double precision))
          ->  HashAggregate
                Group Key: l_suppkey
-                                 ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(13 rows)
+               ->  Seq Scan on lineitem_hash_part
+(6 rows)
 
 -- check the plan if the hash aggreate is disabled
 SET enable_hashagg TO off;
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT ceil(count(case when l_partkey > 100000 THEN 1 ELSE 0 END) / 2) AS count
 		FROM lineitem_hash_part
 		GROUP BY l_suppkey
 		ORDER BY 1;
                                                  QUERY PLAN                                                  
 -------------------------------------------------------------------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: (ceil(((COALESCE((pg_catalog.sum(remote_scan.count))::bigint, '0'::bigint) / 2))::double precision))
+         Sort Key: (ceil(((count(CASE WHEN (l_partkey > 100000) THEN 1 ELSE 0 END) / 2))::double precision))
          ->  GroupAggregate
-               Group Key: remote_scan.worker_column_2
-               ->  Sort
-                     Sort Key: remote_scan.worker_column_2
-                     ->  Custom Scan (Citus Adaptive)
-                           Task Count: 4
-                           Tasks Shown: One of 4
-                           ->  Task
-                                 Node: host=localhost port=xxxxx dbname=regression
-                                 ->  HashAggregate
                Group Key: l_suppkey
-                                       ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(15 rows)
+               ->  Sort
+                     Sort Key: l_suppkey
+                     ->  Seq Scan on lineitem_hash_part
+(8 rows)
 
 SET enable_hashagg TO on;
 -- explain the query to see actual plan with array_agg aggregation.
 SELECT coordinator_plan($Q$
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT array_agg(l_linenumber), array_length(array_agg(l_linenumber), 1)
 		FROM lineitem_hash_part
 		GROUP BY l_orderkey
 		ORDER BY 2
 		LIMIT 15;
 $Q$);
                                       coordinator_plan                                      
 --------------------------------------------------------------------------------------------
  Limit
    ->  Sort
-         Sort Key: remote_scan.array_length
+         Sort Key: (array_length(array_agg(l_linenumber), 1))
          ->  HashAggregate
-               Group Key: remote_scan.array_length, remote_scan.array_agg
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-(7 rows)
+               Group Key: array_length(array_agg(l_linenumber), 1), array_agg(l_linenumber)
+               ->  HashAggregate
+                     Group Key: l_orderkey
+                     ->  Seq Scan on lineitem_hash_part
+(8 rows)
 
 -- check the plan if the hash aggreate is disabled.
 SET enable_hashagg TO off;
 SELECT coordinator_plan($Q$
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT array_agg(l_linenumber), array_length(array_agg(l_linenumber), 1)
 		FROM lineitem_hash_part
 		GROUP BY l_orderkey
 		ORDER BY 2
 		LIMIT 15;
 $Q$);
                                        coordinator_plan                                        
 -----------------------------------------------------------------------------------------------
  Limit
    ->  Unique
          ->  Sort
-               Sort Key: remote_scan.array_length, remote_scan.array_agg
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-(6 rows)
+               Sort Key: (array_length(array_agg(l_linenumber), 1)), (array_agg(l_linenumber))
+               ->  GroupAggregate
+                     Group Key: l_orderkey
+                     ->  Sort
+                           Sort Key: l_orderkey
+                           ->  Seq Scan on lineitem_hash_part
+(9 rows)
 
 SET enable_hashagg TO on;
 -- distinct on non-partition column with aggregate
 -- this is the same as non-distinct version due to group by
 SELECT DISTINCT l_partkey, count(*)
 	FROM lineitem_hash_part
 	GROUP BY 1
 	HAVING count(*) > 2
 	ORDER BY 1;
  l_partkey | count 
@@ -990,33 +897,26 @@
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT l_partkey, count(*)
 		FROM lineitem_hash_part
 		GROUP BY 1
 		HAVING count(*) > 2
 		ORDER BY 1;
                     QUERY PLAN                    
 --------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: remote_scan.l_partkey, (COALESCE((pg_catalog.sum(remote_scan.count))::bigint, '0'::bigint))
-         ->  HashAggregate
-               Group Key: remote_scan.l_partkey
-               Filter: (COALESCE((pg_catalog.sum(remote_scan.worker_column_3))::bigint, '0'::bigint) > 2)
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-                     Tasks Shown: One of 4
-                     ->  Task
-                           Node: host=localhost port=xxxxx dbname=regression
+         Sort Key: l_partkey, (count(*))
          ->  HashAggregate
                Group Key: l_partkey
-                                 ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(14 rows)
+               Filter: (count(*) > 2)
+               ->  Seq Scan on lineitem_hash_part
+(7 rows)
 
 -- distinct on non-partition column and avg
 SELECT DISTINCT l_partkey, avg(l_linenumber)
 	FROM lineitem_hash_part
 	WHERE l_partkey < 500
 	GROUP BY 1
 	HAVING avg(l_linenumber) > 2
 	ORDER BY 1;
  l_partkey |        avg         
 -----------+--------------------
@@ -1061,35 +961,26 @@
     197921 |       441
 (15 rows)
 
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT l_partkey, l_suppkey
 		FROM lineitem_hash_part
 		WHERE l_shipmode = 'AIR' AND l_orderkey < 100
 		ORDER BY 1, 2;
                                  QUERY PLAN                                  
 -----------------------------------------------------------------------------
- Sort
-   Sort Key: remote_scan.l_partkey, remote_scan.l_suppkey
-   ->  HashAggregate
-         Group Key: remote_scan.l_partkey, remote_scan.l_suppkey
-         ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-               Tasks Shown: One of 4
-               ->  Task
-                     Node: host=localhost port=xxxxx dbname=regression
-                     ->  Unique
+ Unique
    ->  Sort
          Sort Key: l_partkey, l_suppkey
-                                 ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
+         ->  Seq Scan on lineitem_hash_part
                Filter: ((l_orderkey < 100) AND (l_shipmode = 'AIR'::bpchar))
-(14 rows)
+(5 rows)
 
 -- distinct on partition column
 SELECT DISTINCT ON (l_orderkey) l_orderkey, l_partkey, l_suppkey
 	FROM lineitem_hash_part
 	WHERE l_orderkey < 35
 	ORDER BY 1, 2, 3;
  l_orderkey | l_partkey | l_suppkey 
 ------------+-----------+-----------
           1 |      2132 |      4633
           2 |    106170 |      1191
@@ -1105,32 +996,24 @@
 
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT ON (l_orderkey) l_orderkey, l_partkey, l_suppkey
 		FROM lineitem_hash_part
 		WHERE l_orderkey < 35
 		ORDER BY 1, 2, 3;
                      QUERY PLAN                     
 ----------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: remote_scan.l_orderkey, remote_scan.l_partkey, remote_scan.l_suppkey
-         ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-               Tasks Shown: One of 4
-               ->  Task
-                     Node: host=localhost port=xxxxx dbname=regression
-                     ->  Unique
-                           ->  Sort
          Sort Key: l_orderkey, l_partkey, l_suppkey
-                                 ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
+         ->  Seq Scan on lineitem_hash_part
                Filter: (l_orderkey < 35)
-(13 rows)
+(5 rows)
 
 -- distinct on non-partition column
 -- note order by is required here
 -- otherwise query results will be different since
 -- distinct on clause is on non-partition column
 SELECT DISTINCT ON (l_partkey) l_partkey, l_orderkey
 	FROM lineitem_hash_part
 	ORDER BY 1,2
 	LIMIT 20;
  l_partkey | l_orderkey 
@@ -1160,32 +1043,23 @@
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT ON (l_partkey) l_partkey, l_orderkey
 		FROM lineitem_hash_part
 		ORDER BY 1,2
 		LIMIT 20;
                     QUERY PLAN                    
 --------------------------------------------------
  Limit
    ->  Unique
          ->  Sort
-               Sort Key: remote_scan.l_partkey, remote_scan.l_orderkey
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-                     Tasks Shown: One of 4
-                     ->  Task
-                           Node: host=localhost port=xxxxx dbname=regression
-                           ->  Limit
-                                 ->  Unique
-                                       ->  Sort
                Sort Key: l_partkey, l_orderkey
-                                             ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(14 rows)
+               ->  Seq Scan on lineitem_hash_part
+(5 rows)
 
 -- distinct on with joins
 -- each customer's first order key
 SELECT DISTINCT ON (o_custkey) o_custkey, l_orderkey
 	FROM lineitem_hash_part JOIN orders_hash_part ON (l_orderkey = o_orderkey)
 	WHERE o_custkey < 15
 	ORDER BY 1,2;
  o_custkey | l_orderkey 
 -----------+------------
          1 |       9154
@@ -1204,41 +1078,49 @@
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT ON (o_custkey) o_custkey, l_orderkey
 		FROM lineitem_hash_part JOIN orders_hash_part ON (l_orderkey = o_orderkey)
 		WHERE o_custkey < 15
 		ORDER BY 1,2;
 $Q$);
                                     coordinator_plan                                    
 ----------------------------------------------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: remote_scan.o_custkey, remote_scan.l_orderkey
-         ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-(5 rows)
+         Sort Key: orders_hash_part.o_custkey, lineitem_hash_part.l_orderkey
+         ->  Hash Join
+               Hash Cond: (orders_hash_part.o_orderkey = lineitem_hash_part.l_orderkey)
+               ->  Seq Scan on orders_hash_part
+                     Filter: (o_custkey < 15)
+               ->  Hash
+                     ->  Seq Scan on lineitem_hash_part
+(9 rows)
 
 -- explain without order by
 -- notice master plan has order by on distinct on column
 SELECT coordinator_plan($Q$
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT ON (o_custkey) o_custkey, l_orderkey
 		FROM lineitem_hash_part JOIN orders_hash_part ON (l_orderkey = o_orderkey)
 		WHERE o_custkey < 15;
 $Q$);
                                     coordinator_plan                                    
 ----------------------------------------------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: remote_scan.o_custkey
-         ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-(5 rows)
+         Sort Key: orders_hash_part.o_custkey
+         ->  Hash Join
+               Hash Cond: (orders_hash_part.o_orderkey = lineitem_hash_part.l_orderkey)
+               ->  Seq Scan on orders_hash_part
+                     Filter: (o_custkey < 15)
+               ->  Hash
+                     ->  Seq Scan on lineitem_hash_part
+(9 rows)
 
 -- each customer's each order's first l_partkey
 SELECT DISTINCT ON (o_custkey, l_orderkey) o_custkey, l_orderkey, l_linenumber, l_partkey
 	FROM lineitem_hash_part JOIN orders_hash_part ON (l_orderkey = o_orderkey)
 	WHERE o_custkey < 20
 	ORDER BY 1,2,3;
  o_custkey | l_orderkey | l_linenumber | l_partkey 
 -----------+------------+--------------+-----------
          1 |       9154 |            1 |     86513
          1 |      14656 |            1 |     59539
@@ -1282,24 +1164,28 @@
 SELECT coordinator_plan($Q$
 EXPLAIN (COSTS FALSE)
 	SELECT DISTINCT ON (o_custkey, l_orderkey) o_custkey, l_orderkey, l_linenumber, l_partkey
 		FROM lineitem_hash_part JOIN orders_hash_part ON (l_orderkey = o_orderkey)
 		WHERE o_custkey < 20;
 $Q$);
                                     coordinator_plan                                    
 ----------------------------------------------------------------------------------------
  Unique
    ->  Sort
-         Sort Key: remote_scan.o_custkey, remote_scan.l_orderkey
-         ->  Custom Scan (Citus Adaptive)
-               Task Count: 4
-(5 rows)
+         Sort Key: orders_hash_part.o_custkey, lineitem_hash_part.l_orderkey
+         ->  Hash Join
+               Hash Cond: (orders_hash_part.o_orderkey = lineitem_hash_part.l_orderkey)
+               ->  Seq Scan on orders_hash_part
+                     Filter: (o_custkey < 20)
+               ->  Hash
+                     ->  Seq Scan on lineitem_hash_part
+(9 rows)
 
 -- each customer's each order's last l_partkey
 SELECT DISTINCT ON (o_custkey, l_orderkey) o_custkey, l_orderkey, l_linenumber, l_partkey
 	FROM lineitem_hash_part JOIN orders_hash_part ON (l_orderkey = o_orderkey)
 	WHERE o_custkey < 15
 	ORDER BY 1,2,3 DESC;
  o_custkey | l_orderkey | l_linenumber | l_partkey 
 -----------+------------+--------------+-----------
          1 |       9154 |            7 |    173448
          1 |      14656 |            1 |     59539
@@ -1358,35 +1244,25 @@
 		FROM (
 			SELECT l_orderkey, l_partkey
 			FROM lineitem_hash_part
 			) q
 		ORDER BY 1,2
 		LIMIT 10;
                                       QUERY PLAN                                      
 --------------------------------------------------------------------------------------
  Limit
    ->  Sort
-         Sort Key: remote_scan.l_orderkey, remote_scan.l_partkey
-         ->  HashAggregate
-               Group Key: remote_scan.l_orderkey, remote_scan.l_partkey
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-                     Tasks Shown: One of 4
-                     ->  Task
-                           Node: host=localhost port=xxxxx dbname=regression
-                           ->  Limit
-                                 ->  Sort
-                                       Sort Key: l_orderkey, l_partkey
+         Sort Key: lineitem_hash_part.l_orderkey, lineitem_hash_part.l_partkey
          ->  HashAggregate
-                                             Group Key: l_orderkey, l_partkey
-                                             ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(16 rows)
+               Group Key: lineitem_hash_part.l_orderkey, lineitem_hash_part.l_partkey
+               ->  Seq Scan on lineitem_hash_part
+(6 rows)
 
 SELECT DISTINCT l_orderkey, cnt
 	FROM (
 		SELECT l_orderkey, count(*) as cnt
 		FROM lineitem_hash_part
 		GROUP BY 1
 		) q
 	ORDER BY 1,2
 	LIMIT 10;
  l_orderkey | cnt 
@@ -1409,37 +1285,27 @@
 			SELECT l_orderkey, count(*) as cnt
 			FROM lineitem_hash_part
 			GROUP BY 1
 			) q
 		ORDER BY 1,2
 		LIMIT 10;
                             QUERY PLAN                            
 ------------------------------------------------------------------
  Limit
    ->  Sort
-         Sort Key: remote_scan.l_orderkey, remote_scan.cnt
-         ->  HashAggregate
-               Group Key: remote_scan.l_orderkey, remote_scan.cnt
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-                     Tasks Shown: One of 4
-                     ->  Task
-                           Node: host=localhost port=xxxxx dbname=regression
-                           ->  Limit
-                                 ->  Sort
          Sort Key: lineitem_hash_part.l_orderkey, (count(*))
          ->  HashAggregate
                Group Key: lineitem_hash_part.l_orderkey, count(*)
                ->  HashAggregate
                      Group Key: lineitem_hash_part.l_orderkey
-                                                   ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(18 rows)
+                     ->  Seq Scan on lineitem_hash_part
+(8 rows)
 
 -- distinct on partition column
 -- random() is added to inner query to prevent flattening
 SELECT DISTINCT ON (l_orderkey) l_orderkey, l_partkey
 	FROM (
 		SELECT l_orderkey, l_partkey, (random()*10)::int + 2 as r
 		FROM lineitem_hash_part
 		) q
 	WHERE r > 1
 	ORDER BY 1,2
@@ -1465,34 +1331,25 @@
 			FROM lineitem_hash_part
 			) q
 		WHERE r > 1
 		ORDER BY 1,2
 		LIMIT 10;
                        QUERY PLAN                       
 --------------------------------------------------------
  Limit
    ->  Unique
          ->  Sort
-               Sort Key: remote_scan.l_orderkey, remote_scan.l_partkey
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-                     Tasks Shown: One of 4
-                     ->  Task
-                           Node: host=localhost port=xxxxx dbname=regression
-                           ->  Limit
-                                 ->  Unique
-                                       ->  Sort
                Sort Key: q.l_orderkey, q.l_partkey
                ->  Subquery Scan on q
                      Filter: (q.r > 1)
-                                                   ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(16 rows)
+                     ->  Seq Scan on lineitem_hash_part
+(7 rows)
 
 -- distinct on non-partition column
 SELECT DISTINCT ON (l_partkey) l_orderkey, l_partkey
 	FROM (
 		SELECT l_orderkey, l_partkey, (random()*10)::int + 2 as r
 		FROM lineitem_hash_part
 		) q
 	WHERE r > 1
 	ORDER BY 2,1
 	LIMIT 10;
@@ -1517,25 +1374,16 @@
 			FROM lineitem_hash_part
 			) q
 		WHERE r > 1
 		ORDER BY 2,1
 		LIMIT 10;
                        QUERY PLAN                       
 --------------------------------------------------------
  Limit
    ->  Unique
          ->  Sort
-               Sort Key: remote_scan.l_partkey, remote_scan.l_orderkey
-               ->  Custom Scan (Citus Adaptive)
-                     Task Count: 4
-                     Tasks Shown: One of 4
-                     ->  Task
-                           Node: host=localhost port=xxxxx dbname=regression
-                           ->  Limit
-                                 ->  Unique
-                                       ->  Sort
                Sort Key: q.l_partkey, q.l_orderkey
                ->  Subquery Scan on q
                      Filter: (q.r > 1)
-                                                   ->  Seq Scan on lineitem_hash_part_360041 lineitem_hash_part
-(16 rows)
+                     ->  Seq Scan on lineitem_hash_part
+(7 rows)
 
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_modifications.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_modifications.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_modifications.out.modified	2022-11-09 13:38:21.939312421 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_modifications.out.modified	2022-11-09 13:38:21.959312421 +0300
@@ -19,25 +19,22 @@
 );
 CREATE TABLE multiple_hash (
 	category text NOT NULL,
 	data text NOT NULL
 );
 CREATE TABLE insufficient_shards ( LIKE limit_orders );
 CREATE TABLE range_partitioned ( LIKE limit_orders );
 CREATE TABLE append_partitioned ( LIKE limit_orders );
 SET citus.shard_count TO 2;
 SELECT create_distributed_table('limit_orders', 'id', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT create_distributed_table('multiple_hash', 'id', 'hash');
 ERROR:  column "id" of relation "multiple_hash" does not exist
 SELECT create_distributed_table('range_partitioned', 'id', 'range');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT create_distributed_table('append_partitioned', 'id', 'append');
  create_distributed_table 
@@ -141,50 +138,51 @@
 INSERT INTO limit_orders VALUES (430, upper('ibm'), 214, timestamp '2003-01-28 10:31:17' +
 								 interval '5 hours', 'buy', sqrt(2));
 SELECT COUNT(*) FROM limit_orders WHERE id = 430;
  count 
 -------
      1
 (1 row)
 
 -- INSERT without partition key
 INSERT INTO limit_orders DEFAULT VALUES;
-ERROR:  cannot perform an INSERT without a partition column value
+ERROR:  null value in column "id" of relation "limit_orders" violates not-null constraint
+DETAIL:  Failing row contains (null, null, null, null, null, 0.00).
 -- squelch WARNINGs that contain worker_port
 SET client_min_messages TO ERROR;
 -- INSERT violating NOT NULL constraint
 INSERT INTO limit_orders VALUES (NULL, 'T', 975234, DEFAULT);
-ERROR:  cannot perform an INSERT with NULL in the partition column
+ERROR:  null value in column "id" of relation "limit_orders" violates not-null constraint
+DETAIL:  Failing row contains (null, T, 975234, null, null, 0.00).
 -- INSERT violating column constraint
 \set VERBOSITY terse
 INSERT INTO limit_orders VALUES (18811, 'BUD', 14962, '2014-04-05 08:32:16', 'sell',
 								 -5.00);
-ERROR:  new row for relation "limit_orders_750000" violates check constraint "limit_orders_limit_price_check"
+ERROR:  new row for relation "limit_orders" violates check constraint "limit_orders_limit_price_check"
 -- INSERT violating primary key constraint
 INSERT INTO limit_orders VALUES (32743, 'LUV', 5994, '2001-04-16 03:37:28', 'buy', 0.58);
-ERROR:  duplicate key value violates unique constraint "limit_orders_pkey_750001"
+ERROR:  duplicate key value violates unique constraint "limit_orders_pkey"
 -- INSERT violating primary key constraint, with RETURNING specified.
 INSERT INTO limit_orders VALUES (32743, 'LUV', 5994, '2001-04-16 03:37:28', 'buy', 0.58) RETURNING *;
-ERROR:  duplicate key value violates unique constraint "limit_orders_pkey_750001"
+ERROR:  duplicate key value violates unique constraint "limit_orders_pkey"
 -- INSERT, with RETURNING specified, failing with a non-constraint error
 INSERT INTO limit_orders VALUES (34153, 'LEE', 5994, '2001-04-16 03:37:28', 'buy', 0.58) RETURNING id / 0;
 ERROR:  division by zero
 \set VERBOSITY DEFAULT
 SET client_min_messages TO DEFAULT;
 -- commands with non-constant partition values are supported
 INSERT INTO limit_orders VALUES (random() * 100, 'ORCL', 152, '2011-08-25 11:50:45',
 								 'sell', 0.58);
 -- values for other columns are totally fine
 INSERT INTO limit_orders VALUES (2036, 'GOOG', 5634, now(), 'buy', random());
 -- commands with mutable functions in their quals
 DELETE FROM limit_orders WHERE id = 246 AND bidder_id = (random() * 1000);
-ERROR:  functions used in the WHERE clause of modification queries on distributed tables must not be VOLATILE
 -- commands with mutable but non-volatile functions(ie: stable func.) in their quals
 -- (the cast to timestamp is because the timestamp_eq_timestamptz operator is stable)
 DELETE FROM limit_orders WHERE id = 246 AND placed_at = current_timestamp::timestamp;
 -- multi-row inserts are supported
 INSERT INTO limit_orders VALUES (12037, 'GOOG', 5634, '2001-04-16 03:37:28', 'buy', 0.50),
                                 (12038, 'GOOG', 5634, '2001-04-17 03:37:28', 'buy', 2.50),
                                 (12039, 'GOOG', 5634, '2001-04-18 03:37:28', 'buy', 1.50);
 SELECT COUNT(*) FROM limit_orders WHERE id BETWEEN 12037 AND 12039;
  count 
 -------
@@ -327,186 +325,175 @@
 UPDATE limit_orders SET (kind, limit_price) = ('buy', 999) WHERE id = 246 RETURNING *;
  id  | symbol | bidder_id |        placed_at         | kind | limit_price 
 -----+--------+-----------+--------------------------+------+-------------
  246 | GM     |        30 | Mon Jul 02 16:32:15 2007 | buy  |         999
 (1 row)
 
 -- Test that on unique contraint violations, we fail fast
 \set VERBOSITY terse
 INSERT INTO limit_orders VALUES (275, 'ADR', 140, '2007-07-02 16:32:15', 'sell', 43.67);
 INSERT INTO limit_orders VALUES (275, 'ADR', 140, '2007-07-02 16:32:15', 'sell', 43.67);
-ERROR:  duplicate key value violates unique constraint "limit_orders_pkey_750001"
+ERROR:  duplicate key value violates unique constraint "limit_orders_pkey"
 -- Test that shards which miss a modification are marked unhealthy
 -- First: Connect to the second worker node
 \c - - - :worker_2_port
 -- Second: Move aside limit_orders shard on the second worker node
 ALTER TABLE limit_orders_750000 RENAME TO renamed_orders;
+ERROR:  relation "limit_orders_750000" does not exist
 -- Third: Connect back to master node
 \c - - - :master_port
 -- Fourth: Perform an INSERT on the remaining node
 -- the whole transaction should fail
 \set VERBOSITY terse
 INSERT INTO limit_orders VALUES (276, 'ADR', 140, '2007-07-02 16:32:15', 'sell', 43.67);
-ERROR:  relation "public.limit_orders_750000" does not exist
 -- set the shard name back
 \c - - - :worker_2_port
 -- Second: Move aside limit_orders shard on the second worker node
 ALTER TABLE renamed_orders RENAME TO limit_orders_750000;
+ERROR:  relation "renamed_orders" does not exist
 -- Verify the insert failed and both placements are healthy
 -- or the insert succeeded and placement marked unhealthy
 \c - - - :worker_1_port
 SELECT count(*) FROM limit_orders_750000 WHERE id = 276;
- count
----------------------------------------------------------------------
-     0
-(1 row)
-
+ERROR:  relation "limit_orders_750000" does not exist at character 22
 \c - - - :worker_2_port
 SELECT count(*) FROM limit_orders_750000 WHERE id = 276;
- count
----------------------------------------------------------------------
-     0
-(1 row)
-
+ERROR:  relation "limit_orders_750000" does not exist at character 22
 \c - - - :master_port
 SELECT count(*) FROM limit_orders WHERE id = 276;
  count 
 -------
-     0
+     1
 (1 row)
 
 SELECT count(*)
 FROM   pg_dist_shard_placement AS sp,
 	   pg_dist_shard           AS s
 WHERE  sp.shardid = s.shardid
 AND    sp.shardstate = 3
 AND    s.logicalrelid = 'limit_orders'::regclass;
  count 
 -------
      0
 (1 row)
 
 -- Test that if all shards miss a modification, no state change occurs
 -- First: Connect to the first worker node
 \c - - - :worker_1_port
 -- Second: Move aside limit_orders shard on the second worker node
 ALTER TABLE limit_orders_750000 RENAME TO renamed_orders;
+ERROR:  relation "limit_orders_750000" does not exist
 -- Third: Connect back to master node
 \c - - - :master_port
 -- Fourth: Perform an INSERT on the remaining node
 \set VERBOSITY terse
 INSERT INTO limit_orders VALUES (276, 'ADR', 140, '2007-07-02 16:32:15', 'sell', 43.67);
-ERROR:  relation "public.limit_orders_750000" does not exist
+ERROR:  duplicate key value violates unique constraint "limit_orders_pkey"
 \set VERBOSITY DEFAULT
 -- Last: Verify worker is still healthy
 SELECT count(*)
 FROM   pg_dist_shard_placement AS sp,
 	   pg_dist_shard           AS s
 WHERE  sp.shardid = s.shardid
 AND    sp.nodename = 'localhost'
 AND    sp.nodeport = :worker_1_port
 AND    sp.shardstate = 1
 AND    s.logicalrelid = 'limit_orders'::regclass;
  count 
 -------
-     2
+     0
 (1 row)
 
 -- Undo our change...
 -- First: Connect to the first worker node
 \c - - - :worker_1_port
 -- Second: Move aside limit_orders shard on the second worker node
 ALTER TABLE renamed_orders RENAME TO limit_orders_750000;
+ERROR:  relation "renamed_orders" does not exist
 -- Third: Connect back to master node
 \c - - - :master_port
 -- attempting to change the partition key is unsupported
 UPDATE limit_orders SET id = 0 WHERE id = 246;
-ERROR:  modifying the partition value of rows is not allowed
 UPDATE limit_orders SET id = 0 WHERE id = 0 OR id = 246;
-ERROR:  modifying the partition value of rows is not allowed
 -- setting the partition column value to itself is allowed
 UPDATE limit_orders SET id = 246 WHERE id = 246;
 UPDATE limit_orders SET id = 246 WHERE id = 246 AND symbol = 'GM';
 UPDATE limit_orders SET id = limit_orders.id WHERE id = 246;
 -- UPDATEs with a FROM clause are supported even with local tables
 UPDATE limit_orders SET limit_price = 0.00 FROM bidders
 					WHERE limit_orders.id = 246 AND
 						  limit_orders.bidder_id = bidders.id AND
 						  bidders.name = 'Bernie Madoff';
 -- should succeed with a CTE
 WITH deleted_orders AS (INSERT INTO limit_orders VALUES (399, 'PDR', 14, '2017-07-02 16:32:15', 'sell', 43))
 UPDATE limit_orders SET symbol = 'GM';
 SELECT symbol, bidder_id FROM limit_orders WHERE id = 246;
  symbol | bidder_id 
 --------+-----------
- GM     |        30
-(1 row)
+(0 rows)
 
 -- updates referencing just a var are supported
 UPDATE limit_orders SET bidder_id = id WHERE id = 246;
 -- updates referencing a column are supported
 UPDATE limit_orders SET bidder_id = bidder_id + 1 WHERE id = 246;
 -- IMMUTABLE functions are allowed
 UPDATE limit_orders SET symbol = LOWER(symbol) WHERE id = 246;
 SELECT symbol, bidder_id FROM limit_orders WHERE id = 246;
  symbol | bidder_id 
 --------+-----------
- gm     |       247
-(1 row)
+(0 rows)
 
 -- IMMUTABLE functions are allowed -- even in returning
 UPDATE limit_orders SET symbol = UPPER(symbol) WHERE id = 246 RETURNING id, LOWER(symbol), symbol;
  id | lower | symbol 
 ----+-------+--------
- 246 | gm    | GM
-(1 row)
+(0 rows)
 
 ALTER TABLE limit_orders ADD COLUMN array_of_values integer[];
 -- updates referencing STABLE functions are allowed
 UPDATE limit_orders SET placed_at = LEAST(placed_at, now()::timestamp) WHERE id = 246;
 -- so are binary operators
 UPDATE limit_orders SET array_of_values = 1 || array_of_values WHERE id = 246;
 CREATE FUNCTION immutable_append(old_values int[], new_value int)
 RETURNS int[] AS $$ SELECT old_values || new_value $$ LANGUAGE SQL IMMUTABLE;
 -- immutable function calls with vars are also allowed
 UPDATE limit_orders
 SET array_of_values = immutable_append(array_of_values, 2) WHERE id = 246;
 CREATE FUNCTION stable_append(old_values int[], new_value int)
 RETURNS int[] AS $$ BEGIN RETURN old_values || new_value; END; $$
 LANGUAGE plpgsql STABLE;
 -- but STABLE function calls with vars are not allowed
 UPDATE limit_orders
 SET array_of_values = stable_append(array_of_values, 3) WHERE id = 246;
-ERROR:  STABLE functions used in UPDATE queries cannot be called with column references
 SELECT array_of_values FROM limit_orders WHERE id = 246;
  array_of_values 
 -----------------
- {1,2}
-(1 row)
+(0 rows)
 
 -- STRICT functions work as expected
 CREATE FUNCTION temp_strict_func(integer,integer) RETURNS integer AS
 'SELECT COALESCE($1, 2) + COALESCE($1, 3);' LANGUAGE SQL STABLE STRICT;
 \set VERBOSITY terse
 UPDATE limit_orders SET bidder_id = temp_strict_func(1, null) WHERE id = 246;
-ERROR:  null value in column "bidder_id" violates not-null constraint
 \set VERBOSITY default
 SELECT array_of_values FROM limit_orders WHERE id = 246;
  array_of_values 
 -----------------
- {1,2}
-(1 row)
+(0 rows)
 
 ALTER TABLE limit_orders DROP array_of_values;
 -- even in RETURNING
 UPDATE limit_orders SET placed_at = placed_at WHERE id = 246 RETURNING NOW();
-ERROR:  non-IMMUTABLE functions are not allowed in the RETURNING clause
+ now 
+-----
+(0 rows)
+
 -- check that multi-row UPDATE/DELETEs with RETURNING work
 INSERT INTO multiple_hash VALUES ('0', '1');
 INSERT INTO multiple_hash VALUES ('0', '2');
 INSERT INTO multiple_hash VALUES ('0', '3');
 INSERT INTO multiple_hash VALUES ('0', '4');
 INSERT INTO multiple_hash VALUES ('0', '5');
 INSERT INTO multiple_hash VALUES ('0', '6');
 UPDATE multiple_hash SET data = data ||'-1' WHERE category = '0' RETURNING *;
  category | data 
 ----------+------
@@ -602,25 +589,22 @@
 SELECT * FROM multiple_hash WHERE category = '2' ORDER BY category, data;
  category | data 
 ----------+------
 (0 rows)
 
 -- verify interaction of default values, SERIAL, and RETURNING
 \set QUIET on
 CREATE TABLE app_analytics_events (id serial, app_id integer, name text);
 SET citus.shard_count TO 4;
 SELECT create_distributed_table('app_analytics_events', 'app_id', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO app_analytics_events VALUES (DEFAULT, 101, 'Fauxkemon Geaux') RETURNING id;
  id 
 ----
   1
 (1 row)
 
 INSERT INTO app_analytics_events (app_id, name) VALUES (102, 'Wayz') RETURNING id;
  id 
 ----
   2
@@ -629,25 +613,22 @@
 INSERT INTO app_analytics_events (app_id, name) VALUES (103, 'Mynt') RETURNING *;
  id | app_id | name 
 ----+--------+------
   3 |    103 | Mynt
 (1 row)
 
 DROP TABLE app_analytics_events;
 -- again with serial in the partition column
 CREATE TABLE app_analytics_events (id serial, app_id integer, name text);
 SELECT create_distributed_table('app_analytics_events', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO app_analytics_events VALUES (DEFAULT, 101, 'Fauxkemon Geaux') RETURNING id;
  id 
 ----
   1
 (1 row)
 
 INSERT INTO app_analytics_events (app_id, name) VALUES (102, 'Wayz') RETURNING id;
  id 
 ----
   2
@@ -760,32 +741,29 @@
  id | name 
 ----+------
  13 | Wayz
  14 | Mynt
 (2 rows)
 
 DROP TABLE app_analytics_events;
 -- Test multi-row insert with a dropped column before the partition column
 CREATE TABLE app_analytics_events (id int default 3, app_id integer, name text);
 SELECT create_distributed_table('app_analytics_events', 'name', colocate_with => 'none');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ALTER TABLE app_analytics_events DROP COLUMN app_id;
 INSERT INTO app_analytics_events (name)
 VALUES ('Wayz'), ('Mynt') RETURNING *;
  id | name 
 ----+------
-  3 | Mynt
   3 | Wayz
+  3 | Mynt
 (2 rows)
 
 SELECT * FROM app_analytics_events WHERE name = 'Wayz';
  id | name 
 ----+------
   3 | Wayz
 (1 row)
 
 DROP TABLE app_analytics_events;
 -- Test multi-row insert with serial in a reference table
@@ -808,61 +786,52 @@
  id | app_id | name 
 ----+--------+------
   1 |    104 | Wayz
   2 |    105 | Mynt
 (2 rows)
 
 DROP TABLE app_analytics_events;
 -- Test multi-row insert with serial in a non-partition column
 CREATE TABLE app_analytics_events (id int, app_id serial, name text);
 SELECT create_distributed_table('app_analytics_events', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO app_analytics_events (id, name)
 VALUES (99, 'Wayz'), (98, 'Mynt') RETURNING name, app_id;
  name | app_id 
 ------+--------
- Mynt |      2
  Wayz |      1
+ Mynt |      2
 (2 rows)
 
 SELECT * FROM app_analytics_events ORDER BY id;
  id | app_id | name 
 ----+--------+------
  98 |      2 | Mynt
  99 |      1 | Wayz
 (2 rows)
 
 DROP TABLE app_analytics_events;
 -- test UPDATE with subqueries
 CREATE TABLE raw_table (id bigint, value bigint);
 CREATE TABLE summary_table (
 	id bigint,
 	min_value numeric,
 	average_value numeric,
 	count int,
 	uniques int);
 SELECT create_distributed_table('raw_table', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT create_distributed_table('summary_table', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO raw_table VALUES (1, 100);
 INSERT INTO raw_table VALUES (1, 200);
 INSERT INTO raw_table VALUES (1, 200);
 INSERT INTO raw_table VALUES (1, 300);
 INSERT INTO raw_table VALUES (2, 400);
 INSERT INTO raw_table VALUES (2, 500);
 INSERT INTO summary_table VALUES (1);
 INSERT INTO summary_table VALUES (2);
 -- test noop deletes and updates
 DELETE FROM summary_table WHERE false;
@@ -1058,58 +1027,55 @@
 (2 rows)
 
 UPDATE reference_summary_table SET average_value = average_query.average FROM (
 	SELECT avg(value) AS average FROM reference_raw_table WHERE id = 1
 	) average_query
 WHERE id = 1;
 UPDATE reference_summary_table SET average_value = average_query.average_value FROM (
 	SELECT average_value FROM summary_table WHERE id = 1 FOR UPDATE
 	) average_query
 WHERE id = 1;
-ERROR:  cannot perform select on a distributed table and modify a reference table
 UPDATE reference_summary_table SET (min_value, average_value) =
 	(SELECT min(value), avg(value) FROM reference_raw_table WHERE id = 2)
 WHERE id = 2;
 SELECT * FROM reference_summary_table ORDER BY id;
  id | min_value |    average_value     | count | uniques 
 ----+-----------+----------------------+-------+---------
-  1 |           | 200.0000000000000000 |       |
+  1 |           |                      |       |        
   2 |       400 | 450.0000000000000000 |       |        
 (2 rows)
 
 -- no need partition colum equalities on reference tables
 UPDATE reference_summary_table SET (count) =
 	(SELECT id AS inner_id FROM reference_raw_table WHERE value = 500)
 WHERE min_value = 400;
 SELECT * FROM reference_summary_table ORDER BY id;
  id | min_value |    average_value     | count | uniques 
 ----+-----------+----------------------+-------+---------
-  1 |           | 200.0000000000000000 |       |
+  1 |           |                      |       |        
   2 |       400 | 450.0000000000000000 |     2 |        
 (2 rows)
 
 -- can read from a reference table and update a distributed table
 UPDATE summary_table SET average_value = average_query.average FROM (
 	SELECT avg(value) AS average FROM reference_raw_table WHERE id = 1
 	) average_query
 WHERE id = 1;
 -- cannot read from a distributed table and update a reference table
 UPDATE reference_summary_table SET average_value = average_query.average FROM (
 	SELECT avg(value) AS average FROM raw_table WHERE id = 1
 	) average_query
 WHERE id = 1;
-ERROR:  cannot perform select on a distributed table and modify a reference table
 UPDATE reference_summary_table SET average_value = average_query.average FROM (
 	SELECT avg(value) AS average FROM raw_table WHERE id = 1 AND  id = 2
 	) average_query
 WHERE id = 1;
-ERROR:  cannot perform select on a distributed table and modify a reference table
 -- test connection API via using COPY
 -- COPY on SELECT part
 BEGIN;
 \COPY raw_table FROM STDIN WITH CSV
 INSERT INTO summary_table VALUES (3);
 UPDATE summary_table SET average_value = average_query.average FROM (
 	SELECT avg(value) AS average FROM raw_table WHERE id = 3
 	) average_query
 WHERE id = 3;
 COMMIT;
@@ -1217,21 +1183,21 @@
 ----+-----------+----------------------+-------+---------
   3 |           | 150.0000000000000000 |       |        
   4 |           | 150.0000000000000000 |       |        
   5 |           | 150.0000000000000000 |       |        
   6 |           | 150.0000000000000000 |       |        
 (4 rows)
 
 -- cannot read from a distributed table and delete from a reference table
 DELETE FROM reference_summary_table USING raw_table
   WHERE reference_summary_table.id = raw_table.id AND raw_table.id = 3;
-ERROR:  cannot perform select on a distributed table and modify a reference table
+ERROR:  relation raw_table is not distributed
 SELECT * FROM summary_table ORDER BY id;
  id | min_value |    average_value     | count | uniques 
 ----+-----------+----------------------+-------+---------
   3 |           | 150.0000000000000000 |       |        
   4 |           | 150.0000000000000000 |       |        
   5 |           | 150.0000000000000000 |       |        
   6 |           | 150.0000000000000000 |       |        
 (4 rows)
 
 -- test connection API via using COPY with DELETEs
@@ -1263,44 +1229,39 @@
 EXECUTE prepared_delete_with_join(4);
 EXECUTE prepared_delete_with_join(5);
 EXECUTE prepared_delete_with_join(6);
 SELECT * FROM summary_table ORDER BY id;
  id | min_value | average_value | count | uniques 
 ----+-----------+---------------+-------+---------
 (0 rows)
 
 -- we don't support subqueries in VALUES clause
 INSERT INTO summary_table (id) VALUES ((SELECT id FROM summary_table));
-ERROR:  subqueries are not supported within INSERT queries
-HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
 INSERT INTO summary_table (id) VALUES (5), ((SELECT id FROM summary_table));
-ERROR:  subqueries are not supported within INSERT queries
-HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
 -- similar queries with reference tables
 INSERT INTO reference_summary_table (id) VALUES ((SELECT id FROM summary_table));
 ERROR:  subqueries are not supported within INSERT queries
 HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
 INSERT INTO summary_table (id) VALUES ((SELECT id FROM reference_summary_table));
 ERROR:  subqueries are not supported within INSERT queries
 HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
 -- subqueries that would be eliminated by = null clauses
 DELETE FROM summary_table WHERE (
     SELECT 1 FROM pg_catalog.pg_statio_sys_sequences
 ) = null;
 DELETE FROM summary_table WHERE (
     SELECT (select min(action_statement) from information_schema.triggers)
     FROM pg_catalog.pg_statio_sys_sequences
 ) = null;
 DELETE FROM summary_table WHERE id < (
     SELECT 0 FROM pg_dist_node
 );
+ERROR:  more than one row returned by a subquery used as an expression
 CREATE TABLE multi_modifications.local (a int default 1, b int);
 INSERT INTO multi_modifications.local VALUES (default, (SELECT min(id) FROM summary_table));
-ERROR:  subqueries are not supported within INSERT queries
-HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
 DROP TABLE insufficient_shards;
 DROP TABLE raw_table;
 DROP TABLE summary_table;
 DROP TABLE reference_raw_table;
 DROP TABLE reference_summary_table;
 DROP SCHEMA multi_modifications CASCADE;
 NOTICE:  drop cascades to table multi_modifications.local
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_distribution_metadata.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_distribution_metadata.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_distribution_metadata.out.modified	2022-11-09 13:38:22.249312420 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_distribution_metadata.out.modified	2022-11-09 13:38:22.259312420 +0300
@@ -46,116 +46,77 @@
 	LANGUAGE C STRICT;
 -- ===================================================================
 -- test distribution metadata functionality
 -- ===================================================================
 -- create hash distributed table
 CREATE TABLE events_hash (
 	id bigint,
 	name text
 );
 SELECT create_distributed_table('events_hash', 'name', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- set shardstate of one replication from each shard to 0 (invalid value)
 UPDATE pg_dist_placement SET shardstate = 0 WHERE shardid BETWEEN 540000 AND 540003
   AND groupid = (SELECT groupid FROM pg_dist_node WHERE nodeport = :worker_2_port);
 -- should see above shard identifiers
 SELECT load_shard_id_array('events_hash');
-      load_shard_id_array
----------------------------------------------------------------------
- {540000,540001,540002,540003}
-(1 row)
-
+ERROR:  relation events_hash is not distributed
 -- should see array with first shard range
 SELECT load_shard_interval_array(540000, 0);
- load_shard_interval_array
----------------------------------------------------------------------
- {-2147483648,-1073741825}
-(1 row)
-
+ERROR:  could not find valid entry for shard 540000
 -- should even work for range-partitioned shards
 -- create range distributed table
 CREATE TABLE events_range (
 	id bigint,
 	name text
 );
 SELECT create_distributed_table('events_range', 'name', 'range');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 -- create empty shard
 SELECT master_create_empty_shard('events_range');
- master_create_empty_shard
----------------------------------------------------------------------
-                    540004
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 UPDATE pg_dist_shard SET
 	shardminvalue = 'Aardvark',
 	shardmaxvalue = 'Zebra'
 WHERE shardid = 540004;
 SELECT load_shard_interval_array(540004, ''::text);
- load_shard_interval_array
----------------------------------------------------------------------
- {Aardvark,Zebra}
-(1 row)
-
+ERROR:  could not find valid entry for shard 540004
 -- should see error for non-existent shard
 SELECT load_shard_interval_array(540005, 0);
 ERROR:  could not find valid entry for shard 540005
 -- should see two placements
 SELECT load_shard_placement_array(540001, false);
-    load_shard_placement_array
----------------------------------------------------------------------
- {localhost:xxxxx,localhost:xxxxx}
-(1 row)
-
+ERROR:  could not find valid entry for shard 540001
 -- only one of which is active
 SELECT load_shard_placement_array(540001, true);
- load_shard_placement_array
----------------------------------------------------------------------
- {localhost:xxxxx}
-(1 row)
-
+ERROR:  could not find valid entry for shard 540001
 -- should see error for non-existent shard
 SELECT load_shard_placement_array(540001, false);
-    load_shard_placement_array
----------------------------------------------------------------------
- {localhost:xxxxx,localhost:xxxxx}
-(1 row)
-
+ERROR:  could not find valid entry for shard 540001
 -- should see column id of 'name'
 SELECT partition_column_id('events_hash');
- partition_column_id
----------------------------------------------------------------------
-                   2
-(1 row)
-
+ERROR:  relation events_hash is not distributed
 -- should see hash partition type and fail for non-distributed tables
 SELECT partition_type('events_hash');
- partition_type
----------------------------------------------------------------------
- h
-(1 row)
-
+ERROR:  relation events_hash is not distributed
 SELECT partition_type('pg_type');
 ERROR:  relation pg_type is not distributed
 -- should see true for events_hash, false for others
 SELECT is_distributed_table('events_hash');
  is_distributed_table 
 ----------------------
- t
+ f
 (1 row)
 
 SELECT is_distributed_table('pg_type');
  is_distributed_table 
 ----------------------
  f
 (1 row)
 
 SELECT is_distributed_table('pg_dist_shard');
  is_distributed_table 
@@ -176,25 +137,21 @@
 ERROR:  column "non_existent" of relation "events_hash" does not exist
 -- drop shard rows (must drop placements first)
 DELETE FROM pg_dist_placement
 	WHERE shardid BETWEEN 540000 AND 540004;
 DELETE FROM pg_dist_shard
 	WHERE logicalrelid = 'events_hash'::regclass;
 DELETE FROM pg_dist_shard
 	WHERE logicalrelid = 'events_range'::regclass;
 -- verify that an eager load shows them missing
 SELECT load_shard_id_array('events_hash');
- load_shard_id_array
----------------------------------------------------------------------
- {}
-(1 row)
-
+ERROR:  relation events_hash is not distributed
 -- create second table to distribute
 CREATE TABLE customers (
 	id bigint,
 	name text
 );
 -- now we'll distribute using function calls but verify metadata manually...
 -- partition on id and manually inspect partition row
 INSERT INTO pg_dist_partition (logicalrelid, partmethod, partkey)
 VALUES
 	('customers'::regclass, 'h', column_name_to_column('customers'::regclass, 'id'));
@@ -255,122 +212,71 @@
 SELECT COUNT(*) FROM pg_locks WHERE locktype = 'advisory' AND objid = 5;
  count 
 -------
      0
 (1 row)
 
 -- test get_shard_id_for_distribution_column
 SET citus.shard_count TO 4;
 CREATE TABLE get_shardid_test_table1(column1 int, column2 int);
 SELECT create_distributed_table('get_shardid_test_table1', 'column1');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 \COPY get_shardid_test_table1 FROM STDIN with delimiter '|';
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table1', 1);
- get_shard_id_for_distribution_column
----------------------------------------------------------------------
-                               540006
-(1 row)
-
+ERROR:  relation is not distributed
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table1', 2);
- get_shard_id_for_distribution_column
----------------------------------------------------------------------
-                               540009
-(1 row)
-
+ERROR:  relation is not distributed
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table1', 3);
- get_shard_id_for_distribution_column
----------------------------------------------------------------------
-                               540007
-(1 row)
-
+ERROR:  relation is not distributed
 -- verify result of the get_shard_id_for_distribution_column
 \c - - - :worker_1_port
 SET search_path TO metadata_test;
 SELECT * FROM get_shardid_test_table1_540006;
- column1 | column2
----------------------------------------------------------------------
-       1 |       1
-(1 row)
-
+ERROR:  relation "get_shardid_test_table1_540006" does not exist
 SELECT * FROM get_shardid_test_table1_540009;
- column1 | column2
----------------------------------------------------------------------
-       2 |       2
-(1 row)
-
+ERROR:  relation "get_shardid_test_table1_540009" does not exist
 SELECT * FROM get_shardid_test_table1_540007;
- column1 | column2
----------------------------------------------------------------------
-       3 |       3
-(1 row)
-
+ERROR:  relation "get_shardid_test_table1_540007" does not exist
 \c - - - :master_port
 SET search_path TO metadata_test;
 -- test non-existing value
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table1', 4);
- get_shard_id_for_distribution_column
----------------------------------------------------------------------
-                               540007
-(1 row)
-
+ERROR:  relation is not distributed
 -- test array type
 SET citus.shard_count TO 4;
 CREATE TABLE get_shardid_test_table2(column1 text[], column2 int);
 SELECT create_distributed_table('get_shardid_test_table2', 'column1');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 \COPY get_shardid_test_table2 FROM STDIN with delimiter '|';
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table2', '{a, b, c}');
- get_shard_id_for_distribution_column
----------------------------------------------------------------------
-                               540013
-(1 row)
-
+ERROR:  relation is not distributed
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table2', '{d, e, f}');
- get_shard_id_for_distribution_column
----------------------------------------------------------------------
-                               540011
-(1 row)
-
+ERROR:  relation is not distributed
 -- verify result of the get_shard_id_for_distribution_column
 \c - - - :worker_1_port
 SET search_path TO metadata_test;
 SELECT * FROM get_shardid_test_table2_540013;
- column1 | column2
----------------------------------------------------------------------
- {a,b,c} |       1
-(1 row)
-
+ERROR:  relation "get_shardid_test_table2_540013" does not exist
 SELECT * FROM get_shardid_test_table2_540011;
- column1 | column2
----------------------------------------------------------------------
- {d,e,f} |       2
-(1 row)
-
+ERROR:  relation "get_shardid_test_table2_540011" does not exist
 \c - - - :master_port
 SET search_path TO metadata_test;
 -- test mismatching data type
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table2', 'a');
-ERROR:  malformed array literal: "a"
-DETAIL:  Array value must start with "{" or dimension information.
+ERROR:  relation is not distributed
 -- test NULL distribution column value for hash distributed table
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table2');
-ERROR:  distribution value cannot be NULL for tables other than reference tables.
+ERROR:  relation is not distributed
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table2', NULL);
-ERROR:  distribution value cannot be NULL for tables other than reference tables.
+ERROR:  relation is not distributed
 -- test non-distributed table
 CREATE TABLE get_shardid_test_table3(column1 int, column2 int);
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table3', 1);
 ERROR:  relation is not distributed
 -- test append distributed table
 SELECT create_distributed_table('get_shardid_test_table3', 'column1', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
@@ -382,138 +288,116 @@
 SELECT create_reference_table('get_shardid_test_table4');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 -- test NULL distribution column value for reference table
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table4');
  get_shard_id_for_distribution_column 
 --------------------------------------
-                               540014
+                               540002
 (1 row)
 
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table4', NULL);
  get_shard_id_for_distribution_column 
 --------------------------------------
-                               540014
+                               540002
 (1 row)
 
 -- test different data types for reference table
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table4', 1);
  get_shard_id_for_distribution_column 
 --------------------------------------
-                               540014
+                               540002
 (1 row)
 
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table4', 'a');
  get_shard_id_for_distribution_column 
 --------------------------------------
-                               540014
+                               540002
 (1 row)
 
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table4', '{a, b, c}');
  get_shard_id_for_distribution_column 
 --------------------------------------
-                               540014
+                               540002
 (1 row)
 
 -- test range distributed table
 CREATE TABLE get_shardid_test_table5(column1 int, column2 int);
 SELECT create_distributed_table('get_shardid_test_table5', 'column1', 'range');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 -- create worker shards
 SELECT master_create_empty_shard('get_shardid_test_table5');
- master_create_empty_shard
----------------------------------------------------------------------
-                    540015
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 SELECT master_create_empty_shard('get_shardid_test_table5');
- master_create_empty_shard
----------------------------------------------------------------------
-                    540016
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 SELECT master_create_empty_shard('get_shardid_test_table5');
- master_create_empty_shard
----------------------------------------------------------------------
-                    540017
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 SELECT master_create_empty_shard('get_shardid_test_table5');
- master_create_empty_shard
----------------------------------------------------------------------
-                    540018
-(1 row)
-
+ERROR:  could only find 1 of 2 possible nodes
 -- now the comparison is done via the partition column type, which is text
 UPDATE pg_dist_shard SET shardminvalue = 1, shardmaxvalue = 1000 WHERE shardid = 540015;
 UPDATE pg_dist_shard SET shardminvalue = 1001, shardmaxvalue = 2000 WHERE shardid = 540016;
 UPDATE pg_dist_shard SET shardminvalue = 2001, shardmaxvalue = 3000 WHERE shardid = 540017;
 UPDATE pg_dist_shard SET shardminvalue = 3001, shardmaxvalue = 4000 WHERE shardid = 540018;
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table5', 5);
  get_shard_id_for_distribution_column 
 --------------------------------------
-                               540015
+                                    0
 (1 row)
 
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table5', 1111);
  get_shard_id_for_distribution_column 
 --------------------------------------
-                               540016
+                                    0
 (1 row)
 
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table5', 2689);
  get_shard_id_for_distribution_column 
 --------------------------------------
-                               540017
+                                    0
 (1 row)
 
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table5', 3248);
  get_shard_id_for_distribution_column 
 --------------------------------------
-                               540018
+                                    0
 (1 row)
 
 -- test non-existing value for range distributed tables
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table5', 4001);
  get_shard_id_for_distribution_column 
 --------------------------------------
                                     0
 (1 row)
 
 SELECT get_shard_id_for_distribution_column('get_shardid_test_table5', -999);
  get_shard_id_for_distribution_column 
 --------------------------------------
                                     0
 (1 row)
 
 SET citus.shard_count TO 2;
 CREATE TABLE events_table_count (user_id int, time timestamp, event_type int, value_2 int, value_3 float, value_4 bigint);
 SELECT create_distributed_table('events_table_count', 'user_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE users_table_count (user_id int, time timestamp, value_1 int, value_2 int, value_3 float, value_4 bigint);
 SELECT create_distributed_table('users_table_count', 'user_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT relation_count_in_query($$-- we can support arbitrary subqueries within UNIONs
 SELECT ("final_query"."event_types") as types, count(*) AS sumOfEventType
 FROM
   ( SELECT
       *, random()
     FROM
      (SELECT
         "t"."user_id", "t"."time", unnest("t"."collected_events") AS "event_types"
       FROM
         ( SELECT
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_upsert.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_upsert.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_upsert.out.modified	2022-11-09 13:38:23.019312417 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_upsert.out.modified	2022-11-09 13:38:23.019312417 +0300
@@ -1,25 +1,22 @@
 -- this test file aims to test UPSERT feature on Citus
 SET citus.next_shard_id TO 980000;
 CREATE TABLE upsert_test
 (
 	part_key int UNIQUE,
 	other_col int,
 	third_col int
 );
 -- distribute the table and create shards
 SELECT create_distributed_table('upsert_test', 'part_key', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- do a regular insert
 INSERT INTO upsert_test (part_key, other_col) VALUES (1, 1), (2, 2);
 -- observe that there is a conflict and the following query does nothing
 INSERT INTO upsert_test (part_key, other_col) VALUES (1, 1) ON CONFLICT DO NOTHING;
 -- same as the above with different syntax
 INSERT INTO upsert_test (part_key, other_col) VALUES (1, 1) ON CONFLICT (part_key) DO NOTHING;
 --again the same query with another syntax
 INSERT INTO upsert_test (part_key, other_col) VALUES (1, 1) ON CONFLICT ON CONSTRAINT upsert_test_part_key_key DO NOTHING;
 -- now, update the columns
 INSERT INTO upsert_test (part_key, other_col) VALUES (1, 1)
@@ -135,64 +132,53 @@
 -- create another table
 CREATE TABLE upsert_test_2
 (
 	part_key int,
 	other_col int,
 	third_col int,
 	PRIMARY KEY (part_key, other_col)
 );
 -- distribute the table and create shards
 SELECT create_distributed_table('upsert_test_2', 'part_key', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- now show that Citus works with multiple columns as the PRIMARY KEY, including the partiton key
 INSERT INTO upsert_test_2 (part_key, other_col) VALUES (1, 1);
 INSERT INTO upsert_test_2 (part_key, other_col) VALUES (1, 1) ON CONFLICT (part_key, other_col) DO NOTHING;
 -- this errors out since there is no unique constraint on partition key
 INSERT INTO upsert_test_2 (part_key, other_col) VALUES (1, 1) ON CONFLICT (part_key) DO NOTHING;
 ERROR:  there is no unique or exclusion constraint matching the ON CONFLICT specification
-CONTEXT:  while executing command on localhost:xxxxx
 -- create another table
 CREATE TABLE upsert_test_3
 (
 	part_key int,
 	count int
 );
 -- note that this is not a unique index
 CREATE INDEX idx_ups_test ON upsert_test_3(part_key);
 -- distribute the table and create shards
 SELECT create_distributed_table('upsert_test_3', 'part_key', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- since there are no unique indexes, error-out
 INSERT INTO upsert_test_3 VALUES (1, 0) ON CONFLICT(part_key) DO UPDATE SET count = upsert_test_3.count + 1;
 ERROR:  there is no unique or exclusion constraint matching the ON CONFLICT specification
-CONTEXT:  while executing command on localhost:xxxxx
 -- create another table
 CREATE TABLE upsert_test_4
 (
 	part_key int UNIQUE,
 	count int
 );
 -- distribute the table and create shards
 SELECT create_distributed_table('upsert_test_4', 'part_key', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- a single row insert
 INSERT INTO upsert_test_4 VALUES (1, 0);
 -- show a simple count example use case
 INSERT INTO upsert_test_4 VALUES (1, 0) ON CONFLICT(part_key) DO UPDATE SET count = upsert_test_4.count + 1;
 INSERT INTO upsert_test_4 VALUES (1, 0) ON CONFLICT(part_key) DO UPDATE SET count = upsert_test_4.count + 1;
 INSERT INTO upsert_test_4 VALUES (1, 0) ON CONFLICT(part_key) DO UPDATE SET count = upsert_test_4.count + 1;
 INSERT INTO upsert_test_4 VALUES (1, 0) ON CONFLICT(part_key) DO UPDATE SET count = upsert_test_4.count + 1;
 INSERT INTO upsert_test_4 VALUES (1, 0) ON CONFLICT(part_key) DO UPDATE SET count = upsert_test_4.count + 1;
 INSERT INTO upsert_test_4 VALUES (1, 0) ON CONFLICT(part_key) DO UPDATE SET count = upsert_test_4.count + 1;
 -- now see the results
@@ -219,28 +205,22 @@
 ALTER TABLE dropcol_distributed DROP COLUMN keep2;
 INSERT INTO dropcol_distributed AS dropcol (key, keep1) VALUES (1, '5') ON CONFLICT(key)
 	DO UPDATE SET keep1 = dropcol.keep1;
 ALTER TABLE dropcol_distributed DROP COLUMN drop1;
 INSERT INTO dropcol_distributed AS dropcol (key, keep1) VALUES (1, '5') ON CONFLICT(key)
 	DO UPDATE SET keep1 = dropcol.keep1;
 -- below we test the cases that Citus does not support
 -- subquery in the SET clause
 INSERT INTO upsert_test (part_key, other_col) VALUES (1, 1) ON CONFLICT (part_key) DO
 	UPDATE SET other_col = (SELECT count(*) from upsert_test);
-ERROR:  subqueries are not supported within INSERT queries
-HINT:  Try rewriting your queries with 'INSERT INTO ... SELECT' syntax.
 -- non mutable function call in the SET
 INSERT INTO upsert_test (part_key, other_col) VALUES (1, 1) ON CONFLICT (part_key) DO
 	UPDATE SET other_col = random()::int;
-ERROR:  functions used in the DO UPDATE SET clause of INSERTs on distributed tables must be marked IMMUTABLE
 -- non mutable function call in the WHERE
 INSERT INTO upsert_test (part_key, other_col) VALUES (1, 1) ON CONFLICT (part_key) DO
 	UPDATE SET other_col = 5 WHERE upsert_test.other_col = random()::int;
-ERROR:  functions used in the WHERE clause of the ON CONFLICT clause of INSERTs on distributed tables must be marked IMMUTABLE
 -- non mutable function call in the arbiter WHERE
 INSERT INTO upsert_test (part_key, other_col) VALUES (1, 1) ON CONFLICT (part_key)  WHERE part_key = random()::int
 	DO UPDATE SET other_col = 5;
-ERROR:  functions used in the WHERE clause of the ON CONFLICT clause of INSERTs on distributed tables must be marked IMMUTABLE
 -- error out on attempt to update the partition key
 INSERT INTO upsert_test (part_key, other_col) VALUES (1, 1) ON CONFLICT (part_key) DO
 	UPDATE SET part_key = 15;
-ERROR:  modifying the partition value of rows is not allowed
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_simple_queries.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_simple_queries.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_simple_queries.out.modified	2022-11-09 13:38:23.129312416 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_simple_queries.out.modified	2022-11-09 13:38:23.139312416 +0300
@@ -561,47 +561,29 @@
 -------+------+-------+-----
  18185 | 2728 | 61782 |   5
 (1 row)
 
 -- error out for queries with repartition jobs
 SELECT *
 	FROM articles a, articles b
 	WHERE a.id = b.id  AND a.author_id = 1;
 DEBUG:  Router planner cannot handle multi-shard select queries
 DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
 DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
 DEBUG:  pruning merge fetch taskId 1
 DETAIL:  Creating dependency on merge taskId 2
 DEBUG:  pruning merge fetch taskId 2
 DETAIL:  Creating dependency on merge taskId 3
 DEBUG:  pruning merge fetch taskId 4
 DETAIL:  Creating dependency on merge taskId 4
 DEBUG:  pruning merge fetch taskId 5
 DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 9
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 12
 ERROR:  the query contains a join that requires repartitioning
 HINT:  Set citus.enable_repartition_joins to on to enable repartitioning
 -- system columns from shard tables can be queried and retrieved
 SELECT count(*) FROM (
     SELECT tableoid, ctid, cmin, cmax, xmin, xmax
         FROM articles
         WHERE tableoid IS NOT NULL OR
                   ctid IS NOT NULL OR
                   cmin IS NOT NULL OR
                   cmax IS NOT NULL OR
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/citus_copy_shard_placement.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/citus_copy_shard_placement.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/citus_copy_shard_placement.out.modified	2022-11-09 13:38:23.449312415 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/citus_copy_shard_placement.out.modified	2022-11-09 13:38:23.459312415 +0300
@@ -41,90 +41,69 @@
 INSERT INTO data VALUES ('key-1', 'value-1');
 INSERT INTO data VALUES ('key-2', 'value-2');
 INSERT INTO history VALUES ('key-1', '2020-02-01', 'old');
 INSERT INTO history VALUES ('key-1', '2019-10-01', 'older');
 -- verify we error out if no healthy placement exists at source
 SELECT citus_copy_shard_placement(
            get_shard_id_for_distribution_column('data', 'key-1'),
            'localhost', :worker_1_port,
            'localhost', :worker_2_port,
            transfer_mode := 'block_writes');
-ERROR:  could not find placement matching "localhost:xxxxx"
-HINT:  Confirm the placement still exists and try again.
+ERROR:  Moving shards to a non-existing node is not supported
 -- verify we error out if source and destination are the same
 SELECT citus_copy_shard_placement(
            get_shard_id_for_distribution_column('data', 'key-1'),
            'localhost', :worker_2_port,
            'localhost', :worker_2_port,
            transfer_mode := 'block_writes');
 ERROR:  cannot copy shard to the same node
 -- verify we warn if target already contains a healthy placement
 SELECT citus_copy_shard_placement(
            (SELECT shardid FROM pg_dist_shard WHERE logicalrelid='ref_table'::regclass::oid),
            'localhost', :worker_1_port,
            'localhost', :worker_2_port,
            transfer_mode := 'block_writes');
-WARNING:  shard is already present on node localhost:xxxxx
-DETAIL:  Copy may have already completed.
- citus_copy_shard_placement
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  Moving shards to a non-existing node is not supported
 -- verify we error out if table has foreign key constraints
 INSERT INTO ref_table SELECT 1, value FROM data;
 ALTER TABLE data ADD CONSTRAINT distfk FOREIGN KEY (value) REFERENCES ref_table (b) MATCH FULL;
 SELECT citus_copy_shard_placement(
            get_shard_id_for_distribution_column('data', 'key-1'),
            'localhost', :worker_2_port,
            'localhost', :worker_1_port);
-ERROR:  cannot replicate shards with foreign keys
+ERROR:  Copying shards to a non-existing node is not supported
+HINT:  Add the target node via SELECT citus_add_node('localhost', 57637);
 ALTER TABLE data DROP CONSTRAINT distfk;
 -- replicate shard that contains key-1
 SELECT citus_copy_shard_placement(
            get_shard_id_for_distribution_column('data', 'key-1'),
            'localhost', :worker_2_port,
            'localhost', :worker_1_port,
            transfer_mode := 'block_writes');
- citus_copy_shard_placement
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  Copying shards to a non-existing node is not supported
+HINT:  Add the target node via SELECT citus_add_node('localhost', 57637);
 -- forcefully mark the old replica as inactive
 UPDATE pg_dist_shard_placement SET shardstate = 3
 WHERE shardid = get_shard_id_for_distribution_column('data', 'key-1') AND nodeport = :worker_2_port;
 UPDATE pg_dist_shard_placement SET shardstate = 3
 WHERE shardid = get_shard_id_for_distribution_column('history', 'key-1') AND nodeport = :worker_2_port;
 -- should still have all data available thanks to new replica
 SELECT count(*) FROM data;
- count
----------------------------------------------------------------------
-     2
-(1 row)
-
+ERROR:  no active placements were found for shard 8139002
 SELECT count(*) FROM history;
- count
----------------------------------------------------------------------
-     2
-(1 row)
-
+ERROR:  no active placements were found for shard 8139006
 -- test we can replicate MX tables
 SET citus.shard_replication_factor TO 1;
 -- metadata sync will succeed even if we have rep > 1 tables
 SET client_min_messages TO warning;
 SELECT start_metadata_sync_to_node('localhost', :worker_1_port);
- start_metadata_sync_to_node
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  node at "localhost:57637" does not exist
 RESET client_min_messages;
 CREATE TABLE mx_table(a int);
 SELECT create_distributed_table('mx_table', 'a');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT citus_copy_shard_placement(
            get_shard_id_for_distribution_column('mx_table', '1'),
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_utilities.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_utilities.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_utilities.out.modified	2022-11-09 13:38:42.599312336 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_utilities.out.modified	2022-11-09 13:38:42.609312336 +0300
@@ -169,104 +169,82 @@
 ERROR:  no locks specified
 -- drop table
 DROP TABLE sharded_table;
 DROP TABLE lockable_table;
 -- VACUUM tests
 -- create a table with a single shard (for convenience)
 SET citus.shard_count TO 1;
 SET citus.shard_replication_factor TO 2;
 CREATE TABLE dustbunnies (id integer, name text, age integer);
 SELECT create_distributed_table('dustbunnies', 'id', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- add some data to the distributed table
 \copy dustbunnies (id, name) from stdin with csv
 CREATE TABLE second_dustbunnies(id integer, name text, age integer);
 SET citus.shard_replication_factor TO 2;
 SELECT create_distributed_table('second_dustbunnies', 'id', 'hash', shard_count := 1);
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- run VACUUM and ANALYZE against the table on the master
 \c - - :master_host :master_port
 SET search_path TO multi_utilities, public;
 VACUUM dustbunnies;
 ANALYZE dustbunnies;
 -- send a VACUUM FULL and a VACUUM ANALYZE
 VACUUM (FULL) dustbunnies;
 VACUUM ANALYZE dustbunnies;
 \c - - :public_worker_1_host :worker_1_port
 SET search_path TO multi_utilities, public;
 -- disable auto-VACUUM for next test
 ALTER TABLE dustbunnies_990002 SET (autovacuum_enabled = false);
+ERROR:  relation "dustbunnies_990002" does not exist
 SELECT relfrozenxid AS frozenxid FROM pg_class WHERE oid='dustbunnies_990002'::regclass
 \gset
+ERROR:  relation "dustbunnies_990002" does not exist
 -- send a VACUUM FREEZE after adding a new row
 \c - - :master_host :master_port
 SET search_path TO multi_utilities, public;
 INSERT INTO dustbunnies VALUES (5, 'peter');
 VACUUM (FREEZE) dustbunnies;
 -- verify that relfrozenxid increased
 \c - - :public_worker_1_host :worker_1_port
 SET search_path TO multi_utilities, public;
 SELECT relfrozenxid::text::integer > :frozenxid AS frozen_performed FROM pg_class
 WHERE oid='dustbunnies_990002'::regclass;
- frozen_performed
----------------------------------------------------------------------
- t
-(1 row)
-
+ERROR:  syntax error at or near ":"
 -- check there are no nulls in either column
 SELECT attname, null_frac FROM pg_stats
 WHERE tablename = 'dustbunnies_990002' ORDER BY attname;
  attname | null_frac 
 ---------+-----------
- age     |         1
- id      |         0
- name    |         0
-(3 rows)
+(0 rows)
 
 -- add NULL values, then perform column-specific ANALYZE
 \c - - :master_host :master_port
 SET search_path TO multi_utilities, public;
 INSERT INTO dustbunnies VALUES (6, NULL, NULL);
 ANALYZE dustbunnies (name);
 -- verify that name's NULL ratio is updated but age's is not
 \c - - :public_worker_1_host :worker_1_port
 SET search_path TO multi_utilities, public;
 SELECT attname, null_frac FROM pg_stats
 WHERE tablename = 'dustbunnies_990002' ORDER BY attname;
  attname | null_frac 
 ---------+-----------
- age     |         1
- id      |         0
- name    |  0.166667
-(3 rows)
+(0 rows)
 
 \c - - :master_host :master_port
 SET search_path TO multi_utilities, public;
 SET citus.log_remote_commands TO ON;
 -- check for multiple table vacuum
 VACUUM dustbunnies, second_dustbunnies;
-NOTICE:  issuing VACUUM multi_utilities.dustbunnies_990002
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing VACUUM multi_utilities.dustbunnies_990002
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing VACUUM multi_utilities.second_dustbunnies_990003
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing VACUUM multi_utilities.second_dustbunnies_990003
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 -- and do not propagate when using targeted VACUUM without DDL propagation
 SET citus.enable_ddl_propagation to false;
 VACUUM dustbunnies;
 ANALYZE dustbunnies;
 SET citus.enable_ddl_propagation to DEFAULT;
 -- test worker_hash
 SELECT worker_hash(123);
  worker_hash 
 -------------
   -205084363
@@ -292,24 +270,20 @@
 ERROR:  must be called as trigger
 -- make sure worker_create_or_alter_role does not crash with NULL input
 SELECT worker_create_or_alter_role(NULL, NULL, NULL);
 ERROR:  role name cannot be NULL
 SELECT worker_create_or_alter_role(NULL, 'create role dontcrash', NULL);
 ERROR:  role name cannot be NULL
 -- confirm that citus_create_restore_point works
 SELECT 1 FROM citus_create_restore_point('regression-test');
 NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(0, 0, '1999-12-31 16:00:00-08');
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
-NOTICE:  issuing BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;SELECT assign_distributed_transaction_id(xx, xx, 'xxxxxxx');
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing SELECT pg_catalog.pg_create_restore_point($1::text)
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing SELECT pg_catalog.pg_create_restore_point($1::text)
 DETAIL:  on server postgres@localhost:57638 connectionId: 1
  ?column? 
 ----------
         1
 (1 row)
 
 SET citus.shard_count TO 1;
 SET citus.shard_replication_factor TO 1;
 SET citus.next_shard_id TO 970000;
@@ -327,28 +301,22 @@
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SET citus.log_remote_commands TO ON;
 -- should propagate to all workers because no table is specified
 VACUUM;
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'off'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing VACUUM 
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
-NOTICE:  issuing VACUUM
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
-NOTICE:  issuing SET citus.enable_ddl_propagation TO 'on'
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 NOTICE:  issuing SET citus.enable_ddl_propagation TO 'on'
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
 -- should not propagate because no distributed table is specified
 insert into local_vacuum_table select i from generate_series(1,1000000) i;
 delete from local_vacuum_table;
 VACUUM local_vacuum_table;
 SELECT CASE WHEN s BETWEEN 20000000 AND 25000000 THEN 22500000 ELSE s END
 FROM pg_total_relation_size('local_vacuum_table') s ;
     s     
 ----------
@@ -361,40 +329,34 @@
 FROM pg_total_relation_size('local_vacuum_table') s ;
  size  
 -------
  25000
 (1 row)
 
 -- should propagate to all workers because table is reference table
 VACUUM reference_vacuum_table;
 NOTICE:  issuing VACUUM multi_utilities.reference_vacuum_table_970000
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
-NOTICE:  issuing VACUUM multi_utilities.reference_vacuum_table_970000
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 -- should propagate to all workers because table is distributed table
 VACUUM distributed_vacuum_table;
 NOTICE:  issuing VACUUM multi_utilities.distributed_vacuum_table_970001
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
 -- only distributed_vacuum_table and reference_vacuum_table should propagate
 VACUUM distributed_vacuum_table, local_vacuum_table, reference_vacuum_table;
 NOTICE:  issuing VACUUM multi_utilities.distributed_vacuum_table_970001
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
 NOTICE:  issuing VACUUM multi_utilities.reference_vacuum_table_970000
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
-NOTICE:  issuing VACUUM multi_utilities.reference_vacuum_table_970000
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 -- only reference_vacuum_table should propagate
 VACUUM local_vacuum_table, reference_vacuum_table;
 NOTICE:  issuing VACUUM multi_utilities.reference_vacuum_table_970000
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
-NOTICE:  issuing VACUUM multi_utilities.reference_vacuum_table_970000
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 -- vacuum (disable_page_skipping) aggressively process pages of the relation, it does not respect visibility map
 VACUUM (DISABLE_PAGE_SKIPPING true) local_vacuum_table;
 VACUUM (DISABLE_PAGE_SKIPPING false) local_vacuum_table;
 -- vacuum (index_cleanup on, parallel 1) should execute index vacuuming and index cleanup phases in parallel
 insert into local_vacuum_table select i from generate_series(1,1000000) i;
 delete from local_vacuum_table;
 VACUUM (INDEX_CLEANUP OFF, PARALLEL 1) local_vacuum_table;
 SELECT CASE WHEN s BETWEEN 50000000 AND 70000000 THEN 60000000 ELSE s END size
 FROM pg_total_relation_size('local_vacuum_table') s ;
    size   
@@ -431,22 +393,20 @@
 select analyze_count from pg_stat_all_tables where relname = 'local_vacuum_table' or relname = 'reference_vacuum_table';
  analyze_count 
 ---------------
              0
              0
 (2 rows)
 
 vacuum (analyze) local_vacuum_table, reference_vacuum_table;
 NOTICE:  issuing VACUUM (ANALYZE) multi_utilities.reference_vacuum_table_970000
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
-NOTICE:  issuing VACUUM (ANALYZE) multi_utilities.reference_vacuum_table_970000
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 -- give enough time for stats to be updated.(updated per 500ms by default)
 select pg_sleep(1);
  pg_sleep 
 ----------
  
 (1 row)
 
 select analyze_count from pg_stat_all_tables where relname = 'local_vacuum_table' or relname = 'reference_vacuum_table';
  analyze_count 
 ---------------
@@ -482,48 +442,40 @@
 --------------------------
  
 (1 row)
 
 SET citus.log_remote_commands TO ON;
 SET citus.grep_remote_commands = '%ANALYZE%';
 -- should propagate to all workers because no table is specified
 ANALYZE;
 NOTICE:  issuing ANALYZE 
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
-NOTICE:  issuing ANALYZE
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 -- should not propagate because no distributed table is specified
 ANALYZE local_analyze_table;
 -- should propagate to all workers because table is reference table
 ANALYZE reference_analyze_table;
 NOTICE:  issuing ANALYZE multi_utilities.reference_analyze_table_970002
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
-NOTICE:  issuing ANALYZE multi_utilities.reference_analyze_table_970002
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 -- should propagate to all workers because table is distributed table
 ANALYZE distributed_analyze_table;
 NOTICE:  issuing ANALYZE multi_utilities.distributed_analyze_table_970003
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
 -- only distributed_analyze_table and reference_analyze_table should propagate
 ANALYZE distributed_analyze_table, local_analyze_table, reference_analyze_table;
 NOTICE:  issuing ANALYZE multi_utilities.distributed_analyze_table_970003
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
 NOTICE:  issuing ANALYZE multi_utilities.reference_analyze_table_970002
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
-NOTICE:  issuing ANALYZE multi_utilities.reference_analyze_table_970002
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 -- only reference_analyze_table should propagate
 ANALYZE local_analyze_table, reference_analyze_table;
 NOTICE:  issuing ANALYZE multi_utilities.reference_analyze_table_970002
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
-NOTICE:  issuing ANALYZE multi_utilities.reference_analyze_table_970002
-DETAIL:  on server postgres@localhost:xxxxx connectionId: xxxxxxx
 -- should not propagate because ddl propagation is disabled
 SET citus.enable_ddl_propagation TO OFF;
 ANALYZE distributed_analyze_table;
 SET citus.enable_ddl_propagation TO ON;
 -- analyze only specified columns for corresponding tables
 ANALYZE loc(b), dist(a);
 NOTICE:  issuing ANALYZE multi_utilities.dist_970004 (a)
 DETAIL:  on server postgres@localhost:57638 connectionId: 2
 RESET citus.log_remote_commands;
 RESET citus.grep_remote_commands;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/foreign_key_to_reference_table.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/foreign_key_to_reference_table.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/foreign_key_to_reference_table.out.modified	2022-11-09 13:38:49.079312310 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/foreign_key_to_reference_table.out.modified	2022-11-09 13:38:49.109312310 +0300
@@ -257,48 +257,36 @@
  count 
 -------
      0
 (1 row)
 
 DROP TABLE referencing_table;
 -- foreign keys are only supported when the replication factor = 1
 SET citus.shard_replication_factor TO 2;
 CREATE TABLE referencing_table(id int, ref_id int);
 SELECT create_distributed_table('referencing_table', 'ref_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ALTER TABLE referencing_table ADD CONSTRAINT fkey_ref FOREIGN KEY (id) REFERENCES referenced_table(id);
-ERROR:  cannot create foreign key constraint
-DETAIL:  Citus currently supports foreign key constraints only for "citus.shard_replication_factor = 1".
-HINT:  Please change "citus.shard_replication_factor to 1". To learn more about using foreign keys with other replication factors, please contact us at https://citusdata.com/about/contact_us.
 SELECT COUNT(*) FROM table_fkeys_in_workers WHERE relid SIMILAR TO 'fkey_reference_table.%\d{2,}' AND refd_relid SIMILAR TO 'fkey_reference_table.%\d{2,}';
  count 
 -------
      0
 (1 row)
 
 DROP TABLE referencing_table;
 -- should fail when we add the column as well
 CREATE TABLE referencing_table(id int, ref_id int);
 SELECT create_distributed_table('referencing_table', 'ref_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ALTER TABLE referencing_table ADD COLUMN referencing_col int REFERENCES referenced_table(id) ON DELETE SET NULL;
-ERROR:  cannot create foreign key constraint
-DETAIL:  Citus currently supports foreign key constraints only for "citus.shard_replication_factor = 1".
-HINT:  Please change "citus.shard_replication_factor to 1". To learn more about using foreign keys with other replication factors, please contact us at https://citusdata.com/about/contact_us.
 SELECT COUNT(*) FROM table_fkeys_in_workers WHERE relid SIMILAR TO 'fkey_reference_table.%\d{2,}' AND refd_relid SIMILAR TO 'fkey_reference_table.%\d{2,}';
  count 
 -------
      0
 (1 row)
 
 DROP TABLE referencing_table;
 SET citus.shard_replication_factor TO 1;
 -- simple create_distributed_table should work in/out transactions on tables with foreign key to reference tables
 CREATE TABLE referencing_table(id int, ref_id int, FOREIGN KEY (id) REFERENCES referenced_table(id));
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/validate_constraint.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/validate_constraint.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/validate_constraint.out.modified	2022-11-09 13:38:49.209312309 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/validate_constraint.out.modified	2022-11-09 13:38:49.219312309 +0300
@@ -115,30 +115,29 @@
 ------------------------+------------
  validatable_constraint | t
 (1 row)
 
 SELECT *
 FROM constraint_validations_in_workers
 ORDER BY 1, 2;
               name              | validated 
 --------------------------------+-----------
  validatable_constraint         | t
- validatable_constraint         | t
  validatable_constraint_8000009 | t
  validatable_constraint_8000010 | t
  validatable_constraint_8000011 | t
  validatable_constraint_8000012 | t
  validatable_constraint_8000013 | t
  validatable_constraint_8000014 | t
  validatable_constraint_8000015 | t
  validatable_constraint_8000016 | t
-(10 rows)
+(9 rows)
 
 DROP TABLE constrained_table;
 DROP TABLE referenced_table CASCADE;
 DROP TABLE referencing_table;
 DROP SCHEMA validate_constraint CASCADE;
 NOTICE:  drop cascades to 3 other objects
 DETAIL:  drop cascades to type constraint_validity
 drop cascades to view constraint_validations_in_workers
 drop cascades to view constraint_validations
 SET search_path TO DEFAULT;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_repartition_udt.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_repartition_udt.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_repartition_udt.out.modified	2022-11-09 13:38:49.519312308 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_repartition_udt.out.modified	2022-11-09 13:38:49.519312308 +0300
@@ -60,32 +60,35 @@
 \c - - :public_worker_1_host :worker_1_port
 -- START type creation
 -- Use function to create a custom equality operator...
 CREATE OPERATOR = (
     LEFTARG = test_udt,
     RIGHTARG = test_udt,
     PROCEDURE = equal_test_udt_function,
 	COMMUTATOR = =,
     HASHES
 );
+ERROR:  type "test_udt" does not exist
 -- ... and create a custom operator family for hash indexes...
 CREATE OPERATOR FAMILY tudt_op_fam USING hash;
 -- We need to define two different operator classes for the composite types
 -- One uses BTREE the other uses HASH
 CREATE OPERATOR CLASS tudt_op_fam_clas3
 DEFAULT FOR TYPE test_udt USING BTREE AS
 OPERATOR 3 = (test_udt, test_udt),
 FUNCTION 1 test_udt_cmp(test_udt, test_udt);
+ERROR:  type "test_udt" does not exist
 CREATE OPERATOR CLASS tudt_op_fam_class
 DEFAULT FOR TYPE test_udt USING HASH AS
 OPERATOR 1 = (test_udt, test_udt),
 FUNCTION 1 test_udt_hash(test_udt);
+ERROR:  type "test_udt" does not exist
 -- END type creation
 \c - - :public_worker_2_host :worker_2_port
 -- START type creation
 -- Use function to create a custom equality operator...
 CREATE OPERATOR = (
     LEFTARG = test_udt,
     RIGHTARG = test_udt,
     PROCEDURE = equal_test_udt_function,
 	COMMUTATOR = =,
     HASHES
@@ -148,28 +151,28 @@
 -- Query that should result in a repartition join on UDT column.
 SET citus.log_multi_join_order = true;
 EXPLAIN (COSTS OFF)
 SELECT * FROM repartition_udt JOIN repartition_udt_other
     ON repartition_udt.udtcol = repartition_udt_other.udtcol
 	WHERE repartition_udt.pk > 1;
 LOG:  join order: [ "repartition_udt" ][ dual partition join "repartition_udt_other" ]
                          QUERY PLAN                          
 -------------------------------------------------------------
  Custom Scan (Citus Adaptive)
-   Task Count: 4
+   Task Count: 2
    Tasks Shown: None, not supported for re-partition queries
    ->  MapMergeJob
          Map Task Count: 3
-         Merge Task Count: 4
+         Merge Task Count: 2
    ->  MapMergeJob
          Map Task Count: 5
-         Merge Task Count: 4
+         Merge Task Count: 2
 (9 rows)
 
 SELECT * FROM repartition_udt JOIN repartition_udt_other
     ON repartition_udt.udtcol = repartition_udt_other.udtcol
 	WHERE repartition_udt.pk > 1
 	ORDER BY repartition_udt.pk;
 LOG:  join order: [ "repartition_udt" ][ dual partition join "repartition_udt_other" ]
  pk | udtcol | txtcol | pk | udtcol | txtcol 
 ----+--------+--------+----+--------+--------
   2 | (1,2)  | foo    |  8 | (1,2)  | foo
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_subtransactions.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_subtransactions.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_subtransactions.out.modified	2022-11-09 13:38:49.669312307 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_subtransactions.out.modified	2022-11-09 13:38:49.679312307 +0300
@@ -1,22 +1,19 @@
 CREATE SCHEMA multi_subtransactions;
 SET search_path TO 'multi_subtransactions';
 CREATE TABLE artists (
     id bigint NOT NULL,
     name text NOT NULL
 );
 SELECT create_distributed_table('artists', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- add some data
 INSERT INTO artists VALUES (1, 'Pablo Picasso');
 INSERT INTO artists VALUES (2, 'Vincent van Gogh');
 INSERT INTO artists VALUES (3, 'Claude Monet');
 INSERT INTO artists VALUES (4, 'William Kurelek');
 -- RELEASE SAVEPOINT
 BEGIN;
 INSERT INTO artists VALUES (5, 'Asher Lev');
 SAVEPOINT s1;
 DELETE FROM artists WHERE id=5;
@@ -154,90 +151,86 @@
 ----+--------------------
  10 | Mahmoud Farshchian
 (1 row)
 
 -- Recover from multi-shard copy send failure.
 -- Constraint check for partition column happens at copy send.
 BEGIN;
 DELETE FROM artists;
 SAVEPOINT s1;
 INSERT INTO artists SELECT NULL, NULL FROM generate_series(1, 5) i;
-ERROR:  the partition column of table multi_subtransactions.artists cannot be NULL
+ERROR:  null value in column "id" of relation "artists" violates not-null constraint
 ROLLBACK TO s1;
 INSERT INTO artists VALUES (11, 'Egon Schiele');
 COMMIT;
 SELECT * FROM artists WHERE id IN (10, 11) ORDER BY id;
  id |     name     
 ----+--------------
  11 | Egon Schiele
 (1 row)
 
 -- Recover from multi-shard copy startup failure.
 -- Check for existence of a value for partition columnn happens at copy startup.
 BEGIN;
 DELETE FROM artists;
 SAVEPOINT s1;
 INSERT INTO artists(name) SELECT 'a' FROM generate_series(1, 5) i;
-ERROR:  the partition column of table multi_subtransactions.artists should have a value
+ERROR:  null value in column "id" of relation "artists" violates not-null constraint
 ROLLBACK TO s1;
 INSERT INTO artists VALUES (12, 'Marc Chagall');
 COMMIT;
 SELECT * FROM artists WHERE id IN (11, 12) ORDER BY id;
  id |     name     
 ----+--------------
  12 | Marc Chagall
 (1 row)
 
 -- Recover from multi-shard CTE modify failures
 create table t1(a int, b int);
 create table t2(a int, b int CHECK(b > 0));
 ALTER SEQUENCE pg_catalog.pg_dist_shardid_seq RESTART 1190000;
 select create_distributed_table('t1', 'a'),
        create_distributed_table('t2', 'a');
- create_distributed_table | create_distributed_table
----------------------------------------------------------------------
-                          |
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 begin;
 insert into t2 select i, i+1 from generate_series(1, 3) i;
 with r AS (
     update t2 set b = b + 1
     returning *
 ) insert into t1 select * from r;
 savepoint s1;
 with r AS (
     update t1 set b = b - 10
     returning *
 ) insert into t2 select * from r;
-ERROR:  new row for relation "t2_xxxxxxx" violates check constraint "t2_b_check"
+ERROR:  new row for relation "t2" violates check constraint "t2_b_check"
 rollback to savepoint s1;
 savepoint s2;
 with r AS (
     update t2 set b = b - 10
     returning *
 ) insert into t1 select * from r;
-ERROR:  new row for relation "t2_xxxxxxx" violates check constraint "t2_b_check"
+ERROR:  new row for relation "t2" violates check constraint "t2_b_check"
 rollback to savepoint s2;
 savepoint s3;
 with r AS (
     insert into t2 select i, i+1 from generate_series(-10,-5) i
     returning *
 ) insert into t1 select * from r;
-ERROR:  new row for relation "t2_xxxxxxx" violates check constraint "t2_b_check"
+ERROR:  new row for relation "t2" violates check constraint "t2_b_check"
 rollback to savepoint s3;
 savepoint s4;
 with r AS (
     insert into t1 select i, i+1 from generate_series(-10,-5) i
     returning *
 ) insert into t2 select * from r;
-ERROR:  new row for relation "t2_xxxxxxx" violates check constraint "t2_b_check"
+ERROR:  new row for relation "t2" violates check constraint "t2_b_check"
 rollback to savepoint s4;
 with r AS (
     update t2 set b = b + 1
     returning *
 ) insert into t1 select * from r;
 commit;
 select * from t2 order by a, b;
  a | b 
 ---+---
  1 | 4
@@ -260,25 +253,21 @@
 -- ===================================================================
 -- Tests for replication factor > 1
 -- ===================================================================
 CREATE TABLE researchers (
   id bigint NOT NULL,
   lab_id int NOT NULL,
   name text NOT NULL
 );
 SET citus.shard_count TO 2;
 SELECT create_distributed_table('researchers', 'lab_id', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 -- Basic rollback and release
 BEGIN;
 INSERT INTO researchers VALUES (7, 4, 'Jan Plaza');
 SAVEPOINT s1;
 INSERT INTO researchers VALUES (8, 4, 'Alonzo Church');
 ROLLBACK TO s1;
 RELEASE SAVEPOINT s1;
 COMMIT;
 SELECT * FROM researchers WHERE id in (7, 8);
  id | lab_id |   name    
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_modifying_xacts.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_modifying_xacts.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_modifying_xacts.out.modified	2022-11-09 13:38:50.329312305 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_modifying_xacts.out.modified	2022-11-09 13:38:50.339312305 +0300
@@ -7,25 +7,22 @@
 	id bigint NOT NULL,
     lab_id int NOT NULL,
 	name text NOT NULL
 );
 CREATE TABLE labs (
 	id bigint NOT NULL,
 	name text NOT NULL
 );
 SET citus.shard_replication_factor TO 2;
 SELECT create_distributed_table('researchers', 'lab_id', shard_count:=2);
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SET citus.shard_replication_factor TO 1;
 SELECT create_distributed_table('labs', 'id', shard_count:=1);
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 RESET citus.shard_replication_factor;
 -- might be confusing to have two people in the same lab with the same name
 CREATE UNIQUE INDEX avoid_name_confusion_idx ON researchers (lab_id, name);
@@ -64,21 +61,21 @@
 SELECT name FROM researchers WHERE lab_id = 1 AND id = 1;
      name     
 --------------
  Donald Knuth
 (1 row)
 
 -- trigger a unique constraint violation
 BEGIN;
 \set VERBOSITY TERSE
 UPDATE researchers SET name = 'John Backus' WHERE id = 1 AND lab_id = 1;
-ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx_1200000"
+ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx"
 \set VERBOSITY DEFAULT
 ABORT;
 -- creating savepoints should work...
 BEGIN;
 INSERT INTO researchers VALUES (5, 3, 'Dennis Ritchie');
 SAVEPOINT hire_thompson;
 INSERT INTO researchers VALUES (6, 3, 'Ken Thompson');
 COMMIT;
 SELECT name FROM researchers WHERE lab_id = 3 AND id = 6;
      name     
@@ -135,23 +132,22 @@
 
 -- and the other way around is also allowed
 BEGIN;
 INSERT INTO labs VALUES (6, 'Bell Labs');
 INSERT INTO researchers VALUES (9, 6, 'Leslie Lamport');
 COMMIT;
 --  we should be able to expand the transaction participants
 BEGIN;
 INSERT INTO labs VALUES (6, 'Bell Labs');
 INSERT INTO researchers VALUES (9, 6, 'Leslie Lamport');
-ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx_1200001"
+ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx"
 DETAIL:  Key (lab_id, name)=(6, Leslie Lamport) already exists.
-CONTEXT:  while executing command on localhost:xxxxx
 ABORT;
 -- SELECTs may occur after a modification: First check that selecting
 -- from the modified node works.
 BEGIN;
 INSERT INTO labs VALUES (6, 'Bell Labs');
 SELECT count(*) FROM researchers WHERE lab_id = 6;
  count 
 -------
      1
 (1 row)
@@ -258,22 +254,23 @@
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 -- copy with unique index violation
 BEGIN;
 \copy researchers FROM STDIN delimiter ','
 \copy researchers FROM STDIN delimiter ','
-ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx_1200001"
+ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx"
 DETAIL:  Key (lab_id, name)=(6,  'Bjarne Stroustrup') already exists.
+CONTEXT:  COPY researchers, line 1
 COMMIT;
 -- verify rollback
 SELECT * FROM researchers WHERE lab_id = 6;
  id | lab_id |      name      
 ----+--------+----------------
   9 |      6 | Leslie Lamport
 (1 row)
 
 SELECT count(*) FROM pg_dist_transaction;
  count 
@@ -285,22 +282,23 @@
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 -- copy with unique index violation
 BEGIN;
 \copy researchers FROM STDIN delimiter ','
 \copy researchers FROM STDIN delimiter ','
-ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx_1200001"
+ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx"
 DETAIL:  Key (lab_id, name)=(6,  'Bjarne Stroustrup') already exists.
+CONTEXT:  COPY researchers, line 1
 COMMIT;
 -- verify rollback
 SELECT * FROM researchers WHERE lab_id = 6;
  id | lab_id |      name      
 ----+--------+----------------
   9 |      6 | Leslie Lamport
 (1 row)
 
 SELECT count(*) FROM pg_dist_transaction;
  count 
@@ -318,100 +316,92 @@
 ----+--------+----------------------
   9 |      6 | Leslie Lamport
  17 |      6 |  'Bjarne Stroustrup'
  18 |      6 |  'Dennis Ritchie'
 (3 rows)
 
 -- verify 2pc
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     2
+     0
 (1 row)
 
 -- create a check function
 CREATE FUNCTION reject_large_id() RETURNS trigger AS $rli$
     BEGIN
         IF (NEW.id > 30) THEN
             RAISE 'illegal value';
         END IF;
 
         RETURN NEW;
     END;
 $rli$ LANGUAGE plpgsql;
 -- register after insert trigger
 SELECT * FROM run_command_on_placements('researchers', 'CREATE CONSTRAINT TRIGGER reject_large_researcher_id AFTER INSERT ON %s DEFERRABLE INITIALLY DEFERRED FOR EACH ROW EXECUTE PROCEDURE  reject_large_id()')
 ORDER BY nodeport, shardid;
  nodename | nodeport | shardid | success | result 
 ----------+----------+---------+---------+--------
- localhost |    57637 | 1200000 | t       | CREATE TRIGGER
- localhost |    57637 | 1200001 | t       | CREATE TRIGGER
- localhost |    57638 | 1200000 | t       | CREATE TRIGGER
- localhost |    57638 | 1200001 | t       | CREATE TRIGGER
-(4 rows)
+(0 rows)
 
 -- hide postgresql version dependend messages for next test only
 \set VERBOSITY terse
 -- reduce the log level for differences between PG14 and PG15
 -- in PGconn->errorMessage
 -- relevant PG commit b15f254466aefbabcbed001929f6e09db59fd158
 SET client_min_messages to ERROR;
 -- for replicated tables use 2PC even if multi-shard commit protocol
 -- is set to 2PC
 BEGIN;
 DELETE FROM researchers WHERE lab_id = 6;
 \copy researchers FROM STDIN delimiter ','
 \copy researchers FROM STDIN delimiter ','
 COMMIT;
-ERROR:  illegal value
 -- single row, multi-row INSERTs should also fail
 -- with or without transaction blocks on the COMMIT time
 INSERT INTO researchers VALUES (31, 6, 'Bjarne Stroustrup');
-ERROR:  illegal value
 INSERT INTO researchers VALUES (31, 6, 'Bjarne Stroustrup'), (32, 7, 'Bjarne Stroustrup');
-ERROR:  illegal value
+ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx"
 BEGIN;
     INSERT INTO researchers VALUES (31, 6, 'Bjarne Stroustrup');
+ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx"
 COMMIT;
-ERROR:  illegal value
 BEGIN;
     INSERT INTO researchers VALUES (31, 6, 'Bjarne Stroustrup'), (32, 7, 'Bjarne Stroustrup');
+ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx"
 COMMIT;
-ERROR:  illegal value
 -- and, rollback should be fine
 BEGIN;
     INSERT INTO researchers VALUES (31, 6, 'Bjarne Stroustrup');
+ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx"
 ROLLBACK;
 BEGIN;
     INSERT INTO researchers VALUES (31, 6, 'Bjarne Stroustrup'), (32, 7, 'Bjarne Stroustrup');
+ERROR:  duplicate key value violates unique constraint "avoid_name_confusion_idx"
 ROLLBACK;
 \unset VERBOSITY
 RESET client_min_messages;
 -- verify everyhing including delete is rolled back
 SELECT * FROM researchers WHERE lab_id = 6;
  id | lab_id |         name         
 ----+--------+----------------------
-  9 |      6 | Leslie Lamport
- 17 |      6 |  'Bjarne Stroustrup'
- 18 |      6 |  'Dennis Ritchie'
+ 31 |      6 |  'Bjarne Stroustrup'
+ 30 |      6 |  'Dennis Ritchie'
+ 31 |      6 | Bjarne Stroustrup
 (3 rows)
 
 -- cleanup triggers and the function
 SELECT * from run_command_on_placements('researchers', 'drop trigger reject_large_researcher_id on %s')
 ORDER BY nodeport, shardid;
  nodename | nodeport | shardid | success | result 
 ----------+----------+---------+---------+--------
- localhost |    57637 | 1200000 | t       | DROP TRIGGER
- localhost |    57637 | 1200001 | t       | DROP TRIGGER
- localhost |    57638 | 1200000 | t       | DROP TRIGGER
- localhost |    57638 | 1200001 | t       | DROP TRIGGER
-(4 rows)
+(0 rows)
 
 DROP FUNCTION reject_large_id();
 -- ALTER and copy are compatible
 BEGIN;
 ALTER TABLE labs ADD COLUMN motto text;
 \copy labs from stdin delimiter ','
 ROLLBACK;
 BEGIN;
 \copy labs from stdin delimiter ','
 ALTER TABLE labs ADD COLUMN motto text;
@@ -458,82 +448,79 @@
  12 | fsociety
 (1 row)
 
 -- now, for some special failures...
 CREATE TABLE objects (
 	id bigint PRIMARY KEY,
 	name text NOT NULL
 );
 SET citus.shard_replication_factor TO 2;
 SELECT create_distributed_table('objects', 'id', 'hash', shard_count := 1);
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- test primary key violations
 BEGIN;
 INSERT INTO objects VALUES (1, 'apple');
 INSERT INTO objects VALUES (1, 'orange');
-ERROR:  duplicate key value violates unique constraint "objects_pkey_1200003"
+ERROR:  duplicate key value violates unique constraint "objects_pkey"
 DETAIL:  Key (id)=(1) already exists.
-CONTEXT:  while executing command on localhost:xxxxx
 COMMIT;
 -- data shouldn't have persisted...
 SELECT * FROM objects WHERE id = 1;
  id | name 
 ----+------
 (0 rows)
 
 -- and placements should still be healthy...
 SELECT count(*)
 FROM   pg_dist_shard_placement AS sp,
 	   pg_dist_shard           AS s
 WHERE  sp.shardid = s.shardid
 AND    sp.shardstate = 1
 AND    s.logicalrelid = 'objects'::regclass;
  count 
 -------
-     2
+     0
 (1 row)
 
 -- create trigger on one worker to reject certain values
 \c - - - :worker_2_port
 SET citus.enable_metadata_sync TO OFF;
 CREATE FUNCTION reject_bad() RETURNS trigger AS $rb$
     BEGIN
         IF (NEW.name = 'BAD') THEN
             RAISE 'illegal value';
         END IF;
 
         RETURN NEW;
     END;
 $rb$ LANGUAGE plpgsql;
 RESET citus.enable_metadata_sync;
 CREATE CONSTRAINT TRIGGER reject_bad
 AFTER INSERT ON objects_1200003
 DEFERRABLE INITIALLY IMMEDIATE
 FOR EACH ROW EXECUTE PROCEDURE reject_bad();
+ERROR:  relation "objects_1200003" does not exist
 \c - - - :master_port
 -- test partial failure; worker_1 succeeds, 2 fails
 -- in this case, we expect the transaction to abort
 \set VERBOSITY terse
 BEGIN;
 INSERT INTO objects VALUES (1, 'apple');
 INSERT INTO objects VALUES (2, 'BAD');
-ERROR:  illegal value
 COMMIT;
 -- so the data should noy be persisted
 SELECT * FROM objects WHERE id = 2;
  id | name 
 ----+------
-(0 rows)
+  2 | BAD
+(1 row)
 
 SELECT * FROM labs WHERE id = 7;
  id | name 
 ----+------
 (0 rows)
 
 -- and none of placements should be inactive
 SELECT count(*)
 FROM   pg_dist_shard_placement AS sp,
 	   pg_dist_shard           AS s
@@ -559,75 +546,80 @@
         END IF;
 
         RETURN NEW;
     END;
 $rb$ LANGUAGE plpgsql;
 RESET citus.enable_metadata_sync;
 CREATE CONSTRAINT TRIGGER reject_bad
 AFTER INSERT ON labs_1200002
 DEFERRABLE INITIALLY IMMEDIATE
 FOR EACH ROW EXECUTE PROCEDURE reject_bad();
+ERROR:  relation "labs_1200002" does not exist
 \c - - - :master_port
 BEGIN;
 INSERT INTO objects VALUES (1, 'apple');
 INSERT INTO objects VALUES (2, 'BAD');
-ERROR:  illegal value
 INSERT INTO labs VALUES (8, 'Aperture Science');
-ERROR:  current transaction is aborted, commands ignored until end of transaction block
 INSERT INTO labs VALUES (2, 'BAD');
-ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
 -- data should NOT be persisted
 SELECT * FROM objects WHERE id = 1;
  id | name  
 ----+-------
-(0 rows)
+  1 | apple
+(1 row)
 
 SELECT * FROM labs WHERE id = 8;
  id |       name       
 ----+------------------
-(0 rows)
+  8 | Aperture Science
+(1 row)
 
 -- all placements should remain healthy
 SELECT count(*)
 FROM   pg_dist_shard_placement AS sp,
 	   pg_dist_shard           AS s
 WHERE  sp.shardid = s.shardid
 AND    sp.shardstate = 1
 AND    (s.logicalrelid = 'objects'::regclass OR
 	    s.logicalrelid = 'labs'::regclass);
  count 
 -------
-     3
+     1
 (1 row)
 
 -- what if the failures happen at COMMIT time?
 \c - - - :worker_2_port
 DROP TRIGGER reject_bad ON objects_1200003;
+ERROR:  relation "objects_1200003" does not exist
 CREATE CONSTRAINT TRIGGER reject_bad
 AFTER INSERT ON objects_1200003
 DEFERRABLE INITIALLY DEFERRED
 FOR EACH ROW EXECUTE PROCEDURE reject_bad();
+ERROR:  relation "objects_1200003" does not exist
 \c - - - :master_port
 -- should be the same story as before, just at COMMIT time
 -- as we use 2PC, the transaction is rollbacked
 BEGIN;
 INSERT INTO objects VALUES (1, 'apple');
+ERROR:  duplicate key value violates unique constraint "objects_pkey"
 INSERT INTO objects VALUES (2, 'BAD');
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 INSERT INTO labs VALUES (9, 'Umbrella Corporation');
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
-ERROR:  illegal value
 -- data should not persisted
 SELECT * FROM objects WHERE id = 2;
  id | name 
 ----+------
-(0 rows)
+  2 | BAD
+(1 row)
 
 -- and nonne of the placements should be bad
 SELECT count(*)
 FROM   pg_dist_shard_placement AS sp,
 	   pg_dist_shard           AS s
 WHERE  sp.shardid = s.shardid
 AND    sp.nodename = 'localhost'
 AND    sp.nodeport = :worker_2_port
 AND    sp.shardstate = 3
 AND    s.logicalrelid = 'objects'::regclass;
@@ -638,98 +630,107 @@
 
 DELETE FROM objects;
 -- mark shards as healthy again; delete all data
 UPDATE pg_dist_shard_placement AS sp SET shardstate = 1
 FROM   pg_dist_shard AS s
 WHERE  sp.shardid = s.shardid
 AND    s.logicalrelid = 'objects'::regclass;
 -- what if all nodes have failures at COMMIT time?
 \c - - - :worker_1_port
 DROP TRIGGER reject_bad ON labs_1200002;
+ERROR:  relation "labs_1200002" does not exist
 CREATE CONSTRAINT TRIGGER reject_bad
 AFTER INSERT ON labs_1200002
 DEFERRABLE INITIALLY DEFERRED
 FOR EACH ROW EXECUTE PROCEDURE reject_bad();
+ERROR:  relation "labs_1200002" does not exist
 \c - - - :master_port
 -- reduce the log level for differences between PG14 and PG15
 -- in PGconn->errorMessage
 -- relevant PG commit b15f254466aefbabcbed001929f6e09db59fd158
 SET client_min_messages to ERROR;
 BEGIN;
 INSERT INTO objects VALUES (1, 'apple');
 INSERT INTO objects VALUES (2, 'BAD');
 INSERT INTO labs VALUES (8, 'Aperture Science');
 INSERT INTO labs VALUES (9, 'BAD');
 COMMIT;
-ERROR:  illegal value
 RESET client_min_messages;
 -- data should NOT be persisted
 SELECT * FROM objects WHERE id = 1;
  id | name  
 ----+-------
-(0 rows)
+  1 | apple
+(1 row)
 
 SELECT * FROM labs WHERE id = 8;
  id |       name       
 ----+------------------
-(0 rows)
+  8 | Aperture Science
+  8 | Aperture Science
+(2 rows)
 
 -- all placements should remain healthy
 SELECT count(*)
 FROM   pg_dist_shard_placement AS sp,
 	   pg_dist_shard           AS s
 WHERE  sp.shardid = s.shardid
 AND    sp.shardstate = 1
 AND    (s.logicalrelid = 'objects'::regclass OR
 	    s.logicalrelid = 'labs'::regclass);
  count 
 -------
-     3
+     1
 (1 row)
 
 -- what if one shard (objects) succeeds but another (labs) completely fails?
 \c - - - :worker_2_port
 DROP TRIGGER reject_bad ON objects_1200003;
+ERROR:  relation "objects_1200003" does not exist
 \c - - - :master_port
 SET citus.next_shard_id TO 1200004;
 BEGIN;
 INSERT INTO objects VALUES (1, 'apple');
+ERROR:  duplicate key value violates unique constraint "objects_pkey"
 INSERT INTO labs VALUES (8, 'Aperture Science');
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 INSERT INTO labs VALUES (9, 'BAD');
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COMMIT;
-ERROR:  illegal value
 \set VERBOSITY default
 -- none of the changes should be persisted
 SELECT * FROM objects WHERE id = 1;
  id | name  
 ----+-------
-(0 rows)
+  1 | apple
+(1 row)
 
 SELECT * FROM labs WHERE id = 8;
  id |       name       
 ----+------------------
-(0 rows)
+  8 | Aperture Science
+  8 | Aperture Science
+(2 rows)
 
 -- all placements should be healthy
 SELECT   s.logicalrelid::regclass::text, sp.shardstate, count(*)
 FROM     pg_dist_shard_placement AS sp,
 	     pg_dist_shard           AS s
 WHERE    sp.shardid = s.shardid
 AND      (s.logicalrelid = 'objects'::regclass OR
 	      s.logicalrelid = 'labs'::regclass)
 GROUP BY s.logicalrelid, sp.shardstate
 ORDER BY s.logicalrelid, sp.shardstate;
  logicalrelid | shardstate | count 
 --------------+------------+-------
  labs         |          1 |     1
- objects      |          1 |     2
-(2 rows)
+(1 row)
 
 -- some append-partitioned tests for good measure
 CREATE TABLE append_researchers ( LIKE researchers );
 SELECT create_distributed_table('append_researchers', 'id', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SET citus.shard_replication_factor TO 1;
@@ -841,63 +842,63 @@
         END IF;
 
         RETURN NEW;
     END;
 $rb$ LANGUAGE plpgsql;
 RESET citus.enable_metadata_sync;
 CREATE CONSTRAINT TRIGGER reject_bad_reference
 AFTER INSERT ON reference_modifying_xacts_1200006
 DEFERRABLE INITIALLY IMMEDIATE
 FOR EACH ROW EXECUTE PROCEDURE reject_bad_reference();
+ERROR:  relation "reference_modifying_xacts_1200006" does not exist
 \c - - - :master_port
 \set VERBOSITY terse
 -- try without wrapping inside a transaction
 INSERT INTO reference_modifying_xacts VALUES (999, 3);
-ERROR:  illegal value
 -- same test within a transaction
 BEGIN;
 INSERT INTO reference_modifying_xacts VALUES (999, 3);
-ERROR:  illegal value
 COMMIT;
 -- lets fail one of the workers at COMMIT time
 \c - - - :worker_1_port
 DROP TRIGGER reject_bad_reference ON reference_modifying_xacts_1200006;
+ERROR:  relation "reference_modifying_xacts_1200006" does not exist
 CREATE CONSTRAINT TRIGGER reject_bad_reference
 AFTER INSERT ON reference_modifying_xacts_1200006
 DEFERRABLE INITIALLY  DEFERRED
 FOR EACH ROW EXECUTE PROCEDURE reject_bad_reference();
+ERROR:  relation "reference_modifying_xacts_1200006" does not exist
 \c - - - :master_port
 \set VERBOSITY terse
 -- try without wrapping inside a transaction
 INSERT INTO reference_modifying_xacts VALUES (999, 3);
-ERROR:  illegal value
 -- same test within a transaction
 BEGIN;
 INSERT INTO reference_modifying_xacts VALUES (999, 3);
 COMMIT;
-ERROR:  illegal value
 -- all placements should be healthy
 SELECT   s.logicalrelid::regclass::text, sp.shardstate, count(*)
 FROM     pg_dist_shard_placement AS sp,
 	     pg_dist_shard           AS s
 WHERE    sp.shardid = s.shardid
 AND      s.logicalrelid = 'reference_modifying_xacts'::regclass
 GROUP BY s.logicalrelid, sp.shardstate
 ORDER BY s.logicalrelid, sp.shardstate;
        logicalrelid        | shardstate | count 
 ---------------------------+------------+-------
- reference_modifying_xacts |          1 |     2
+ reference_modifying_xacts |          1 |     1
 (1 row)
 
 -- for the time-being drop the constraint
 \c - - - :worker_1_port
 DROP TRIGGER reject_bad_reference ON reference_modifying_xacts_1200006;
+ERROR:  relation "reference_modifying_xacts_1200006" does not exist
 \c - - - :master_port
 -- now create a hash distributed table and run tests
 -- including both the reference table and the hash
 -- distributed table
 -- To prevent colocating a hash table with append table
 DELETE FROM pg_dist_colocation WHERE colocationid = 100001;
 SET citus.next_shard_id TO 1200007;
 SET citus.shard_count = 4;
 SET citus.shard_replication_factor = 1;
 CREATE TABLE hash_modifying_xacts (key int, value int);
@@ -931,620 +932,223 @@
         END IF;
 
         RETURN NEW;
     END;
 $rb$ LANGUAGE plpgsql;
 RESET citus.enable_metadata_sync;
 CREATE CONSTRAINT TRIGGER reject_bad_hash
 AFTER INSERT ON hash_modifying_xacts_1200007
 DEFERRABLE INITIALLY IMMEDIATE
 FOR EACH ROW EXECUTE PROCEDURE reject_bad_hash();
+ERROR:  relation "hash_modifying_xacts_1200007" does not exist
 \c - - - :master_port
 \set VERBOSITY terse
 -- the transaction as a whole should fail
 BEGIN;
 INSERT INTO reference_modifying_xacts VALUES (55, 10);
 INSERT INTO hash_modifying_xacts VALUES (997, 1);
-ERROR:  illegal value
 COMMIT;
 -- ensure that the value didn't go into the reference table
 SELECT * FROM reference_modifying_xacts WHERE key = 55;
  key | value 
 -----+-------
-(0 rows)
+  55 |    10
+(1 row)
 
 -- now lets fail on of the workers for the hash distributed table table
 -- when there is a reference table involved
 \c - - - :worker_1_port
 DROP TRIGGER reject_bad_hash ON hash_modifying_xacts_1200007;
+ERROR:  relation "hash_modifying_xacts_1200007" does not exist
 -- the trigger is on execution time
 CREATE CONSTRAINT TRIGGER reject_bad_hash
 AFTER INSERT ON hash_modifying_xacts_1200007
 DEFERRABLE INITIALLY DEFERRED
 FOR EACH ROW EXECUTE PROCEDURE reject_bad_hash();
+ERROR:  relation "hash_modifying_xacts_1200007" does not exist
 \c - - - :master_port
 \set VERBOSITY terse
 -- the transaction as a whole should fail
 BEGIN;
 INSERT INTO reference_modifying_xacts VALUES (12, 12);
 INSERT INTO hash_modifying_xacts VALUES (997, 1);
 COMMIT;
-ERROR:  illegal value
 -- ensure that the values didn't go into the reference table
 SELECT * FROM reference_modifying_xacts WHERE key = 12;
  key | value 
 -----+-------
-(0 rows)
+  12 |    12
+(1 row)
 
 -- all placements should be healthy
 SELECT   s.logicalrelid::regclass::text, sp.shardstate, count(*)
 FROM     pg_dist_shard_placement AS sp,
 	     pg_dist_shard           AS s
 WHERE    sp.shardid = s.shardid
 AND      (s.logicalrelid = 'reference_modifying_xacts'::regclass OR
 		  s.logicalrelid = 'hash_modifying_xacts'::regclass)
 GROUP BY s.logicalrelid, sp.shardstate
 ORDER BY s.logicalrelid, sp.shardstate;
        logicalrelid        | shardstate | count 
 ---------------------------+------------+-------
- reference_modifying_xacts |          1 |     2
+ reference_modifying_xacts |          1 |     1
  hash_modifying_xacts      |          1 |     4
 (2 rows)
 
 -- now, fail the insert on reference table
 -- and ensure that hash distributed table's
 -- change is rollbacked as well
 \c - - - :worker_1_port
 CREATE CONSTRAINT TRIGGER reject_bad_reference
 AFTER INSERT ON reference_modifying_xacts_1200006
 DEFERRABLE INITIALLY IMMEDIATE
 FOR EACH ROW EXECUTE PROCEDURE reject_bad_reference();
+ERROR:  relation "reference_modifying_xacts_1200006" does not exist
 \c - - - :master_port
 \set VERBOSITY terse
 BEGIN;
 -- to expand participant to include all worker nodes
 INSERT INTO reference_modifying_xacts VALUES (66, 3);
 INSERT INTO hash_modifying_xacts VALUES (80, 1);
 INSERT INTO reference_modifying_xacts VALUES (999, 3);
-ERROR:  illegal value
 COMMIT;
 SELECT * FROM hash_modifying_xacts WHERE key = 80;
  key | value 
 -----+-------
-(0 rows)
+  80 |     1
+(1 row)
 
 SELECT * FROM reference_modifying_xacts WHERE key = 66;
  key | value 
 -----+-------
-(0 rows)
+  66 |     3
+(1 row)
 
 SELECT * FROM reference_modifying_xacts WHERE key = 999;
  key | value 
 -----+-------
-(0 rows)
+ 999 |     3
+ 999 |     3
+ 999 |     3
+ 999 |     3
+ 999 |     3
+(5 rows)
 
 -- all placements should be healthy
 SELECT   s.logicalrelid::regclass::text, sp.shardstate, count(*)
 FROM     pg_dist_shard_placement AS sp,
 	     pg_dist_shard           AS s
 WHERE    sp.shardid = s.shardid
 AND      (s.logicalrelid = 'reference_modifying_xacts'::regclass OR
 		  s.logicalrelid = 'hash_modifying_xacts'::regclass)
 GROUP BY s.logicalrelid, sp.shardstate
 ORDER BY s.logicalrelid, sp.shardstate;
        logicalrelid        | shardstate | count 
 ---------------------------+------------+-------
- reference_modifying_xacts |          1 |     2
+ reference_modifying_xacts |          1 |     1
  hash_modifying_xacts      |          1 |     4
 (2 rows)
 
 -- now show that all modifications to reference
 -- tables are done in 2PC
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 INSERT INTO reference_modifying_xacts VALUES (70, 70);
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     2
+     0
 (1 row)
 
 -- reset the transactions table
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 BEGIN;
 INSERT INTO reference_modifying_xacts VALUES (71, 71);
 COMMIT;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     2
+     0
 (1 row)
 
 -- create a hash distributed tablw which spans all nodes
 SET citus.shard_count = 4;
 SET citus.shard_replication_factor = 2;
 CREATE TABLE hash_modifying_xacts_second (key int, value int);
 SELECT create_distributed_table('hash_modifying_xacts_second', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 -- reset the transactions table
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 BEGIN;
 INSERT INTO hash_modifying_xacts_second VALUES (72, 1);
 INSERT INTO reference_modifying_xacts VALUES (72, 3);
 COMMIT;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     2
+     0
 (1 row)
 
 -- reset the transactions table
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 DELETE FROM reference_modifying_xacts;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     2
+     0
 (1 row)
 
 -- reset the transactions table
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 UPDATE reference_modifying_xacts SET key = 10;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     2
+     0
 (1 row)
 
 -- now to one more type of failure testing
 -- in which we'll make the remote host unavailable
 -- first create the new user on all nodes
 CREATE USER test_user;
 -- now connect back to the master with the new user
 \c - test_user - :master_port
 SET citus.next_shard_id TO 1200015;
 CREATE TABLE reference_failure_test (key int, value int);
 SELECT create_reference_table('reference_failure_test');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 -- create a hash distributed table
 SET citus.shard_count TO 4;
 CREATE TABLE numbers_hash_failure_test(key int, value int);
 SELECT create_distributed_table('numbers_hash_failure_test', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
 -- ensure that the shard is created for this user
 \c - test_user - :worker_1_port
-SET citus.override_table_visibility TO false;
-\dt reference_failure_test_1200015
-                      List of relations
- Schema |              Name              | Type  |   Owner
----------------------------------------------------------------------
- public | reference_failure_test_1200015 | table | test_user
-(1 row)
-
--- now connect with the default user,
--- and rename the existing user
-\c - :default_user - :worker_1_port
-ALTER USER test_user RENAME TO test_user_new;
-NOTICE:  not propagating ALTER ROLE ... RENAME TO commands to worker nodes
--- connect back to master and query the reference table
- \c - test_user - :master_port
--- should fail since the worker doesn't have test_user anymore
-INSERT INTO reference_failure_test VALUES (1, '1');
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" does not exist
--- the same as the above, but wrapped within a transaction
-BEGIN;
-INSERT INTO reference_failure_test VALUES (1, '1');
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" does not exist
-COMMIT;
-BEGIN;
-COPY reference_failure_test FROM STDIN WITH (FORMAT 'csv');
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" does not exist
-COMMIT;
--- show that no data go through the table and shard states are good
-SET client_min_messages to 'ERROR';
-SELECT * FROM reference_failure_test;
- key | value
----------------------------------------------------------------------
-(0 rows)
-
-RESET client_min_messages;
--- all placements should be healthy
-SELECT   s.logicalrelid::regclass::text, sp.shardstate, count(*)
-FROM     pg_dist_shard_placement AS sp,
-	     pg_dist_shard           AS s
-WHERE    sp.shardid = s.shardid
-AND      s.logicalrelid = 'reference_failure_test'::regclass
-GROUP BY s.logicalrelid, sp.shardstate
-ORDER BY s.logicalrelid, sp.shardstate;
-      logicalrelid      | shardstate | count
----------------------------------------------------------------------
- reference_failure_test |          1 |     2
-(1 row)
-
--- any failure rollbacks the transaction
-BEGIN;
-COPY numbers_hash_failure_test FROM STDIN WITH (FORMAT 'csv');
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" does not exist
-ABORT;
--- none of placements are invalid after abort
-SELECT shardid, shardstate, nodename, nodeport
-FROM pg_dist_shard_placement JOIN pg_dist_shard USING (shardid)
-WHERE logicalrelid = 'numbers_hash_failure_test'::regclass
-ORDER BY shardid, nodeport;
- shardid | shardstate | nodename  | nodeport
----------------------------------------------------------------------
- 1200016 |          1 | localhost |    57637
- 1200016 |          1 | localhost |    57638
- 1200017 |          1 | localhost |    57637
- 1200017 |          1 | localhost |    57638
- 1200018 |          1 | localhost |    57637
- 1200018 |          1 | localhost |    57638
- 1200019 |          1 | localhost |    57637
- 1200019 |          1 | localhost |    57638
-(8 rows)
-
--- verify nothing is inserted
-SELECT count(*) FROM numbers_hash_failure_test;
-WARNING:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" does not exist
-WARNING:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" does not exist
- count
----------------------------------------------------------------------
-     0
-(1 row)
-
--- all placements to be market valid
-SELECT shardid, shardstate, nodename, nodeport
-FROM pg_dist_shard_placement JOIN pg_dist_shard USING (shardid)
-WHERE logicalrelid = 'numbers_hash_failure_test'::regclass
-ORDER BY shardid, nodeport;
- shardid | shardstate | nodename  | nodeport
----------------------------------------------------------------------
- 1200016 |          1 | localhost |    57637
- 1200016 |          1 | localhost |    57638
- 1200017 |          1 | localhost |    57637
- 1200017 |          1 | localhost |    57638
- 1200018 |          1 | localhost |    57637
- 1200018 |          1 | localhost |    57638
- 1200019 |          1 | localhost |    57637
- 1200019 |          1 | localhost |    57638
-(8 rows)
-
--- all failures roll back the transaction
-BEGIN;
-COPY numbers_hash_failure_test FROM STDIN WITH (FORMAT 'csv');
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" does not exist
-COMMIT;
--- expect none of the placements to be market invalid after commit
-SELECT shardid, shardstate, nodename, nodeport
-FROM pg_dist_shard_placement JOIN pg_dist_shard USING (shardid)
-WHERE logicalrelid = 'numbers_hash_failure_test'::regclass
-ORDER BY shardid, nodeport;
- shardid | shardstate | nodename  | nodeport
----------------------------------------------------------------------
- 1200016 |          1 | localhost |    57637
- 1200016 |          1 | localhost |    57638
- 1200017 |          1 | localhost |    57637
- 1200017 |          1 | localhost |    57638
- 1200018 |          1 | localhost |    57637
- 1200018 |          1 | localhost |    57638
- 1200019 |          1 | localhost |    57637
- 1200019 |          1 | localhost |    57638
-(8 rows)
-
--- verify no data is inserted
-SELECT count(*) FROM numbers_hash_failure_test;
-WARNING:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" does not exist
-WARNING:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" does not exist
- count
----------------------------------------------------------------------
-     0
-(1 row)
-
--- break the other node as well
-\c - :default_user - :worker_2_port
-ALTER USER test_user RENAME TO test_user_new;
-NOTICE:  not propagating ALTER ROLE ... RENAME TO commands to worker nodes
-\c - test_user - :master_port
--- fails on all shard placements
-INSERT INTO numbers_hash_failure_test VALUES (2,2);
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" does not exist
--- connect back to the master with the proper user to continue the tests
-\c - :default_user - :master_port
-SET citus.next_shard_id TO 1200020;
-SET citus.next_placement_id TO 1200033;
--- unbreak both nodes by renaming the user back to the original name
-SELECT * FROM run_command_on_workers('ALTER USER test_user_new RENAME TO test_user');
- nodename  | nodeport | success |   result
----------------------------------------------------------------------
- localhost |    57637 | t       | ALTER ROLE
- localhost |    57638 | t       | ALTER ROLE
-(2 rows)
-
-DROP TABLE reference_modifying_xacts, hash_modifying_xacts, hash_modifying_xacts_second,
-	reference_failure_test, numbers_hash_failure_test;
-DROP USER test_user;
--- set up foreign keys to test transactions with co-located and reference tables
-BEGIN;
-SET LOCAL citus.shard_replication_factor TO 1;
-SET LOCAL citus.shard_count TO 4;
-CREATE TABLE usergroups (
-    gid int PRIMARY KEY,
-    name text
-);
-SELECT create_reference_table('usergroups');
- create_reference_table
----------------------------------------------------------------------
-
-(1 row)
-
-CREATE TABLE itemgroups (
-    gid int PRIMARY KEY,
-    name text
-);
-SELECT create_reference_table('itemgroups');
- create_reference_table
----------------------------------------------------------------------
-
-(1 row)
-
-DROP TABLE IF EXISTS users ;
-CREATE TABLE users (
-    id int PRIMARY KEY,
-    name text,
-    user_group int
-);
-SELECT create_distributed_table('users', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
-CREATE TABLE items (
-    user_id int REFERENCES users (id) ON DELETE CASCADE,
-	item_name text,
-    item_group int
-);
-SELECT create_distributed_table('items', 'user_id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
--- Table to find values that live in different shards on the same node
-SELECT id, shard_name('users', shardid), nodename, nodeport
-FROM
-  pg_dist_shard_placement
-JOIN
-  ( SELECT id, get_shard_id_for_distribution_column('users', id) shardid FROM generate_series(1,10) id ) ids
-USING (shardid)
-ORDER BY
-  id;
- id |  shard_name   | nodename  | nodeport
----------------------------------------------------------------------
-  1 | users_1200022 | localhost |    57637
-  2 | users_1200025 | localhost |    57638
-  3 | users_1200023 | localhost |    57638
-  4 | users_1200023 | localhost |    57638
-  5 | users_1200022 | localhost |    57637
-  6 | users_1200024 | localhost |    57637
-  7 | users_1200023 | localhost |    57638
-  8 | users_1200022 | localhost |    57637
-  9 | users_1200025 | localhost |    57638
- 10 | users_1200022 | localhost |    57637
-(10 rows)
-
-END;
--- the INSERTs into items should see the users
-BEGIN;
-\COPY users FROM STDIN WITH CSV
-INSERT INTO items VALUES (1, 'item-1');
-INSERT INTO items VALUES (6, 'item-6');
-END;
-SELECT user_id FROM items ORDER BY user_id;
- user_id
----------------------------------------------------------------------
-       1
-       6
-(2 rows)
-
--- should be able to open multiple connections per node after INSERTing over one connection
-BEGIN;
-INSERT INTO users VALUES (2, 'burak');
-INSERT INTO users VALUES (3, 'burak');
-\COPY items FROM STDIN WITH CSV
-ROLLBACK;
--- perform parallel DDL after a co-located table has been read over 1 connection
-BEGIN;
-SELECT id FROM users WHERE id = 1;
- id
----------------------------------------------------------------------
-  1
-(1 row)
-
-SELECT id FROM users WHERE id = 6;
- id
----------------------------------------------------------------------
-  6
-(1 row)
-
-ALTER TABLE items ADD COLUMN last_update timestamptz;
-ROLLBACK;
--- can perform sequential DDL after a co-located table has been read over 1 connection
-BEGIN;
-SET LOCAL citus.multi_shard_modify_mode TO 'sequential';
-SELECT id FROM users WHERE id = 1;
- id
----------------------------------------------------------------------
-  1
-(1 row)
-
-SELECT id FROM users WHERE id = 6;
- id
----------------------------------------------------------------------
-  6
-(1 row)
-
-ALTER TABLE items ADD COLUMN last_update timestamptz;
-ROLLBACK;
--- and the other way around is also fine
-BEGIN;
-ALTER TABLE items ADD COLUMN last_update timestamptz;
-SELECT id FROM users JOIN items ON (id = user_id) WHERE id = 1;
- id
----------------------------------------------------------------------
-  1
-(1 row)
-
-SELECT id FROM users JOIN items ON (id = user_id) WHERE id = 6;
- id
----------------------------------------------------------------------
-  6
-(1 row)
-
-END;
-BEGIN;
--- establish multiple connections to a node
-\COPY users FROM STDIN WITH CSV
--- now read from the reference table over each connection
-SELECT user_id FROM items JOIN itemgroups ON (item_group = gid) WHERE user_id = 2;
- user_id
----------------------------------------------------------------------
-(0 rows)
-
-SELECT user_id FROM items JOIN itemgroups ON (item_group = gid) WHERE user_id = 3;
- user_id
----------------------------------------------------------------------
-(0 rows)
-
--- perform a DDL command on the reference table errors
--- because the current implementation of COPY always opens one connection
--- per placement SELECTs have to use those connections for correctness
-ALTER TABLE itemgroups ADD COLUMN last_update timestamptz;
-ERROR:  cannot perform DDL on placement xxxxx, which has been read over multiple connections
-END;
-BEGIN;
--- establish multiple connections to a node
-\COPY users FROM STDIN WITH CSV
--- read from the reference table over each connection
-SELECT user_id FROM items JOIN itemgroups ON (item_group = gid) WHERE user_id = 2;
- user_id
----------------------------------------------------------------------
-(0 rows)
-
-SELECT user_id FROM items JOIN itemgroups ON (item_group = gid) WHERE user_id = 3;
- user_id
----------------------------------------------------------------------
-(0 rows)
-
--- perform a DDL command on a co-located reference table
-ALTER TABLE usergroups ADD COLUMN last_update timestamptz;
-ERROR:  cannot perform DDL on placement xxxxx since a co-located placement has been read over multiple connections
-END;
-BEGIN;
--- make a modification over connection 1
-INSERT INTO usergroups VALUES (0,'istanbul');
--- copy over connections 1 and 2
-\COPY users FROM STDIN WITH CSV
--- cannot read modifications made over different connections
-SELECT id FROM users JOIN usergroups ON (gid = user_group) WHERE id = 3;
-ERROR:  cannot perform query with placements that were modified over multiple connections
-END;
--- make sure we can see cascading deletes
-BEGIN;
-DELETE FROM users;
-SELECT user_id FROM items JOIN itemgroups ON (item_group = gid) WHERE user_id = 1;
- user_id
----------------------------------------------------------------------
-(0 rows)
-
-SELECT user_id FROM items JOIN itemgroups ON (item_group = gid) WHERE user_id = 6;
- user_id
----------------------------------------------------------------------
-(0 rows)
-
-END;
--- test visibility after COPY
-INSERT INTO usergroups VALUES (2,'group');
-BEGIN;
--- opens two separate connections to node
-\COPY users FROM STDIN WITH CSV
--- Uses first connection, which wrote the row with id = 2
-SELECT * FROM users JOIN usergroups ON (user_group = gid) WHERE id = 2;
- id | name  | user_group | gid | name
----------------------------------------------------------------------
-  2 | onder |          2 |   2 | group
-(1 row)
-
--- Should use second connection, which wrote the row with id = 4
-SELECT * FROM users JOIN usergroups ON (user_group = gid) WHERE id = 4;
- id | name  | user_group | gid | name
----------------------------------------------------------------------
-  4 | murat |          2 |   2 | group
-(1 row)
-
-END;
--- make sure functions that throw an error roll back propertly
-CREATE FUNCTION insert_abort()
-RETURNS bool
-AS $BODY$
-BEGIN
-  INSERT INTO labs VALUES (1001, 'Abort Labs');
-  UPDATE labs SET name = 'Rollback Labs' WHERE id = 1001;
-  RAISE 'do not insert';
-END;
-$BODY$ LANGUAGE plpgsql;
-SELECT insert_abort();
-ERROR:  do not insert
-SELECT name FROM labs WHERE id = 1001;
- name
----------------------------------------------------------------------
-(0 rows)
-
--- if function_opens_transaction-block is disabled the insert commits immediately
-SET citus.function_opens_transaction_block TO off;
-SELECT insert_abort();
-ERROR:  do not insert
-SELECT name FROM labs WHERE id = 1001;
-     name
----------------------------------------------------------------------
- Rollback Labs
-(1 row)
-
-RESET citus.function_opens_transaction_block;
-DROP FUNCTION insert_abort();
-DROP TABLE items, users, itemgroups, usergroups, researchers, labs;
+\connect: connection to server at "localhost" (127.0.0.1), port 57637 failed: FATAL:  role "test_user" does not exist
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_generate_ddl_commands.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_generate_ddl_commands.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_generate_ddl_commands.out.modified	2022-11-09 13:38:50.529312304 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_generate_ddl_commands.out.modified	2022-11-09 13:38:50.529312304 +0300
@@ -54,42 +54,36 @@
  ALTER TABLE public.table_constraint_table OWNER TO postgres
 (2 rows)
 
 -- tables with "simple" CHECK constraints should be able to be distributed
 CREATE TABLE check_constraint_table_1(
 	id int,
 	b boolean,
 	CHECK(b)
 );
 SELECT create_distributed_table('check_constraint_table_1', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT master_get_table_ddl_events('check_constraint_table_1');
                                                       master_get_table_ddl_events                                                       
 ----------------------------------------------------------------------------------------------------------------------------------------
  CREATE TABLE public.check_constraint_table_1 (id integer, b boolean, CONSTRAINT check_constraint_table_1_b_check CHECK (b)) USING heap
  ALTER TABLE public.check_constraint_table_1 OWNER TO postgres
 (2 rows)
 
 -- including hardcoded Booleans
 CREATE TABLE check_constraint_table_2(
 	id int CHECK(true)
 );
 SELECT create_distributed_table('check_constraint_table_2', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT master_get_table_ddl_events('check_constraint_table_2');
                                                  master_get_table_ddl_events                                                  
 ------------------------------------------------------------------------------------------------------------------------------
  CREATE TABLE public.check_constraint_table_2 (id integer, CONSTRAINT check_constraint_table_2_check CHECK (true)) USING heap
  ALTER TABLE public.check_constraint_table_2 OWNER TO postgres
 (2 rows)
 
 -- default values are supported
 CREATE TABLE default_value_table (
 	name text,
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_create_shards.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_create_shards.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_create_shards.out.modified	2022-11-09 13:38:50.669312303 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_create_shards.out.modified	2022-11-09 13:38:50.679312303 +0300
@@ -61,63 +61,43 @@
 DETAIL:  Partition column types must have a hash function defined to use hash partitioning.
 -- use a bad shard count
 SELECT create_distributed_table('table_to_distribute', 'name', 'hash', shard_count := 0);
 ERROR:  0 is outside the valid range for parameter "shard_count" (1 .. 64000)
 -- use a bad replication factor
 SET citus.shard_replication_factor TO 0;
 ERROR:  0 is outside the valid range for parameter "citus.shard_replication_factor" (1 .. 100)
 -- use a replication factor higher than shard count
 SET citus.shard_replication_factor TO 3;
 SELECT create_distributed_table('table_to_distribute', 'name', 'hash');
-ERROR:  replication_factor (3) exceeds number of worker nodes (2)
+ERROR:  replication_factor (3) exceeds number of worker nodes (1)
 HINT:  Add more worker nodes or try again with a lower replication factor.
 RESET citus.shard_replication_factor;
 -- finally, create shards and inspect metadata
 SELECT create_distributed_table('table_to_distribute', 'name', 'hash', shard_count := 16);
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT shardstorage, shardminvalue, shardmaxvalue FROM pg_dist_shard
 	WHERE logicalrelid = 'table_to_distribute'::regclass
 	ORDER BY (shardminvalue::integer) ASC;
  shardstorage | shardminvalue | shardmaxvalue 
 --------------+---------------+---------------
- t            | -2147483648   | -1879048193
- t            | -1879048192   | -1610612737
- t            | -1610612736   | -1342177281
- t            | -1342177280   | -1073741825
- t            | -1073741824   | -805306369
- t            | -805306368    | -536870913
- t            | -536870912    | -268435457
- t            | -268435456    | -1
- t            | 0             | 268435455
- t            | 268435456     | 536870911
- t            | 536870912     | 805306367
- t            | 805306368     | 1073741823
- t            | 1073741824    | 1342177279
- t            | 1342177280    | 1610612735
- t            | 1610612736    | 1879048191
- t            | 1879048192    | 2147483647
-(16 rows)
+(0 rows)
 
 -- all shards should have the same size (16 divides evenly into the hash space)
 SELECT count(*) AS shard_count,
 	shardmaxvalue::integer - shardminvalue::integer AS shard_size
 	FROM pg_dist_shard
 	WHERE logicalrelid='table_to_distribute'::regclass
 	GROUP BY shard_size;
  shard_count | shard_size 
 -------------+------------
-          16 |  268435455
-(1 row)
+(0 rows)
 
 SELECT COUNT(*) FROM pg_class WHERE relname LIKE 'table_to_distribute%' AND relkind = 'r';
  count 
 -------
      1
 (1 row)
 
 -- test list sorting
 SELECT sort_names('sumedh', 'jason', 'ozgun');
  sort_names 
@@ -135,31 +115,21 @@
 (1 row)
 
 -- test shard creation using weird shard count
 CREATE TABLE weird_shard_count
 (
 	name text,
 	id bigint
 );
 SET citus.shard_count TO 7;
 SELECT create_distributed_table('weird_shard_count', 'id', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- Citus ensures all shards are roughly the same size
 SELECT shardmaxvalue::integer - shardminvalue::integer AS shard_size
 	FROM pg_dist_shard
 	WHERE logicalrelid = 'weird_shard_count'::regclass
 	ORDER BY shardminvalue::integer ASC;
  shard_size 
 ------------
-  613566755
-  613566755
-  613566755
-  613566755
-  613566755
-  613566755
-  613566759
-(7 rows)
+(0 rows)
 
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_transaction_recovery.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_transaction_recovery.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_transaction_recovery.out.modified	2022-11-09 13:38:52.509312296 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_transaction_recovery.out.modified	2022-11-09 13:38:52.519312296 +0300
@@ -52,27 +52,27 @@
 PREPARE TRANSACTION 'citus_0_should_be_sorted_into_middle';
 SET citus.force_max_query_parallelization TO ON;
 -- Add "fake" pg_dist_transaction records and run recovery
 INSERT INTO pg_dist_transaction VALUES (1, 'citus_0_should_commit'),
                                        (0, 'citus_0_should_commit');
 INSERT INTO pg_dist_transaction VALUES (1, 'citus_0_should_be_forgotten'),
                                        (0, 'citus_0_should_be_forgotten');
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
-                             6
+                             3
 (1 row)
 
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     0
+     2
 (1 row)
 
 SELECT count(*) FROM pg_tables WHERE tablename = 'should_abort';
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM pg_tables WHERE tablename = 'should_commit';
  count 
@@ -84,38 +84,35 @@
 \c - - - :worker_1_port
 SELECT count(*) FROM pg_tables WHERE tablename = 'should_abort';
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM pg_tables WHERE tablename = 'should_commit';
  count 
 -------
-     1
+     0
 (1 row)
 
 \c - - - :master_port
 SET citus.force_max_query_parallelization TO ON;
 SET citus.shard_replication_factor TO 2;
 SET citus.shard_count TO 2;
 -- create_distributed_table may behave differently if shards
 -- created via the executor or not, so not checking its value
 -- may result multiple test outputs, so instead just make sure that
 -- there are at least 2 entries
 CREATE TABLE test_recovery (x text);
 SELECT create_distributed_table('test_recovery', 'x');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT count(*) >= 2 FROM pg_dist_transaction;
  ?column? 
 ----------
  t
 (1 row)
 
 -- create_reference_table should add another 2 recovery records
 CREATE TABLE test_recovery_ref (x text);
 SELECT create_reference_table('test_recovery_ref');
  create_reference_table 
@@ -149,87 +146,87 @@
                              0
 (1 row)
 
 -- Aborted DDL commands should not write transaction recovery records
 BEGIN;
 ALTER TABLE test_recovery ADD COLUMN y text;
 ROLLBACK;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     0
+     2
 (1 row)
 
 -- Committed DDL commands should write 4 transaction recovery records
 ALTER TABLE test_recovery ADD COLUMN y text;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     4
+     2
 (1 row)
 
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     0
+     2
 (1 row)
 
 -- Aborted INSERT..SELECT should not write transaction recovery records
 BEGIN;
 INSERT INTO test_recovery SELECT x, 'earth' FROM test_recovery;
 ROLLBACK;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     0
+     2
 (1 row)
 
 -- Committed INSERT..SELECT should write 4 transaction recovery records
 INSERT INTO test_recovery SELECT x, 'earth' FROM test_recovery;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     4
+     2
 (1 row)
 
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 -- Committed INSERT..SELECT via coordinator should write 4 transaction recovery records
 INSERT INTO test_recovery (x) SELECT 'hello-'||s FROM generate_series(1,100) s;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     4
+     2
 (1 row)
 
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 -- Committed COPY should write 4 transaction records
 COPY test_recovery (x) FROM STDIN CSV;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     4
+     2
 (1 row)
 
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 -- Create a single-replica table to enable 2PC in multi-statement transactions
 SET citus.shard_replication_factor TO 1;
@@ -352,21 +349,21 @@
                              0
 (1 row)
 
 SELECT shardid INTO selected_shard
 FROM citus_shards
 WHERE table_name='test_2pcskip'::regclass AND nodeport = :worker_1_port
 LIMIT 1;
 SELECT COUNT(*) FROM pg_dist_transaction;
  count 
 -------
-     0
+     2
 (1 row)
 
 BEGIN;
 SELECT citus_move_shard_placement((SELECT * FROM selected_shard), 'localhost', :worker_1_port, 'localhost', :worker_2_port, shard_transfer_mode := 'block_writes');
  citus_move_shard_placement 
 ----------------------------
  
 (1 row)
 
 COMMIT;
@@ -385,21 +382,21 @@
 SELECT citus_move_shard_placement((SELECT * FROM selected_shard), 'localhost', :worker_2_port, 'localhost', :worker_1_port, shard_transfer_mode := 'block_writes');
  citus_move_shard_placement 
 ----------------------------
  
 (1 row)
 
 -- for the following test, ensure that 6 and 7 go to different shards on different workers
 SELECT count(DISTINCT nodeport) FROM pg_dist_shard_placement WHERE shardid IN (get_shard_id_for_distribution_column('test_2pcskip', 6),get_shard_id_for_distribution_column('test_2pcskip', 7));
  count 
 -------
-     2
+     1
 (1 row)
 
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 -- only two of the connections will perform a write (INSERT)
 SET citus.force_max_query_parallelization TO ON;
@@ -435,21 +432,21 @@
 SELECT COUNT(*) FROM test_2pcskip;
  count 
 -------
     10
 (1 row)
 
 COMMIT;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     2
+     4
 (1 row)
 
 -- check that reads from a reference table don't trigger 2PC
 -- despite repmodel being 2PC
 CREATE TABLE test_reference (b int);
 SELECT create_reference_table('test_reference');
  create_reference_table 
 ------------------------
  
 (1 row)
@@ -467,21 +464,21 @@
  b 
 ---
  1
  2
 (2 rows)
 
 COMMIT;
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     0
+     2
 (1 row)
 
 SELECT recover_prepared_transactions();
  recover_prepared_transactions 
 -------------------------------
                              0
 (1 row)
 
 -- Test whether auto-recovery runs
 ALTER SYSTEM SET citus.recover_2pc_interval TO 10;
@@ -494,21 +491,21 @@
 -- Sleep 1 second to give Valgrind enough time to clear transactions
 SELECT pg_sleep(1);
  pg_sleep 
 ----------
  
 (1 row)
 
 SELECT count(*) FROM pg_dist_transaction;
  count 
 -------
-     0
+     2
 (1 row)
 
 ALTER SYSTEM RESET citus.recover_2pc_interval;
 SELECT pg_reload_conf();
  pg_reload_conf 
 ----------------
  t
 (1 row)
 
 DROP TABLE test_recovery_ref;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/local_dist_join_modifications.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/local_dist_join_modifications.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/local_dist_join_modifications.out.modified	2022-11-09 13:38:52.739312295 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/local_dist_join_modifications.out.modified	2022-11-09 13:38:52.749312295 +0300
@@ -3,103 +3,75 @@
 CREATE TABLE postgres_table (key int, value text, value_2 jsonb);
 CREATE TABLE reference_table (key int, value text, value_2 jsonb);
 SELECT create_reference_table('reference_table');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 CREATE TABLE distributed_table (key int, value text, value_2 jsonb);
 SELECT create_distributed_table('distributed_table', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE distributed_table_pkey (key int primary key, value text, value_2 jsonb);
 SELECT create_distributed_table('distributed_table_pkey', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE distributed_table_windex (key int primary key, value text, value_2 jsonb);
 SELECT create_distributed_table('distributed_table_windex', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE UNIQUE INDEX key_index ON distributed_table_windex (key);
 CREATE TABLE distributed_partitioned_table(key int, value text) PARTITION BY RANGE (key);
 CREATE TABLE distributed_partitioned_table_1 PARTITION OF distributed_partitioned_table FOR VALUES FROM (0) TO (50);
 CREATE TABLE distributed_partitioned_table_2 PARTITION OF distributed_partitioned_table FOR VALUES FROM (50) TO (200);
 SELECT create_distributed_table('distributed_partitioned_table', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE local_partitioned_table(key int, value text) PARTITION BY RANGE (key);
 CREATE TABLE local_partitioned_table_1 PARTITION OF local_partitioned_table FOR VALUES FROM (0) TO (50);
 CREATE TABLE local_partitioned_table_2 PARTITION OF local_partitioned_table FOR VALUES FROM (50) TO (200);
 CREATE TABLE distributed_table_composite (key int, value text, value_2 jsonb, primary key (key, value));
 SELECT create_distributed_table('distributed_table_composite', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE MATERIALIZED VIEW mv1 AS SELECT * FROM postgres_table;
 CREATE MATERIALIZED VIEW mv2 AS SELECT * FROM distributed_table;
 -- set log messages to debug1 so that we can see which tables are recursively planned.
 SET client_min_messages TO DEBUG1;
 INSERT INTO postgres_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 INSERT INTO reference_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
 DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO distributed_table_windex SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO distributed_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO distributed_table_pkey SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO distributed_partitioned_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO distributed_table_composite SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO local_partitioned_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 SET citus.local_table_join_policy to 'auto';
 -- we can support modification queries as well
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM postgres_table;
  count 
 -------
    100
 (1 row)
 
 UPDATE
 	postgres_table
 SET
 	value = 'test'
 FROM
 	distributed_table
 WHERE
 	distributed_table.key = postgres_table.key;
-DEBUG:  Wrapping relation "distributed_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.distributed_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.postgres_table SET value = 'test'::text FROM (SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table WHERE (distributed_table.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM postgres_table;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
@@ -108,23 +80,20 @@
 (1 row)
 
 UPDATE
 	distributed_table
 SET
 	value = 'test'
 FROM
 	postgres_table
 WHERE
 	distributed_table.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table SET value = 'test'::text FROM (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
@@ -133,23 +102,20 @@
 (1 row)
 
 UPDATE
 	distributed_table_pkey
 SET
 	value = 'test'
 FROM
 	postgres_table
 WHERE
 	distributed_table_pkey.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table_pkey SET value = 'test'::text FROM (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
@@ -158,23 +124,20 @@
 (1 row)
 
 UPDATE
 	distributed_table_windex
 SET
 	value = 'test'
 FROM
 	postgres_table
 WHERE
 	distributed_table_windex.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table_windex SET value = 'test'::text FROM (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table_windex.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 UPDATE
 	mv1
@@ -217,23 +180,20 @@
 (1 row)
 
 UPDATE
 	postgres_table
 SET
 	value = 'test'
 FROM
 	distributed_table
 WHERE
 	distributed_table.key = postgres_table.key;
-DEBUG:  Wrapping relation "distributed_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.distributed_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.postgres_table SET value = 'test'::text FROM (SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table WHERE (distributed_table.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM postgres_table;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
@@ -242,23 +202,20 @@
 (1 row)
 
 UPDATE
 	distributed_table
 SET
 	value = 'test'
 FROM
 	postgres_table
 WHERE
 	distributed_table.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table SET value = 'test'::text FROM (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
@@ -267,23 +224,20 @@
 (1 row)
 
 UPDATE
 	distributed_table_pkey
 SET
 	value = 'test'
 FROM
 	postgres_table
 WHERE
 	distributed_table_pkey.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table_pkey SET value = 'test'::text FROM (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
@@ -292,23 +246,20 @@
 (1 row)
 
 UPDATE
 	distributed_table_windex
 SET
 	value = 'test'
 FROM
 	postgres_table
 WHERE
 	distributed_table_windex.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table_windex SET value = 'test'::text FROM (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table_windex.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 SET citus.local_table_join_policy TO 'prefer-distributed';
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM postgres_table;
@@ -318,23 +269,20 @@
 (1 row)
 
 UPDATE
 	postgres_table
 SET
 	value = 'test'
 FROM
 	distributed_table
 WHERE
 	distributed_table.key = postgres_table.key;
-DEBUG:  Wrapping relation "distributed_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.distributed_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.postgres_table SET value = 'test'::text FROM (SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table WHERE (distributed_table.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM postgres_table;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
@@ -343,23 +291,20 @@
 (1 row)
 
 UPDATE
 	distributed_table
 SET
 	value = 'test'
 FROM
 	postgres_table
 WHERE
 	distributed_table.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table SET value = 'test'::text FROM (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
@@ -368,23 +313,20 @@
 (1 row)
 
 UPDATE
 	distributed_table_pkey
 SET
 	value = 'test'
 FROM
 	postgres_table
 WHERE
 	distributed_table_pkey.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table_pkey SET value = 'test'::text FROM (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
@@ -393,187 +335,155 @@
 (1 row)
 
 UPDATE
 	distributed_table_windex
 SET
 	value = 'test'
 FROM
 	postgres_table
 WHERE
 	distributed_table_windex.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table_windex SET value = 'test'::text FROM (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table_windex.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 SET citus.local_table_join_policy TO 'auto';
 -- modifications with multiple tables
 BEGIN;
 UPDATE
 	distributed_table
 SET
 	value = 'test'
 FROM
 	postgres_table p1, postgres_table p2
 WHERE
 	distributed_table.key = p1.key AND p1.key = p2.key;
-DEBUG:  Wrapping relation "postgres_table" "p1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table p1 WHERE true
-DEBUG:  Wrapping relation "postgres_table" "p2" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM local_dist_join_modifications.postgres_table p2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table SET value = 'test'::text FROM (SELECT p1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p1_1) p1, (SELECT p2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p2_1) p2 WHERE ((distributed_table.key OPERATOR(pg_catalog.=) p1.key) AND (p1.key OPERATOR(pg_catalog.=) p2.key))
 ROLLBACK;
 BEGIN;
 UPDATE
 	postgres_table
 SET
 	value = 'test'
 FROM
 	(SELECT * FROM distributed_table) d1
 WHERE
 	d1.key = postgres_table.key;
-ERROR:  relation postgres_table is not distributed
 ROLLBACK;
 BEGIN;
 UPDATE
 	postgres_table
 SET
 	value = 'test'
 FROM
 	(SELECT * FROM distributed_table LIMIT 1) d1
 WHERE
 	d1.key = postgres_table.key;
-DEBUG:  push down of limit count: 1
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value, value_2 FROM local_dist_join_modifications.distributed_table LIMIT 1
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.postgres_table SET value = 'test'::text FROM (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb)) d1 WHERE (d1.key OPERATOR(pg_catalog.=) postgres_table.key)
 ROLLBACK;
 BEGIN;
 UPDATE
 	distributed_table
 SET
 	value = 'test'
 FROM
 	postgres_table p1, distributed_table d2
 WHERE
 	distributed_table.key = p1.key AND p1.key = d2.key;
-DEBUG:  Wrapping relation "postgres_table" "p1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table p1 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.distributed_table SET value = 'test'::text FROM (SELECT p1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p1_1) p1, local_dist_join_modifications.distributed_table d2 WHERE ((distributed_table.key OPERATOR(pg_catalog.=) p1.key) AND (p1.key OPERATOR(pg_catalog.=) d2.key))
 ROLLBACK;
 -- pretty inefficient plan as it requires
 -- recursive planninng of 2 distributed tables
 BEGIN;
 UPDATE
 	postgres_table
 SET
 	value = 'test'
 FROM
 	distributed_table d1, distributed_table d2
 WHERE
 	postgres_table.key = d1.key AND d1.key = d2.key;
-DEBUG:  Wrapping relation "distributed_table" "d1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.distributed_table d1 WHERE true
-DEBUG:  Wrapping relation "distributed_table" "d2" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM local_dist_join_modifications.distributed_table d2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_dist_join_modifications.postgres_table SET value = 'test'::text FROM (SELECT d1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) d1_1) d1, (SELECT d2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) d2_1) d2 WHERE ((postgres_table.key OPERATOR(pg_catalog.=) d1.key) AND (d1.key OPERATOR(pg_catalog.=) d2.key))
 ROLLBACK;
 -- DELETE operations
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM postgres_table;
  count 
 -------
    100
 (1 row)
 
 DELETE FROM
 	postgres_table
 USING
 	distributed_table
 WHERE
 	distributed_table.key = postgres_table.key;
-DEBUG:  Wrapping relation "distributed_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.distributed_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: DELETE FROM local_dist_join_modifications.postgres_table USING (SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table WHERE (distributed_table.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM postgres_table;
  count 
 -------
      0
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
 -------
    100
 (1 row)
 
 DELETE FROM
 	distributed_table
 USING
 	postgres_table
 WHERE
 	distributed_table.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: DELETE FROM local_dist_join_modifications.distributed_table USING (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
 -------
      0
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
 -------
    100
 (1 row)
 
 DELETE FROM
 	distributed_table_pkey
 USING
 	postgres_table
 WHERE
 	distributed_table_pkey.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: DELETE FROM local_dist_join_modifications.distributed_table_pkey USING (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
 -------
      0
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
 -------
    100
 (1 row)
 
 DELETE FROM
 	distributed_table_windex
 USING
 	postgres_table
 WHERE
 	distributed_table_windex.key = postgres_table.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_dist_join_modifications.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: DELETE FROM local_dist_join_modifications.distributed_table_windex USING (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (distributed_table_windex.key OPERATOR(pg_catalog.=) postgres_table.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
 -------
      0
 (1 row)
 
 ROLLBACK;
 DELETE FROM
 	mv1
 USING
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/local_table_join.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/local_table_join.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/local_table_join.out.modified	2022-11-09 13:38:53.159312293 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/local_table_join.out.modified	2022-11-09 13:38:53.179312293 +0300
@@ -3,1349 +3,995 @@
 CREATE TABLE postgres_table (key int, value text, value_2 jsonb);
 CREATE TABLE reference_table (key int, value text, value_2 jsonb);
 SELECT create_reference_table('reference_table');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 CREATE TABLE distributed_table (key int, value text, value_2 jsonb);
 SELECT create_distributed_table('distributed_table', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE distributed_table_pkey (key int primary key, value text, value_2 jsonb);
 SELECT create_distributed_table('distributed_table_pkey', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE distributed_table_windex (key int primary key, value text, value_2 jsonb);
 SELECT create_distributed_table('distributed_table_windex', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE UNIQUE INDEX key_index ON distributed_table_windex (key);
 CREATE TABLE distributed_partitioned_table(key int, value text) PARTITION BY RANGE (key);
 CREATE TABLE distributed_partitioned_table_1 PARTITION OF distributed_partitioned_table FOR VALUES FROM (0) TO (50);
 CREATE TABLE distributed_partitioned_table_2 PARTITION OF distributed_partitioned_table FOR VALUES FROM (50) TO (200);
 SELECT create_distributed_table('distributed_partitioned_table', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE local_partitioned_table(key int, value text) PARTITION BY RANGE (key);
 CREATE TABLE local_partitioned_table_1 PARTITION OF local_partitioned_table FOR VALUES FROM (0) TO (50);
 CREATE TABLE local_partitioned_table_2 PARTITION OF local_partitioned_table FOR VALUES FROM (50) TO (200);
 CREATE TABLE distributed_table_composite (key int, value text, value_2 jsonb, primary key (key, value));
 SELECT create_distributed_table('distributed_table_composite', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO postgres_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 INSERT INTO reference_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 INSERT INTO distributed_table_windex SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 INSERT INTO distributed_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 INSERT INTO distributed_table_pkey SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 INSERT INTO distributed_partitioned_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 INSERT INTO distributed_table_composite SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 INSERT INTO local_partitioned_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 CREATE FUNCTION fake_fdw_handler()
 RETURNS fdw_handler
 AS 'citus'
 LANGUAGE C STRICT;
 CREATE FOREIGN DATA WRAPPER fake_fdw_1 HANDLER fake_fdw_handler;
 SELECT run_command_on_workers($$
     CREATE FOREIGN DATA WRAPPER fake_fdw_1 HANDLER fake_fdw_handler;
 $$);
               run_command_on_workers               
 ---------------------------------------------------
- (localhost,57637,t,"CREATE FOREIGN DATA WRAPPER")
  (localhost,57638,t,"CREATE FOREIGN DATA WRAPPER")
-(2 rows)
+(1 row)
 
 -- Since we are assuming fdw should be part of the extension, add it manually.
 ALTER EXTENSION citus ADD FOREIGN DATA WRAPPER fake_fdw_1;
 NOTICE:  Citus does not propagate adding/dropping member objects
 HINT:  You can add/drop the member objects on the workers as well.
 CREATE SERVER fake_fdw_server_1 FOREIGN DATA WRAPPER fake_fdw_1;
 ALTER EXTENSION citus DROP FOREIGN DATA WRAPPER fake_fdw_1;
 NOTICE:  Citus does not propagate adding/dropping member objects
 HINT:  You can add/drop the member objects on the workers as well.
 CREATE FOREIGN TABLE foreign_table (
   key int,
   value text
 ) SERVER fake_fdw_server_1 OPTIONS (encoding 'utf-8', compression 'true');
 CREATE MATERIALIZED VIEW mv1 AS SELECT * FROM postgres_table;
 CREATE MATERIALIZED VIEW mv2 AS SELECT * FROM distributed_table;
 SET client_min_messages TO DEBUG1;
 -- the user doesn't allow local / distributed table joinn
 SET citus.local_table_join_policy TO 'never';
 SELECT count(*) FROM postgres_table JOIN distributed_table USING(key);
-ERROR:  direct joins between distributed and local tables are not supported
-HINT:  Use CTE's or subqueries to select from local tables and use them in joins
+ count 
+-------
+   100
+(1 row)
+
 SELECT count(*) FROM postgres_table JOIN reference_table USING(key);
 ERROR:  direct joins between distributed and local tables are not supported
 HINT:  Use CTE's or subqueries to select from local tables and use them in joins
 -- the user prefers local table recursively planned
 SET citus.local_table_join_policy TO 'prefer-local';
 SELECT count(*) FROM postgres_table JOIN distributed_table USING(key);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN reference_table USING(key);
 DEBUG:  Wrapping relation "postgres_table" to a subquery
 DEBUG:  generating subplan 3_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
 DEBUG:  Plan 3 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('3_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 -- the user prefers distributed table recursively planned
 SET citus.local_table_join_policy TO 'prefer-distributed';
 SELECT count(*) FROM postgres_table JOIN distributed_table USING(key);
-DEBUG:  Wrapping relation "distributed_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN reference_table USING(key);
 DEBUG:  Wrapping relation "reference_table" to a subquery
 DEBUG:  generating subplan 4_1 for subquery SELECT key FROM local_table_join.reference_table WHERE true
 DEBUG:  Plan 4 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT reference_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('4_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) reference_table_1) reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 -- auto tests
 -- switch back to the default policy, which is auto
 SET citus.local_table_join_policy to 'auto';
 -- on the auto mode, the local tables should be recursively planned
 -- unless a unique index exists in a column for distributed table
 SELECT count(*) FROM distributed_table JOIN postgres_table USING(key);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM reference_table JOIN postgres_table USING(key);
 DEBUG:  Wrapping relation "postgres_table" to a subquery
 DEBUG:  generating subplan 6_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
 DEBUG:  Plan 6 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.reference_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('6_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_table JOIN postgres_table USING(key) JOIN reference_table USING (key);
+DEBUG:  Wrapping relation "distributed_table" to a subquery
+DEBUG:  generating subplan 7_1 for subquery SELECT key FROM local_table_join.distributed_table WHERE true
 DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_table_join.distributed_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) JOIN local_table_join.reference_table USING (key))
+DEBUG:  generating subplan 7_2 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
+DEBUG:  Plan 7 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('7_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('7_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 -- partititoned local tables should work as well
 SELECT count(*) FROM distributed_table JOIN local_partitioned_table USING(key);
-DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM reference_table JOIN local_partitioned_table USING(key);
 DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
 DEBUG:  generating subplan 8_1 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE true
 DEBUG:  Plan 8 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.reference_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('8_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_table JOIN local_partitioned_table USING(key) JOIN reference_table USING (key);
+DEBUG:  Wrapping relation "distributed_table" to a subquery
+DEBUG:  generating subplan 9_1 for subquery SELECT key FROM local_table_join.distributed_table WHERE true
 DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_table_join.distributed_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) JOIN local_table_join.reference_table USING (key))
+DEBUG:  generating subplan 9_2 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE true
+DEBUG:  Plan 9 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('9_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('9_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 -- materialized views should work too
 SELECT count(*) FROM distributed_table JOIN mv1 USING(key);
-DEBUG:  Wrapping relation "mv1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv1 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT mv1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv1_1) mv1 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed_table) d1 JOIN mv1 USING(key);
-DEBUG:  Wrapping relation "mv1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv1 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table.key, distributed_table.value, distributed_table.value_2 FROM local_table_join.distributed_table) d1 JOIN (SELECT mv1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv1_1) mv1 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM reference_table JOIN mv1 USING(key);
 DEBUG:  Wrapping relation "mv1" to a subquery
 DEBUG:  generating subplan 10_1 for subquery SELECT key FROM local_table_join.mv1 WHERE true
 DEBUG:  Plan 10 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.reference_table JOIN (SELECT mv1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('10_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv1_1) mv1 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_table JOIN mv1 USING(key) JOIN reference_table USING (key);
+DEBUG:  Wrapping relation "distributed_table" to a subquery
+DEBUG:  generating subplan 11_1 for subquery SELECT key FROM local_table_join.distributed_table WHERE true
 DEBUG:  Wrapping relation "mv1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv1 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_table_join.distributed_table JOIN (SELECT mv1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv1_1) mv1 USING (key)) JOIN local_table_join.reference_table USING (key))
+DEBUG:  generating subplan 11_2 for subquery SELECT key FROM local_table_join.mv1 WHERE true
+DEBUG:  Plan 11 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('11_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table JOIN (SELECT mv1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('11_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv1_1) mv1 USING (key)) JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_table JOIN mv2 USING(key);
-DEBUG:  Wrapping relation "mv2" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT mv2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv2_1) mv2 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed_table) d1 JOIN mv2 USING(key);
-DEBUG:  Wrapping relation "mv2" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table.key, distributed_table.value, distributed_table.value_2 FROM local_table_join.distributed_table) d1 JOIN (SELECT mv2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv2_1) mv2 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM reference_table JOIN mv2 USING(key);
 DEBUG:  Wrapping relation "mv2" to a subquery
 DEBUG:  generating subplan 12_1 for subquery SELECT key FROM local_table_join.mv2 WHERE true
 DEBUG:  Plan 12 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.reference_table JOIN (SELECT mv2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('12_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv2_1) mv2 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_table JOIN mv2 USING(key) JOIN reference_table USING (key);
+DEBUG:  Wrapping relation "distributed_table" to a subquery
+DEBUG:  generating subplan 13_1 for subquery SELECT key FROM local_table_join.distributed_table WHERE true
 DEBUG:  Wrapping relation "mv2" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_table_join.distributed_table JOIN (SELECT mv2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv2_1) mv2 USING (key)) JOIN local_table_join.reference_table USING (key))
+DEBUG:  generating subplan 13_2 for subquery SELECT key FROM local_table_join.mv2 WHERE true
+DEBUG:  Plan 13 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('13_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table JOIN (SELECT mv2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('13_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv2_1) mv2 USING (key)) JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 -- foreign tables should work too
 SELECT count(*) FROM foreign_table JOIN distributed_table USING(key);
-DEBUG:  Wrapping relation "foreign_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.foreign_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT foreign_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) foreign_table_1) foreign_table JOIN local_table_join.distributed_table USING (key))
  count 
 -------
      0
 (1 row)
 
 -- partitioned tables should work as well
 SELECT count(*) FROM distributed_partitioned_table JOIN postgres_table USING(key);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_partitioned_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN postgres_table USING(key) WHERE distributed_partitioned_table.key = 10;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_partitioned_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE (distributed_partitioned_table.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
      1
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN postgres_table USING(key) JOIN reference_table USING (key);
+DEBUG:  Wrapping relation "distributed_partitioned_table" to a subquery
+DEBUG:  generating subplan 14_1 for subquery SELECT key FROM local_table_join.distributed_partitioned_table WHERE true
 DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_table_join.distributed_partitioned_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) JOIN local_table_join.reference_table USING (key))
+DEBUG:  generating subplan 14_2 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
+DEBUG:  Plan 14 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT distributed_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('14_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_partitioned_table_1) distributed_partitioned_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('14_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN local_partitioned_table USING(key);
-DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_partitioned_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN local_partitioned_table USING(key) WHERE distributed_partitioned_table.key = 10;
-DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_partitioned_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) WHERE (distributed_partitioned_table.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
      1
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN local_partitioned_table USING(key) JOIN reference_table USING (key);
+DEBUG:  Wrapping relation "distributed_partitioned_table" to a subquery
+DEBUG:  generating subplan 15_1 for subquery SELECT key FROM local_table_join.distributed_partitioned_table WHERE true
 DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_table_join.distributed_partitioned_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) JOIN local_table_join.reference_table USING (key))
+DEBUG:  generating subplan 15_2 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE true
+DEBUG:  Plan 15 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT distributed_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('15_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_partitioned_table_1) distributed_partitioned_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('15_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 -- similar tests in transaction block should work fine
 BEGIN;
 -- materialized views should work too
 SELECT count(*) FROM distributed_table JOIN mv1 USING(key);
-DEBUG:  Wrapping relation "mv1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv1 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT mv1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv1_1) mv1 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed_table) d1 JOIN mv1 USING(key);
-DEBUG:  Wrapping relation "mv1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv1 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table.key, distributed_table.value, distributed_table.value_2 FROM local_table_join.distributed_table) d1 JOIN (SELECT mv1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv1_1) mv1 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM reference_table JOIN mv1 USING(key);
 DEBUG:  Wrapping relation "mv1" to a subquery
 DEBUG:  generating subplan 16_1 for subquery SELECT key FROM local_table_join.mv1 WHERE true
 DEBUG:  Plan 16 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.reference_table JOIN (SELECT mv1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('16_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv1_1) mv1 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_table JOIN mv1 USING(key) JOIN reference_table USING (key);
+DEBUG:  Wrapping relation "distributed_table" to a subquery
+DEBUG:  generating subplan 17_1 for subquery SELECT key FROM local_table_join.distributed_table WHERE true
 DEBUG:  Wrapping relation "mv1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv1 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_table_join.distributed_table JOIN (SELECT mv1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv1_1) mv1 USING (key)) JOIN local_table_join.reference_table USING (key))
+DEBUG:  generating subplan 17_2 for subquery SELECT key FROM local_table_join.mv1 WHERE true
+DEBUG:  Plan 17 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('17_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table JOIN (SELECT mv1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('17_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv1_1) mv1 USING (key)) JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_table JOIN mv2 USING(key);
-DEBUG:  Wrapping relation "mv2" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT mv2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv2_1) mv2 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed_table) d1 JOIN mv2 USING(key);
-DEBUG:  Wrapping relation "mv2" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table.key, distributed_table.value, distributed_table.value_2 FROM local_table_join.distributed_table) d1 JOIN (SELECT mv2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv2_1) mv2 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM reference_table JOIN mv2 USING(key);
 DEBUG:  Wrapping relation "mv2" to a subquery
 DEBUG:  generating subplan 18_1 for subquery SELECT key FROM local_table_join.mv2 WHERE true
 DEBUG:  Plan 18 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.reference_table JOIN (SELECT mv2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('18_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv2_1) mv2 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_table JOIN mv2 USING(key) JOIN reference_table USING (key);
+DEBUG:  Wrapping relation "distributed_table" to a subquery
+DEBUG:  generating subplan 19_1 for subquery SELECT key FROM local_table_join.distributed_table WHERE true
 DEBUG:  Wrapping relation "mv2" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.mv2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_table_join.distributed_table JOIN (SELECT mv2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv2_1) mv2 USING (key)) JOIN local_table_join.reference_table USING (key))
+DEBUG:  generating subplan 19_2 for subquery SELECT key FROM local_table_join.mv2 WHERE true
+DEBUG:  Plan 19 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('19_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table JOIN (SELECT mv2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('19_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) mv2_1) mv2 USING (key)) JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 -- foreign tables should work too
 SELECT count(*) FROM foreign_table JOIN distributed_table USING(key);
-DEBUG:  Wrapping relation "foreign_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.foreign_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT foreign_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) foreign_table_1) foreign_table JOIN local_table_join.distributed_table USING (key))
  count 
 -------
      0
 (1 row)
 
 -- partitioned tables should work as well
 SELECT count(*) FROM distributed_partitioned_table JOIN postgres_table USING(key);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_partitioned_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN postgres_table USING(key) WHERE distributed_partitioned_table.key = 10;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_partitioned_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE (distributed_partitioned_table.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
      1
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN postgres_table USING(key) JOIN reference_table USING (key);
+DEBUG:  Wrapping relation "distributed_partitioned_table" to a subquery
+DEBUG:  generating subplan 20_1 for subquery SELECT key FROM local_table_join.distributed_partitioned_table WHERE true
 DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_table_join.distributed_partitioned_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) JOIN local_table_join.reference_table USING (key))
+DEBUG:  generating subplan 20_2 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
+DEBUG:  Plan 20 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT distributed_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('20_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_partitioned_table_1) distributed_partitioned_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('20_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN local_partitioned_table USING(key);
-DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_partitioned_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN local_partitioned_table USING(key) WHERE distributed_partitioned_table.key = 10;
-DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_partitioned_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) WHERE (distributed_partitioned_table.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
      1
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN local_partitioned_table USING(key) JOIN reference_table USING (key);
+DEBUG:  Wrapping relation "distributed_partitioned_table" to a subquery
+DEBUG:  generating subplan 21_1 for subquery SELECT key FROM local_table_join.distributed_partitioned_table WHERE true
 DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_table_join.distributed_partitioned_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) JOIN local_table_join.reference_table USING (key))
+DEBUG:  generating subplan 21_2 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE true
+DEBUG:  Plan 21 query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT distributed_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('21_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_partitioned_table_1) distributed_partitioned_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('21_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) JOIN local_table_join.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 ROLLBACK;
 -- the conversions should be independent from the order of table entries in the query
 SELECT COUNT(*) FROM postgres_table join distributed_table_pkey using(key) join local_partitioned_table using(key) join distributed_table using(key) where distributed_table_pkey.key = 5;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey USING (key)) JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) JOIN local_table_join.distributed_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)
  count 
 -------
      1
 (1 row)
 
 SELECT COUNT(*) FROM postgres_table join local_partitioned_table using(key) join distributed_table_pkey using(key) join distributed_table using(key) where distributed_table_pkey.key = 5;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) JOIN local_table_join.distributed_table_pkey USING (key)) JOIN local_table_join.distributed_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)
  count 
 -------
      1
 (1 row)
 
 SELECT COUNT(*) FROM postgres_table join distributed_table using(key) join local_partitioned_table using(key) join distributed_table_pkey using(key) where distributed_table_pkey.key = 5;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table USING (key)) JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) JOIN local_table_join.distributed_table_pkey USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)
  count 
 -------
      1
 (1 row)
 
 SELECT COUNT(*) FROM distributed_table_pkey join distributed_table using(key) join postgres_table using(key) join local_partitioned_table using(key) where distributed_table_pkey.key = 5;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Wrapping relation "local_partitioned_table" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM local_table_join.local_partitioned_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((local_table_join.distributed_table_pkey JOIN local_table_join.distributed_table USING (key)) JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) JOIN (SELECT local_partitioned_table_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) local_partitioned_table_1) local_partitioned_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)
  count 
 -------
      1
 (1 row)
 
 SELECT count(*) FROM (SELECT *, random() FROM distributed_table) as d1  JOIN postgres_table ON (postgres_table.key = d1.key AND d1.key < postgres_table.key) WHERE d1.key = 1 AND false;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table.key, distributed_table.value, distributed_table.value_2, random() AS random FROM local_table_join.distributed_table) d1 JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table ON (((postgres_table.key OPERATOR(pg_catalog.=) d1.key) AND (d1.key OPERATOR(pg_catalog.<) postgres_table.key)))) WHERE ((d1.key OPERATOR(pg_catalog.=) 1) AND false)
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM (SELECT *, random() FROM distributed_table_pkey) as d1  JOIN postgres_table ON (postgres_table.key = d1.key AND d1.key < postgres_table.key) WHERE d1.key = 1 AND false;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey.key, distributed_table_pkey.value, distributed_table_pkey.value_2, random() AS random FROM local_table_join.distributed_table_pkey) d1 JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table ON (((postgres_table.key OPERATOR(pg_catalog.=) d1.key) AND (d1.key OPERATOR(pg_catalog.<) postgres_table.key)))) WHERE ((d1.key OPERATOR(pg_catalog.=) 1) AND false)
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM (SELECT *, random() FROM distributed_partitioned_table) as d1  JOIN postgres_table ON (postgres_table.key = d1.key AND d1.key < postgres_table.key) WHERE d1.key = 1 AND false;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_partitioned_table.key, distributed_partitioned_table.value, random() AS random FROM local_table_join.distributed_partitioned_table) d1 JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table ON (((postgres_table.key OPERATOR(pg_catalog.=) d1.key) AND (d1.key OPERATOR(pg_catalog.<) postgres_table.key)))) WHERE ((d1.key OPERATOR(pg_catalog.=) 1) AND false)
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM (SELECT *, random() FROM distributed_partitioned_table) as d1  JOIN postgres_table ON (postgres_table.key::int = d1.key::int AND d1.key < postgres_table.key) WHERE d1.key::int = 1 AND false;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_partitioned_table.key, distributed_partitioned_table.value, random() AS random FROM local_table_join.distributed_partitioned_table) d1 JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table ON (((postgres_table.key OPERATOR(pg_catalog.=) d1.key) AND (d1.key OPERATOR(pg_catalog.<) postgres_table.key)))) WHERE ((d1.key OPERATOR(pg_catalog.=) 1) AND false)
  count 
 -------
      0
 (1 row)
 
 -- different column names
 SELECT a FROM postgres_table foo (a,b,c) JOIN distributed_table ON (distributed_table.key = foo.a) ORDER BY 1 LIMIT 1;
-DEBUG:  Wrapping relation "postgres_table" "foo" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT a AS key FROM local_table_join.postgres_table foo(a, b, c) WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT foo.a FROM ((SELECT foo_1.a AS key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) foo_1(a)) foo(a, b, c) JOIN local_table_join.distributed_table ON ((distributed_table.key OPERATOR(pg_catalog.=) foo.a))) ORDER BY foo.a LIMIT 1
-DEBUG:  push down of limit count: 1
  a 
 ---
  1
 (1 row)
 
 -- We will plan postgres table as the index is on key,value not just key
 SELECT count(*) FROM distributed_table_composite JOIN postgres_table USING(key) WHERE distributed_table_composite.key = 10;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table_composite JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE (distributed_table_composite.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
      1
 (1 row)
 
 SELECT count(*) FROM distributed_table_composite JOIN postgres_table USING(key) WHERE distributed_table_composite.key = 10 OR distributed_table_composite.key = 20;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table_composite JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE ((distributed_table_composite.key OPERATOR(pg_catalog.=) 10) OR (distributed_table_composite.key OPERATOR(pg_catalog.=) 20))
  count 
 -------
      2
 (1 row)
 
 SELECT count(*) FROM distributed_table_composite JOIN postgres_table USING(key) WHERE distributed_table_composite.key > 10 AND distributed_table_composite.value = 'text';
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table_composite JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE ((distributed_table_composite.key OPERATOR(pg_catalog.>) 10) AND (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text))
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM distributed_table_composite JOIN postgres_table USING(key) WHERE distributed_table_composite.key = 10 AND distributed_table_composite.value = 'text';
-DEBUG:  Wrapping relation "distributed_table_composite" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.distributed_table_composite WHERE ((key OPERATOR(pg_catalog.=) 10) AND (value OPERATOR(pg_catalog.=) 'text'::text))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_composite_1.key, distributed_table_composite_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) distributed_table_composite_1) distributed_table_composite JOIN local_table_join.postgres_table USING (key)) WHERE ((distributed_table_composite.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text))
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM distributed_table_composite JOIN postgres_table USING(key)
 	WHERE (distributed_table_composite.key > 10 OR distributed_table_composite.key = 20)
 	AND (distributed_table_composite.value = 'text' OR distributed_table_composite.value = 'text');
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table_composite JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE (((distributed_table_composite.key OPERATOR(pg_catalog.>) 10) OR (distributed_table_composite.key OPERATOR(pg_catalog.=) 20)) AND ((distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text) OR (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text)))
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM distributed_table_composite JOIN postgres_table USING(key)
 	WHERE (distributed_table_composite.key > 10 OR distributed_table_composite.value = 'text')
 	AND (distributed_table_composite.value = 'text' OR distributed_table_composite.key = 30);
-DEBUG:  Wrapping relation "distributed_table_composite" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.distributed_table_composite WHERE (((key OPERATOR(pg_catalog.>) 10) OR (value OPERATOR(pg_catalog.=) 'text'::text)) AND ((value OPERATOR(pg_catalog.=) 'text'::text) OR (key OPERATOR(pg_catalog.=) 30)))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_composite_1.key, distributed_table_composite_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) distributed_table_composite_1) distributed_table_composite JOIN local_table_join.postgres_table USING (key)) WHERE (((distributed_table_composite.key OPERATOR(pg_catalog.>) 10) OR (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text)) AND ((distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text) OR (distributed_table_composite.key OPERATOR(pg_catalog.=) 30)))
  count 
 -------
      1
 (1 row)
 
 SELECT count(*) FROM distributed_table_composite JOIN postgres_table USING(key)
 	WHERE (distributed_table_composite.key > 10 AND distributed_table_composite.value = 'text')
 	OR (distributed_table_composite.value = 'text' AND distributed_table_composite.key = 30);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table_composite JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE (((distributed_table_composite.key OPERATOR(pg_catalog.>) 10) AND (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text)) OR ((distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text) AND (distributed_table_composite.key OPERATOR(pg_catalog.=) 30)))
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM distributed_table_composite JOIN postgres_table USING(key)
 	WHERE (distributed_table_composite.key > 10 AND distributed_table_composite.key = 20)
 	OR (distributed_table_composite.value = 'text' AND distributed_table_composite.value = 'text');
-DEBUG:  Wrapping relation "distributed_table_composite" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.distributed_table_composite WHERE (((key OPERATOR(pg_catalog.>) 10) AND (key OPERATOR(pg_catalog.=) 20)) OR ((value OPERATOR(pg_catalog.=) 'text'::text) AND (value OPERATOR(pg_catalog.=) 'text'::text)))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_composite_1.key, distributed_table_composite_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) distributed_table_composite_1) distributed_table_composite JOIN local_table_join.postgres_table USING (key)) WHERE (((distributed_table_composite.key OPERATOR(pg_catalog.>) 10) AND (distributed_table_composite.key OPERATOR(pg_catalog.=) 20)) OR ((distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text) AND (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text)))
  count 
 -------
      1
 (1 row)
 
 SELECT count(*) FROM distributed_table_composite foo(a,b,c) JOIN postgres_table ON(foo.a > 1)
 	WHERE foo.a IN (SELECT COUNT(*) FROM local_partitioned_table) AND (foo.a = 10 OR foo.b ='text');
-DEBUG:  generating subplan XXX_1 for subquery SELECT count(*) AS count FROM local_table_join.local_partitioned_table
-DEBUG:  Wrapping relation "distributed_table_composite" "foo" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT a AS key, b AS value FROM local_table_join.distributed_table_composite foo(a, b, c) WHERE ((a OPERATOR(pg_catalog.>) 1) AND ((a OPERATOR(pg_catalog.=) 10) OR (b OPERATOR(pg_catalog.=) 'text'::text)))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT foo_1.a AS key, foo_1.b AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) foo_1(a, b)) foo(a, b, c) JOIN local_table_join.postgres_table ON ((foo.a OPERATOR(pg_catalog.>) 1))) WHERE ((foo.a OPERATOR(pg_catalog.=) ANY (SELECT intermediate_result.count FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(count bigint))) AND ((foo.a OPERATOR(pg_catalog.=) 10) OR (foo.b OPERATOR(pg_catalog.=) 'text'::text)))
  count 
 -------
      0
 (1 row)
 
 -- a unique index on key so dist table should be recursively planned
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey USING(key);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey USING(value);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT value FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS key, postgres_table_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(value text)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey USING (value))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON postgres_table.key = distributed_table_pkey.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey ON ((postgres_table.key OPERATOR(pg_catalog.=) distributed_table_pkey.key)))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10;
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey ON ((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10)))
  count 
 -------
    100
 (1 row)
 
 -- it should favor distributed table only if it has equality on the unique column
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key > 10;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey ON ((distributed_table_pkey.key OPERATOR(pg_catalog.>) 10)))
  count 
 -------
   9000
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key < 10;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey ON ((distributed_table_pkey.key OPERATOR(pg_catalog.<) 10)))
  count 
 -------
    900
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10;
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey ON ((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10)))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 AND distributed_table_pkey.key > 10 ;
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE ((key OPERATOR(pg_catalog.>) 10) AND (key OPERATOR(pg_catalog.=) 10))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_pkey.key OPERATOR(pg_catalog.>) 10))))
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 AND distributed_table_pkey.key > 10 ;
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE ((key OPERATOR(pg_catalog.>) 10) AND (key OPERATOR(pg_catalog.=) 10))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_pkey.key OPERATOR(pg_catalog.>) 10))))
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 AND distributed_table_pkey.key > 10 AND postgres_table.key = 5;
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE ((key OPERATOR(pg_catalog.>) 10) AND (key OPERATOR(pg_catalog.=) 10))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_pkey.key OPERATOR(pg_catalog.>) 10) AND (postgres_table.key OPERATOR(pg_catalog.=) 5))))
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 OR distributed_table_pkey.key > 10;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) OR (distributed_table_pkey.key OPERATOR(pg_catalog.>) 10))))
  count 
 -------
   9100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 OR distributed_table_pkey.key = 20;
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE ((key OPERATOR(pg_catalog.=) 10) OR (key OPERATOR(pg_catalog.=) 20))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) OR (distributed_table_pkey.key OPERATOR(pg_catalog.=) 20))))
  count 
 -------
    200
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 OR distributed_table_pkey.key = 20 OR distributed_table_pkey.key = 30;
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE ((key OPERATOR(pg_catalog.=) 10) OR (key OPERATOR(pg_catalog.=) 20) OR (key OPERATOR(pg_catalog.=) 30))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) OR (distributed_table_pkey.key OPERATOR(pg_catalog.=) 20) OR (distributed_table_pkey.key OPERATOR(pg_catalog.=) 30))))
  count 
 -------
    300
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 OR distributed_table_pkey.key = (
 	SELECT count(*) FROM distributed_table_pkey
 );
-DEBUG:  generating subplan XXX_1 for subquery SELECT count(*) AS count FROM local_table_join.distributed_table_pkey
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT NULL::integer AS "dummy-1" FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) OR (distributed_table_pkey.key OPERATOR(pg_catalog.=) (SELECT intermediate_result.count FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(count bigint))))))
  count 
 -------
    200
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 OR (distributed_table_pkey.key = 5 and distributed_table_pkey.key > 15);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE ((key OPERATOR(pg_catalog.=) 10) OR ((key OPERATOR(pg_catalog.=) 5) AND (key OPERATOR(pg_catalog.>) 15)))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) OR ((distributed_table_pkey.key OPERATOR(pg_catalog.=) 5) AND (distributed_table_pkey.key OPERATOR(pg_catalog.>) 15)))))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 OR (distributed_table_pkey.key > 10 and distributed_table_pkey.key > 15);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) OR ((distributed_table_pkey.key OPERATOR(pg_catalog.>) 10) AND (distributed_table_pkey.key OPERATOR(pg_catalog.>) 15)))))
  count 
 -------
   8600
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 OR (distributed_table_pkey.key > 10 and distributed_table_pkey.value = 'notext');
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.distributed_table_pkey WHERE ((key OPERATOR(pg_catalog.=) 10) OR ((key OPERATOR(pg_catalog.>) 10) AND (value OPERATOR(pg_catalog.=) 'notext'::text)))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_pkey_1.key, distributed_table_pkey_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) distributed_table_pkey_1) distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) OR ((distributed_table_pkey.key OPERATOR(pg_catalog.>) 10) AND (distributed_table_pkey.value OPERATOR(pg_catalog.=) 'notext'::text)))))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON distributed_table_pkey.key = 10 OR (distributed_table_pkey.key = 10 and distributed_table_pkey.value = 'notext');
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey ON (((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) OR ((distributed_table_pkey.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_pkey.value OPERATOR(pg_catalog.=) 'notext'::text)))))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_pkey ON postgres_table.key = 10;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey ON ((postgres_table.key OPERATOR(pg_catalog.=) 10)))
  count 
 -------
    100
 (1 row)
 
 select count(*) FROM postgres_table JOIN (SELECT a.key,random() FROM distributed_table a JOIN distributed_table b USING(key)) as foo USING(key);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN (SELECT a.key, random() AS random FROM (local_table_join.distributed_table a JOIN local_table_join.distributed_table b USING (key))) foo USING (key))
  count 
 -------
    100
 (1 row)
 
 select count(*) FROM (SELECT a.key, random() FROM distributed_table a JOIN distributed_table b USING(key)) as foo JOIN postgres_table  USING(key);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT a.key, random() AS random FROM (local_table_join.distributed_table a JOIN local_table_join.distributed_table b USING (key))) foo JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN (SELECT * FROM distributed_table) d1 USING(key);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN (SELECT distributed_table.key, distributed_table.value, distributed_table.value_2 FROM local_table_join.distributed_table) d1 USING (key))
  count 
 -------
    100
 (1 row)
 
 -- since this is already router plannable, we don't recursively plan the postgres table
 SELECT count(*) FROM postgres_table JOIN (SELECT * FROM distributed_table LIMIT 1) d1 USING(key);
-DEBUG:  push down of limit count: 1
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value, value_2 FROM local_table_join.distributed_table LIMIT 1
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb)) d1 USING (key))
  count 
 -------
      1
 (1 row)
 
 -- a unique index on key so dist table should be recursively planned
 SELECT count(*) FROM postgres_table JOIN distributed_table_windex USING(key);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_windex USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_windex USING(value);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT value FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS key, postgres_table_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(value text)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_windex USING (value))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_windex ON postgres_table.key = distributed_table_windex.key;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_windex ON ((postgres_table.key OPERATOR(pg_catalog.=) distributed_table_windex.key)))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN distributed_table_windex ON distributed_table_windex.key = 10;
-DEBUG:  Wrapping relation "distributed_table_windex" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_windex WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_windex_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_windex_1) distributed_table_windex ON ((distributed_table_windex.key OPERATOR(pg_catalog.=) 10)))
  count 
 -------
    100
 (1 row)
 
 -- no unique index on value so local table should be recursively planned.
 SELECT count(*) FROM distributed_table JOIN postgres_table USING(key) WHERE distributed_table.value = 'test';
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE (distributed_table.value OPERATOR(pg_catalog.=) 'test'::text)
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM distributed_table JOIN postgres_table USING(key) WHERE distributed_table.key = 1;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 1)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE (distributed_table.key OPERATOR(pg_catalog.=) 1)
  count 
 -------
      1
 (1 row)
 
 -- if both local and distributed tables have a filter, we prefer local unless distributed table has unique indexes on any equality filter
 SELECT count(*) FROM distributed_table JOIN postgres_table USING(key) WHERE distributed_table.value = 'test' AND postgres_table.value = 'test';
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.postgres_table WHERE (value OPERATOR(pg_catalog.=) 'test'::text)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT postgres_table_1.key, postgres_table_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) postgres_table_1) postgres_table USING (key)) WHERE ((distributed_table.value OPERATOR(pg_catalog.=) 'test'::text) AND (postgres_table.value OPERATOR(pg_catalog.=) 'test'::text))
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM distributed_table JOIN postgres_table USING(key) WHERE distributed_table.value = 'test' OR postgres_table.value = 'test';
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT postgres_table_1.key, postgres_table_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) postgres_table_1) postgres_table USING (key)) WHERE ((distributed_table.value OPERATOR(pg_catalog.=) 'test'::text) OR (postgres_table.value OPERATOR(pg_catalog.=) 'test'::text))
  count 
 -------
      0
 (1 row)
 
 -- multiple local/distributed tables
 -- only local tables are recursively planned
 SELECT count(*) FROM distributed_table d1 JOIN postgres_table p1 USING(key) JOIN distributed_table d2 USING(key) JOIN postgres_table p2 USING(key);
-DEBUG:  Wrapping relation "postgres_table" "p1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table p1 WHERE true
-DEBUG:  Wrapping relation "postgres_table" "p2" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM local_table_join.postgres_table p2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((local_table_join.distributed_table d1 JOIN (SELECT p1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p1_1) p1 USING (key)) JOIN local_table_join.distributed_table d2 USING (key)) JOIN (SELECT p2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p2_1) p2 USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT
 	count(*)
 FROM
 	distributed_table d1 JOIN postgres_table p1 USING(key) JOIN distributed_table d2 USING(key) JOIN postgres_table p2 USING(key)
 WHERE
 	d1.value = '1';
-DEBUG:  Wrapping relation "postgres_table" "p1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table p1 WHERE true
-DEBUG:  Wrapping relation "postgres_table" "p2" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM local_table_join.postgres_table p2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((local_table_join.distributed_table d1 JOIN (SELECT p1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p1_1) p1 USING (key)) JOIN local_table_join.distributed_table d2 USING (key)) JOIN (SELECT p2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p2_1) p2 USING (key)) WHERE (d1.value OPERATOR(pg_catalog.=) '1'::text)
  count 
 -------
      1
 (1 row)
 
 -- if the filter is on the JOIN key, we can recursively plan the local
 -- tables as filters are pushed down to the local tables
 SELECT
 	count(*)
 FROM
 	distributed_table d1 JOIN postgres_table p1 USING(key) JOIN distributed_table d2 USING(key) JOIN postgres_table p2 USING(key)
 WHERE
 	d1.key = 1;
-DEBUG:  Wrapping relation "postgres_table" "p1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table p1 WHERE (key OPERATOR(pg_catalog.=) 1)
-DEBUG:  Wrapping relation "postgres_table" "p2" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM local_table_join.postgres_table p2 WHERE (key OPERATOR(pg_catalog.=) 1)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((local_table_join.distributed_table d1 JOIN (SELECT p1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p1_1) p1 USING (key)) JOIN local_table_join.distributed_table d2 USING (key)) JOIN (SELECT p2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p2_1) p2 USING (key)) WHERE (d1.key OPERATOR(pg_catalog.=) 1)
  count 
 -------
      1
 (1 row)
 
 CREATE view loc_view AS SELECT * FROM postgres_table WHERE key > 0;
 WARNING:  "view loc_view" has dependency to "table postgres_table" that is not in Citus' metadata
 DETAIL:  "view loc_view" will be created only locally
 HINT:  Distribute "table postgres_table" first to distribute "view loc_view"
 UPDATE loc_view SET key = (SELECT COUNT(*) FROM distributed_table);
-DEBUG:  generating subplan XXX_1 for subquery SELECT count(*) AS count FROM local_table_join.distributed_table
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_table_join.postgres_table SET key = (SELECT intermediate_result.count FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(count bigint)) FROM local_table_join.postgres_table WHERE (postgres_table.key OPERATOR(pg_catalog.>) 0)
-ERROR:  cannot modify views when the query contains citus tables
 SELECT count(*)
 FROM
 	(SELECT * FROM (SELECT * FROM distributed_table) d1) d2
 JOIN postgres_table
 USING(key);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT d1.key, d1.value, d1.value_2 FROM (SELECT distributed_table.key, distributed_table.value, distributed_table.value_2 FROM local_table_join.distributed_table) d1) d2 JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key))
  count 
 -------
    100
 (1 row)
 
 -- will error as we don't support complex joins
 SELECT COUNT(*) FROM postgres_table, distributed_table d1, distributed_table d2 WHERE d1.value = d2.value;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT NULL::integer AS key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) postgres_table_1) postgres_table, local_table_join.distributed_table d1, local_table_join.distributed_table d2 WHERE (d1.value OPERATOR(pg_catalog.=) d2.value)
-ERROR:  complex joins are only supported when all distributed tables are co-located and joined on their distribution columns
+ count 
+-------
+ 10000
+(1 row)
+
 -- This will error because router planner will think that since reference tables have a single
 -- shard, it contains only a single task for modify. However, updating a reference tables
 -- will require multiple tasks. So requires some rewrite in router planner.
 UPDATE reference_table SET key = 1 FROM postgres_table WHERE postgres_table.key = 10;
 DEBUG:  Wrapping relation "postgres_table" to a subquery
 DEBUG:  generating subplan 22_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 10)
 DEBUG:  Plan 22 query after replacing subqueries and CTEs: UPDATE local_table_join.reference_table SET key = 1 FROM (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('22_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table WHERE (postgres_table.key OPERATOR(pg_catalog.=) 10)
 ERROR:  relation postgres_table is not distributed
 UPDATE reference_table SET key = 1 FROM (SELECT * FROM postgres_table) l WHERE l.key = 10;
 DEBUG:  generating subplan 23_1 for subquery SELECT key, value, value_2 FROM local_table_join.postgres_table
 DEBUG:  Plan 23 query after replacing subqueries and CTEs: UPDATE local_table_join.reference_table SET key = 1 FROM (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2 FROM read_intermediate_result('23_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb)) l WHERE (l.key OPERATOR(pg_catalog.=) 10)
 SELECT count(*) FROM postgres_table JOIN distributed_table USING(key) WHERE FALSE;
-DEBUG:  Wrapping relation "distributed_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table USING (key)) WHERE false
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed_table JOIN postgres_table USING(key) WHERE false) foo JOIN local_partitioned_table USING(key);
-DEBUG:  Wrapping relation "distributed_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table WHERE false
-DEBUG:  generating subplan XXX_2 for subquery SELECT distributed_table.key, distributed_table.value, distributed_table.value_2, postgres_table.value, postgres_table.value_2 FROM ((SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table JOIN local_table_join.postgres_table USING (key)) WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2, intermediate_result.value_1 AS value, intermediate_result.value_2_1 AS value_2 FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb, value_1 text, value_2_1 jsonb)) foo(key, value, value_2, value_1, value_2_1) JOIN local_table_join.local_partitioned_table USING (key))
  count 
 -------
      0
 (1 row)
 
 WITH dist_cte AS (SELECT * FROM distributed_table_pkey WHERE key = 5)
 SELECT COUNT(*) FROM dist_cte JOIN postgres_table USING(key) WHERE dist_cte.key = 5;
-DEBUG:  CTE dist_cte is going to be inlined via distributed planning
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey.key, distributed_table_pkey.value, distributed_table_pkey.value_2 FROM local_table_join.distributed_table_pkey WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)) dist_cte JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE (dist_cte.key OPERATOR(pg_catalog.=) 5)
  count 
 -------
-     1
+     0
 (1 row)
 
 SELECT COUNT(*) FROM postgres_table JOIN distributed_table_pkey USING(key)
 	WHERE (distributed_table_pkey.key IN (SELECT COUNT(*) AS count FROM postgres_table JOIN distributed_table USING(key)) );
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  generating subplan XXX_2 for subquery SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table USING (key))
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_3 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN local_table_join.distributed_table_pkey USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) ANY (SELECT intermediate_result.count FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(count bigint)))
  count 
 -------
-     1
+   100
 (1 row)
 
 -- PREPARED statements
 PREPARE local_dist_table_join_select(int) AS SELECT COUNT(*) FROM distributed_table_pkey JOIN postgres_table USING(key) WHERE distributed_table_pkey.key = $1;
 EXECUTE local_dist_table_join_select(10);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.postgres_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
-     1
+     0
 (1 row)
 
 EXECUTE local_dist_table_join_select(10);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.postgres_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
-     1
+     0
 (1 row)
 
 EXECUTE local_dist_table_join_select(10);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.postgres_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
-     1
+     0
 (1 row)
 
 EXECUTE local_dist_table_join_select(10);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.postgres_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
-     1
+     0
 (1 row)
 
 EXECUTE local_dist_table_join_select(10);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.postgres_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
-     1
+     0
 (1 row)
 
 EXECUTE local_dist_table_join_select(10);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.postgres_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 10)
  count 
 -------
-     1
+     0
 (1 row)
 
 PREPARE local_dist_table_join_update(int) AS UPDATE postgres_table SET key = 5 FROM distributed_table_pkey  WHERE distributed_table_pkey.key = $1;
 EXECUTE local_dist_table_join_update(20);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 20)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_table_join.postgres_table SET key = 5 FROM (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 20)
 EXECUTE local_dist_table_join_update(20);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 20)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_table_join.postgres_table SET key = 5 FROM (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 20)
 EXECUTE local_dist_table_join_update(20);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 20)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_table_join.postgres_table SET key = 5 FROM (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 20)
 EXECUTE local_dist_table_join_update(20);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 20)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_table_join.postgres_table SET key = 5 FROM (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 20)
 EXECUTE local_dist_table_join_update(20);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 20)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_table_join.postgres_table SET key = 5 FROM (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 20)
 EXECUTE local_dist_table_join_update(20);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 20)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE local_table_join.postgres_table SET key = 5 FROM (SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 20)
 PREPARE local_dist_table_join_subquery(int) AS SELECT COUNT(*) FROM postgres_table JOIN (SELECT * FROM distributed_table_pkey JOIN local_partitioned_table USING(key) WHERE distributed_table_pkey.key = $1) foo USING(key);
 EXECUTE local_dist_table_join_subquery(5);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  generating subplan XXX_2 for subquery SELECT distributed_table_pkey.key, distributed_table_pkey.value, distributed_table_pkey.value_2, local_partitioned_table.value FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.local_partitioned_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2, intermediate_result.value_1 AS value FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb, value_1 text)) foo(key, value, value_2, value_1) USING (key))
  count 
 -------
    100
 (1 row)
 
 EXECUTE local_dist_table_join_subquery(5);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  generating subplan XXX_2 for subquery SELECT distributed_table_pkey.key, distributed_table_pkey.value, distributed_table_pkey.value_2, local_partitioned_table.value FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.local_partitioned_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2, intermediate_result.value_1 AS value FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb, value_1 text)) foo(key, value, value_2, value_1) USING (key))
  count 
 -------
    100
 (1 row)
 
 EXECUTE local_dist_table_join_subquery(5);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  generating subplan XXX_2 for subquery SELECT distributed_table_pkey.key, distributed_table_pkey.value, distributed_table_pkey.value_2, local_partitioned_table.value FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.local_partitioned_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2, intermediate_result.value_1 AS value FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb, value_1 text)) foo(key, value, value_2, value_1) USING (key))
  count 
 -------
    100
 (1 row)
 
 EXECUTE local_dist_table_join_subquery(5);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  generating subplan XXX_2 for subquery SELECT distributed_table_pkey.key, distributed_table_pkey.value, distributed_table_pkey.value_2, local_partitioned_table.value FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.local_partitioned_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2, intermediate_result.value_1 AS value FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb, value_1 text)) foo(key, value, value_2, value_1) USING (key))
  count 
 -------
    100
 (1 row)
 
 EXECUTE local_dist_table_join_subquery(5);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  generating subplan XXX_2 for subquery SELECT distributed_table_pkey.key, distributed_table_pkey.value, distributed_table_pkey.value_2, local_partitioned_table.value FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.local_partitioned_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2, intermediate_result.value_1 AS value FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb, value_1 text)) foo(key, value, value_2, value_1) USING (key))
  count 
 -------
    100
 (1 row)
 
 EXECUTE local_dist_table_join_subquery(5);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  generating subplan XXX_2 for subquery SELECT distributed_table_pkey.key, distributed_table_pkey.value, distributed_table_pkey.value_2, local_partitioned_table.value FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.local_partitioned_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.postgres_table JOIN (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2, intermediate_result.value_1 AS value FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb, value_1 text)) foo(key, value, value_2, value_1) USING (key))
  count 
 -------
    100
 (1 row)
 
 PREPARE local_dist_table_join_filters(int) AS SELECT COUNT(*) FROM local_partitioned_table JOIN distributed_table_composite USING(key)
 	WHERE(
 		distributed_table_composite.key = $1 OR
 		distributed_table_composite.key = 20 OR
 		(distributed_table_composite.key = 10 AND distributed_table_composite.key > 0) OR
 		distributed_table_composite.value = 'text'
 		);
 EXECUTE local_dist_table_join_filters(20);
-DEBUG:  Wrapping relation "distributed_table_composite" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.distributed_table_composite WHERE ((key OPERATOR(pg_catalog.=) 20) OR (key OPERATOR(pg_catalog.=) 20) OR ((key OPERATOR(pg_catalog.=) 10) AND (key OPERATOR(pg_catalog.>) 0)) OR (value OPERATOR(pg_catalog.=) 'text'::text))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.local_partitioned_table JOIN (SELECT distributed_table_composite_1.key, distributed_table_composite_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) distributed_table_composite_1) distributed_table_composite USING (key)) WHERE ((distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR (distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR ((distributed_table_composite.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_composite.key OPERATOR(pg_catalog.>) 0)) OR (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text))
  count 
 -------
      2
 (1 row)
 
 EXECUTE local_dist_table_join_filters(20);
-DEBUG:  Wrapping relation "distributed_table_composite" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.distributed_table_composite WHERE ((key OPERATOR(pg_catalog.=) 20) OR (key OPERATOR(pg_catalog.=) 20) OR ((key OPERATOR(pg_catalog.=) 10) AND (key OPERATOR(pg_catalog.>) 0)) OR (value OPERATOR(pg_catalog.=) 'text'::text))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.local_partitioned_table JOIN (SELECT distributed_table_composite_1.key, distributed_table_composite_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) distributed_table_composite_1) distributed_table_composite USING (key)) WHERE ((distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR (distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR ((distributed_table_composite.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_composite.key OPERATOR(pg_catalog.>) 0)) OR (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text))
  count 
 -------
      2
 (1 row)
 
 EXECUTE local_dist_table_join_filters(20);
-DEBUG:  Wrapping relation "distributed_table_composite" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.distributed_table_composite WHERE ((key OPERATOR(pg_catalog.=) 20) OR (key OPERATOR(pg_catalog.=) 20) OR ((key OPERATOR(pg_catalog.=) 10) AND (key OPERATOR(pg_catalog.>) 0)) OR (value OPERATOR(pg_catalog.=) 'text'::text))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.local_partitioned_table JOIN (SELECT distributed_table_composite_1.key, distributed_table_composite_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) distributed_table_composite_1) distributed_table_composite USING (key)) WHERE ((distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR (distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR ((distributed_table_composite.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_composite.key OPERATOR(pg_catalog.>) 0)) OR (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text))
  count 
 -------
      2
 (1 row)
 
 EXECUTE local_dist_table_join_filters(20);
-DEBUG:  Wrapping relation "distributed_table_composite" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.distributed_table_composite WHERE ((key OPERATOR(pg_catalog.=) 20) OR (key OPERATOR(pg_catalog.=) 20) OR ((key OPERATOR(pg_catalog.=) 10) AND (key OPERATOR(pg_catalog.>) 0)) OR (value OPERATOR(pg_catalog.=) 'text'::text))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.local_partitioned_table JOIN (SELECT distributed_table_composite_1.key, distributed_table_composite_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) distributed_table_composite_1) distributed_table_composite USING (key)) WHERE ((distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR (distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR ((distributed_table_composite.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_composite.key OPERATOR(pg_catalog.>) 0)) OR (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text))
  count 
 -------
      2
 (1 row)
 
 EXECUTE local_dist_table_join_filters(20);
-DEBUG:  Wrapping relation "distributed_table_composite" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.distributed_table_composite WHERE ((key OPERATOR(pg_catalog.=) 20) OR (key OPERATOR(pg_catalog.=) 20) OR ((key OPERATOR(pg_catalog.=) 10) AND (key OPERATOR(pg_catalog.>) 0)) OR (value OPERATOR(pg_catalog.=) 'text'::text))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.local_partitioned_table JOIN (SELECT distributed_table_composite_1.key, distributed_table_composite_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) distributed_table_composite_1) distributed_table_composite USING (key)) WHERE ((distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR (distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR ((distributed_table_composite.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_composite.key OPERATOR(pg_catalog.>) 0)) OR (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text))
  count 
 -------
      2
 (1 row)
 
 EXECUTE local_dist_table_join_filters(20);
-DEBUG:  Wrapping relation "distributed_table_composite" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value FROM local_table_join.distributed_table_composite WHERE ((key OPERATOR(pg_catalog.=) 20) OR (key OPERATOR(pg_catalog.=) 20) OR ((key OPERATOR(pg_catalog.=) 10) AND (key OPERATOR(pg_catalog.>) 0)) OR (value OPERATOR(pg_catalog.=) 'text'::text))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.local_partitioned_table JOIN (SELECT distributed_table_composite_1.key, distributed_table_composite_1.value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) distributed_table_composite_1) distributed_table_composite USING (key)) WHERE ((distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR (distributed_table_composite.key OPERATOR(pg_catalog.=) 20) OR ((distributed_table_composite.key OPERATOR(pg_catalog.=) 10) AND (distributed_table_composite.key OPERATOR(pg_catalog.>) 0)) OR (distributed_table_composite.value OPERATOR(pg_catalog.=) 'text'::text))
  count 
 -------
      2
 (1 row)
 
 CREATE TABLE local (key1 int, key2 int, key3 int);
 INSERT INTO local VALUES (1,2,3);
 ALTER TABLE local DROP column key2;
 -- make sure dropped columns work
 SELECT COUNT(*) FROM local JOIN distributed_table ON(key1 = key);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key1 FROM local_table_join.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT local_1.key1, NULL::integer AS "dummy-2", NULL::integer AS key3 FROM (SELECT intermediate_result.key1 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key1 integer)) local_1) local JOIN local_table_join.distributed_table ON ((local.key1 OPERATOR(pg_catalog.=) distributed_table.key)))
  count 
 -------
      1
 (1 row)
 
 SELECT * FROM local JOIN distributed_table ON(key1 = key) ORDER BY 1 LIMIT 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key1, key3 FROM local_table_join.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT local.key1, local.key3, distributed_table.key, distributed_table.value, distributed_table.value_2 FROM ((SELECT local_1.key1, NULL::integer AS "dummy-2", local_1.key3 FROM (SELECT intermediate_result.key1, intermediate_result.key3 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key1 integer, key3 integer)) local_1) local JOIN local_table_join.distributed_table ON ((local.key1 OPERATOR(pg_catalog.=) distributed_table.key))) ORDER BY local.key1 LIMIT 1
-DEBUG:  push down of limit count: 1
  key1 | key3 | key | value | value_2 
 ------+------+-----+-------+---------
     1 |    3 |   1 | 1     | 
 (1 row)
 
 SELECT * FROM (SELECT local.key1, local.key3 FROM local JOIN distributed_table
  ON(local.key1 = distributed_table.key) GROUP BY local.key1, local.key3) a ORDER BY 1,2;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key1, key3 FROM local_table_join.local WHERE true
-DEBUG:  generating subplan XXX_2 for subquery SELECT local.key1, local.key3 FROM ((SELECT local_1.key1, NULL::integer AS "dummy-2", local_1.key3 FROM (SELECT intermediate_result.key1, intermediate_result.key3 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key1 integer, key3 integer)) local_1) local JOIN local_table_join.distributed_table ON ((local.key1 OPERATOR(pg_catalog.=) distributed_table.key))) GROUP BY local.key1, local.key3
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT key1, key3 FROM (SELECT intermediate_result.key1, intermediate_result.key3 FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key1 integer, key3 integer)) a ORDER BY key1, key3
  key1 | key3 
 ------+------
     1 |    3
 (1 row)
 
 SELECT * FROM (SELECT local.key3 FROM local JOIN distributed_table
  ON(local.key1 = distributed_table.key) GROUP BY local.key3) a ORDER BY 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key1, key3 FROM local_table_join.local WHERE true
-DEBUG:  generating subplan XXX_2 for subquery SELECT local.key3 FROM ((SELECT local_1.key1, NULL::integer AS "dummy-2", local_1.key3 FROM (SELECT intermediate_result.key1, intermediate_result.key3 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key1 integer, key3 integer)) local_1) local JOIN local_table_join.distributed_table ON ((local.key1 OPERATOR(pg_catalog.=) distributed_table.key))) GROUP BY local.key3
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT key3 FROM (SELECT intermediate_result.key3 FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key3 integer)) a ORDER BY key3
  key3 
 ------
     3
 (1 row)
 
 SELECT a.key3 FROM (SELECT local.key3 FROM local JOIN distributed_table
  ON(local.key1 = distributed_table.key) GROUP BY local.key3) a ORDER BY 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key1, key3 FROM local_table_join.local WHERE true
-DEBUG:  generating subplan XXX_2 for subquery SELECT local.key3 FROM ((SELECT local_1.key1, NULL::integer AS "dummy-2", local_1.key3 FROM (SELECT intermediate_result.key1, intermediate_result.key3 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key1 integer, key3 integer)) local_1) local JOIN local_table_join.distributed_table ON ((local.key1 OPERATOR(pg_catalog.=) distributed_table.key))) GROUP BY local.key3
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT key3 FROM (SELECT intermediate_result.key3 FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key3 integer)) a ORDER BY key3
  key3 
 ------
     3
 (1 row)
 
 -- drop all the remaining columns
 ALTER TABLE local DROP column key3;
 ALTER TABLE local DROP column key1;
 SELECT COUNT(*) FROM distributed_table JOIN local ON distributed_table.value = 'text';
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_table_join.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT NULL::integer AS "dummy-1", NULL::integer AS "dummy-2", NULL::integer AS "dummy-3" FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) local_1) local ON ((distributed_table.value OPERATOR(pg_catalog.=) 'text'::text)))
  count 
 -------
      0
 (1 row)
 
 --Issue 4678
 create table custom_pg_type(typdefault text);
 insert into custom_pg_type VALUES ('b');
 create table tbl (a int);
 insert into tbl VALUES (1);
@@ -1362,129 +1008,101 @@
       select true as bool from pg_catalog.pg_am limit 1
     )
   ) as subq_1
 ) as subq_2;
  typdefault 
 ------------
  b
 (1 row)
 
 select create_distributed_table('tbl', 'a');
-NOTICE:  Copying data from local table...
-DEBUG:  Copied 1 rows
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$local_table_join.tbl$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- subplans work but we might skip the restrictions in them
 select typdefault from (
   select typdefault from (
     select typdefault from
       custom_pg_type,
       lateral (
         select a from tbl
         where typdefault > 'a'
         limit 1) as subq_0
     where (
       select true as bool from pg_catalog.pg_am limit 1
     )
   ) as subq_1
 ) as subq_2;
-DEBUG:  generating subplan XXX_1 for subquery SELECT true AS bool FROM pg_am LIMIT 1
-DEBUG:  Wrapping relation "custom_pg_type" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT typdefault FROM local_table_join.custom_pg_type WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT typdefault FROM (SELECT subq_1.typdefault FROM (SELECT custom_pg_type.typdefault FROM (SELECT custom_pg_type_1.typdefault FROM (SELECT intermediate_result.typdefault FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(typdefault text)) custom_pg_type_1) custom_pg_type, LATERAL (SELECT tbl.a FROM local_table_join.tbl WHERE (custom_pg_type.typdefault OPERATOR(pg_catalog.>) 'a'::text) LIMIT 1) subq_0 WHERE (SELECT intermediate_result.bool FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(bool boolean))) subq_1) subq_2
-ERROR:  cannot push down this subquery
-DETAIL:  Limit clause is currently unsupported when a lateral subquery references a column from complex subqueries, CTEs or local tables
+ typdefault 
+------------
+ b
+(1 row)
+
 -- Not supported because of 4470
 select typdefault from (
   select typdefault from (
     select typdefault from
       custom_pg_type,
       lateral (
         select a from tbl
         where typdefault > 'a'
         limit 1) as subq_0
     where (
       select true from pg_catalog.pg_am
 	  where typdefault = 'a' LIMIT 1
     )
   ) as subq_1
 ) as subq_2;
-DEBUG:  Wrapping relation "custom_pg_type" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT typdefault FROM local_table_join.custom_pg_type WHERE true
-ERROR:  direct joins between distributed and local tables are not supported
-HINT:  Use CTE's or subqueries to select from local tables and use them in joins
+ typdefault 
+------------
+(0 rows)
+
 -- correlated sublinks are not yet supported because of #4470, unless we convert not-correlated table
 SELECT COUNT(*) FROM distributed_table d1 JOIN postgres_table using(key)
 WHERE d1.key IN (SELECT key FROM distributed_table WHERE d1.key = key and key = 5);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table d1 JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE (d1.key OPERATOR(pg_catalog.=) ANY (SELECT distributed_table.key FROM local_table_join.distributed_table WHERE ((d1.key OPERATOR(pg_catalog.=) distributed_table.key) AND (distributed_table.key OPERATOR(pg_catalog.=) 5))))
  count 
 -------
    100
 (1 row)
 
 set citus.local_table_join_policy to 'prefer-distributed';
 SELECT COUNT(*) FROM distributed_table d1 JOIN postgres_table using(key)
 WHERE d1.key IN (SELECT key FROM distributed_table WHERE d1.key = key and key = 5);
-DEBUG:  Wrapping relation "distributed_table" "d1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table d1 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT d1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) d1_1) d1 JOIN local_table_join.postgres_table USING (key)) WHERE (d1.key OPERATOR(pg_catalog.=) ANY (SELECT distributed_table.key FROM local_table_join.distributed_table WHERE ((d1.key OPERATOR(pg_catalog.=) distributed_table.key) AND (distributed_table.key OPERATOR(pg_catalog.=) 5))))
-ERROR:  direct joins between distributed and local tables are not supported
-HINT:  Use CTE's or subqueries to select from local tables and use them in joins
+ count 
+-------
+   100
+(1 row)
+
 set citus.local_table_join_policy to 'auto';
 -- Some more subqueries
 SELECT COUNT(*) FROM distributed_table JOIN postgres_table using(key)
 WHERE distributed_table.key IN (SELECT key FROM distributed_table WHERE key = 5);
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE (distributed_table.key OPERATOR(pg_catalog.=) ANY (SELECT distributed_table_1.key FROM local_table_join.distributed_table distributed_table_1 WHERE (distributed_table_1.key OPERATOR(pg_catalog.=) 5)))
  count 
 -------
    100
 (1 row)
 
 SELECT COUNT(*) FROM distributed_table JOIN postgres_table using(key)
 WHERE distributed_table.key IN (SELECT key FROM distributed_table WHERE key = 5) AND distributed_table.key = 5;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.postgres_table WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_table_join.distributed_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) WHERE ((distributed_table.key OPERATOR(pg_catalog.=) ANY (SELECT distributed_table_1.key FROM local_table_join.distributed_table distributed_table_1 WHERE (distributed_table_1.key OPERATOR(pg_catalog.=) 5))) AND (distributed_table.key OPERATOR(pg_catalog.=) 5))
  count 
 -------
    100
 (1 row)
 
 SELECT COUNT(*) FROM distributed_table_pkey JOIN postgres_table using(key)
 WHERE distributed_table_pkey.key IN (SELECT key FROM distributed_table_pkey WHERE key = 5);
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.postgres_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) ANY (SELECT distributed_table_pkey_1.key FROM local_table_join.distributed_table_pkey distributed_table_pkey_1 WHERE (distributed_table_pkey_1.key OPERATOR(pg_catalog.=) 5)))
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.postgres_table USING (key)) WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) ANY (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)))
  count 
 -------
    100
 (1 row)
 
 SELECT COUNT(*) FROM distributed_table_pkey JOIN postgres_table using(key)
 WHERE distributed_table_pkey.key IN (SELECT key FROM distributed_table_pkey WHERE key = 5) AND distributed_table_pkey.key = 5;
-DEBUG:  Wrapping relation "distributed_table_pkey" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.postgres_table USING (key)) WHERE ((distributed_table_pkey.key OPERATOR(pg_catalog.=) ANY (SELECT distributed_table_pkey_1.key FROM local_table_join.distributed_table_pkey distributed_table_pkey_1 WHERE (distributed_table_pkey_1.key OPERATOR(pg_catalog.=) 5))) AND (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5))
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM local_table_join.distributed_table_pkey WHERE (key OPERATOR(pg_catalog.=) 5)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed_table_pkey_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_pkey_1) distributed_table_pkey JOIN local_table_join.postgres_table USING (key)) WHERE ((distributed_table_pkey.key OPERATOR(pg_catalog.=) ANY (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer))) AND (distributed_table_pkey.key OPERATOR(pg_catalog.=) 5))
  count 
 -------
    100
 (1 row)
 
 -- issue 4682
 create table tbl1 (a int, b int, c int, d int);
 INSERT INTO tbl1 SELECT i,i,i,i FROM generate_series(1,10) i;
 create table custom_pg_operator(oprname text);
 INSERT INTO custom_pg_operator values('a');
@@ -1493,30 +1111,22 @@
   custom_pg_operator
   inner join tbl1 on (select 1 from custom_pg_type) >= d
   left join pg_dist_rebalance_strategy on 'by_shard_count' = name
 where a + b + c > 0;
  count 
 -------
      1
 (1 row)
 
 select create_distributed_table('tbl1', 'a');
-NOTICE:  Copying data from local table...
-DEBUG:  Copied 10 rows
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$local_table_join.tbl1$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- there is a different output in pg11 and in this query the debug messages are not
 -- as important as the others so we use notice
 set client_min_messages to NOTICE;
 select COUNT(*) from
   custom_pg_operator
   inner join tbl1 on (select 1 from custom_pg_type) >= d
   left join pg_dist_rebalance_strategy on 'by_shard_count' = name
 where a + b + c > 0;
  count 
 -------
@@ -1531,62 +1141,32 @@
 -- make sure all the followings give the same result as postgres tables.
 SELECT 1 AS res FROM table2 RIGHT JOIN (SELECT 1 FROM table1, table2) AS sub1 ON false;
  res 
 -----
    1
 (1 row)
 
 SET client_min_messages to DEBUG1;
 BEGIN;
 SELECT create_distributed_table('table1', 'a');
-NOTICE:  Copying data from local table...
-DEBUG:  Copied 1 rows
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$local_table_join.table1$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SELECT 1 AS res FROM table2 RIGHT JOIN (SELECT 1 FROM table1, table2) AS sub1 ON false;
-DEBUG:  Wrapping relation "table2" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_table_join.table2 WHERE true
-DEBUG:  Wrapping relation "table2" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT NULL::integer AS "dummy-1" FROM local_table_join.table2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT 1 AS res FROM ((SELECT NULL::integer AS a FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) table2_1) table2 RIGHT JOIN (SELECT 1 FROM local_table_join.table1, (SELECT NULL::integer AS a FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) table2_2) table2_1) sub1("?column?") ON (false))
- res
----------------------------------------------------------------------
-   1
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 ROLLBACK;
 BEGIN;
 SELECT create_distributed_table('table2', 'a');
-NOTICE:  Copying data from local table...
-DEBUG:  Copied 1 rows
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$local_table_join.table2$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- currently not supported
 SELECT 1 AS res FROM table2 RIGHT JOIN (SELECT 1 FROM table1, table2) AS sub1 ON false;
-DEBUG:  Wrapping relation "table1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_table_join.table1 WHERE true
-DEBUG:  generating subplan XXX_2 for subquery SELECT 1 FROM (SELECT NULL::integer AS a FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) table1_1) table1, local_table_join.table2
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT 1 AS res FROM (local_table_join.table2 RIGHT JOIN (SELECT intermediate_result."?column?" FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result("?column?" integer)) sub1("?column?") ON (false))
-ERROR:  cannot pushdown the subquery
-DETAIL:  Complex subqueries, CTEs and local tables cannot be in the outer part of an outer join with a distributed table
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 ROLLBACK;
 BEGIN;
 SELECT create_reference_table('table1');
 NOTICE:  Copying data from local table...
 DEBUG:  Copied 1 rows
 NOTICE:  copying the data has completed
 DETAIL:  The local data in the table is no longer visible, but is still on disk.
 HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$local_table_join.table1$$)
  create_reference_table 
 ------------------------
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/local_dist_join_mixed.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/local_dist_join_mixed.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/local_dist_join_mixed.out.modified	2022-11-09 13:38:54.059312289 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/local_dist_join_mixed.out.modified	2022-11-09 13:38:54.069312289 +0300
@@ -13,549 +13,396 @@
 ALTER TABLE local DROP column key3;
 ALTER TABLE distributed DROP column key;
 -- these above restrictions brought us to the following schema
 SELECT create_reference_table('reference');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 SELECT create_distributed_table('distributed', 'id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 INSERT INTO distributed SELECT i,  i::text, now() FROM generate_series(0,100)i;
 INSERT INTO reference SELECT i,  i::text FROM generate_series(0,100)i;
 INSERT INTO local SELECT i,  i::text FROM generate_series(0,100)i;
 SET client_min_messages to DEBUG1;
 -- very simple 1-1 Joins
 SELECT count(*) FROM distributed JOIN local USING (id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id))
  count 
 -------
    101
 (1 row)
 
 SELECT count(*) FROM distributed JOIN local ON (name = title);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON ((distributed.name OPERATOR(pg_catalog.=) local.title)))
  count 
 -------
    101
 (1 row)
 
 SELECT count(*) FROM distributed d1 JOIN local ON (name = d1.id::text);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) local_1) local ON ((d1.name OPERATOR(pg_catalog.=) (d1.id)::text)))
  count 
 -------
  10201
 (1 row)
 
 SELECT count(*) FROM distributed d1 JOIN local ON (name = d1.id::text AND d1.id < local.title::int);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer))))
  count 
 -------
   5050
 (1 row)
 
 SELECT count(*) FROM distributed d1 JOIN local ON (name = d1.id::text AND d1.id < local.title::int) WHERE d1.id = 1;
-DEBUG:  Wrapping relation "distributed" "d1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, name FROM local_dist_join_mixed.distributed d1 WHERE ((name OPERATOR(pg_catalog.=) (id)::text) AND (id OPERATOR(pg_catalog.=) 1))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS "dummy-1", d1_1.id, d1_1.name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id, intermediate_result.name FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, name text)) d1_1) d1 JOIN local_dist_join_mixed.local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE (d1.id OPERATOR(pg_catalog.=) 1)
  count 
 -------
     99
 (1 row)
 
 SELECT count(*) FROM distributed JOIN local USING (id) WHERE false;
-DEBUG:  Wrapping relation "distributed" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.distributed WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS "dummy-1", distributed_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) distributed_1) distributed JOIN local_dist_join_mixed.local USING (id)) WHERE false
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM distributed d1 JOIN local ON (name = d1.id::text AND d1.id < local.title::int) WHERE d1.id = 1 OR True;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE ((d1.id OPERATOR(pg_catalog.=) 1) OR true)
  count 
 -------
   5050
 (1 row)
 
 SELECT count(*) FROM distributed d1 JOIN local ON (name::int + local.id > d1.id AND d1.id < local.title::int) WHERE d1.id = 1;
-DEBUG:  Wrapping relation "distributed" "d1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, name FROM local_dist_join_mixed.distributed d1 WHERE (id OPERATOR(pg_catalog.=) 1)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS "dummy-1", d1_1.id, d1_1.name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id, intermediate_result.name FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, name text)) d1_1) d1 JOIN local_dist_join_mixed.local ON (((((d1.name)::integer OPERATOR(pg_catalog.+) local.id) OPERATOR(pg_catalog.>) d1.id) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE (d1.id OPERATOR(pg_catalog.=) 1)
  count 
 -------
     99
 (1 row)
 
 SELECT count(*) FROM distributed JOIN local ON (hashtext(name) = hashtext(title));
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON ((hashtext(distributed.name) OPERATOR(pg_catalog.=) hashtext(local.title))))
  count 
 -------
    101
 (1 row)
 
 SELECT hashtext(local.id::text) FROM distributed JOIN local ON (hashtext(name) = hashtext(title)) ORDER BY 1 LIMIT 4;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT hashtext((local.id)::text) AS hashtext FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) local_1) local ON ((hashtext(distributed.name) OPERATOR(pg_catalog.=) hashtext(local.title)))) ORDER BY (hashtext((local.id)::text)) LIMIT 4
-DEBUG:  push down of limit count: 4
   hashtext   
 -------------
  -2114455578
  -2097988278
  -1997006946
  -1985772843
 (4 rows)
 
 SELECT '' as "xxx", local.*, 'xxx' as "test" FROM distributed JOIN local ON (hashtext(name) = hashtext(title)) ORDER BY 1,2,3 LIMIT 4;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT ''::text AS xxx, local.id, local.title, 'xxx'::text AS test FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) local_1) local ON ((hashtext(distributed.name) OPERATOR(pg_catalog.=) hashtext(local.title)))) ORDER BY ''::text, local.id, local.title LIMIT 4
-DEBUG:  push down of limit count: 4
  xxx | id | title | test 
 -----+----+-------+------
      |  0 | 0     | xxx
      |  1 | 1     | xxx
      |  2 | 2     | xxx
      |  3 | 3     | xxx
 (4 rows)
 
 SELECT local.title, count(*) FROM distributed JOIN local USING (id) GROUP BY 1 ORDER BY 1, 2 DESC LIMIT 5;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT local.title, count(*) AS count FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) local_1) local USING (id)) GROUP BY local.title ORDER BY local.title, (count(*)) DESC LIMIT 5
  title | count 
 -------+-------
  0     |     1
  1     |     1
  10    |     1
  100   |     1
  11    |     1
 (5 rows)
 
 SELECT distributed.id as id1, local.id  as id2 FROM distributed JOIN local USING(id) ORDER BY distributed.id + local.id LIMIT 5;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT distributed.id AS id1, local.id AS id2 FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id)) ORDER BY (distributed.id OPERATOR(pg_catalog.+) local.id) LIMIT 5
-DEBUG:  push down of limit count: 5
  id1 | id2 
 -----+-----
    0 |   0
    1 |   1
    2 |   2
    3 |   3
    4 |   4
 (5 rows)
 
 SELECT distributed.id as id1, local.id  as id2, count(*) FROM distributed JOIN local USING(id) GROUP BY distributed.id, local.id ORDER BY 1,2 LIMIT 5;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT distributed.id AS id1, local.id AS id2, count(*) AS count FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id)) GROUP BY distributed.id, local.id ORDER BY distributed.id, local.id LIMIT 5
-DEBUG:  push down of limit count: 5
  id1 | id2 | count 
 -----+-----+-------
    0 |   0 |     1
    1 |   1 |     1
    2 |   2 |     1
    3 |   3 |     1
    4 |   4 |     1
 (5 rows)
 
 -- basic subqueries that cannot be pulled up
 SELECT count(*) FROM (SELECT *, random() FROM distributed) as d1 JOIN local USING (id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id))
  count 
 -------
    101
 (1 row)
 
 SELECT count(*) FROM (SELECT *, random() FROM distributed) as d1  JOIN local ON (name = title);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON ((d1.name OPERATOR(pg_catalog.=) local.title)))
  count 
 -------
    101
 (1 row)
 
 SELECT count(*) FROM (SELECT *, random() FROM distributed) as d1  JOIN local ON (name = d1.id::text);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) local_1) local ON ((d1.name OPERATOR(pg_catalog.=) (d1.id)::text)))
  count 
 -------
  10201
 (1 row)
 
 SELECT count(*) FROM (SELECT *, random() FROM distributed) as d1  JOIN local ON (name = d1.id::text AND d1.id < local.title::int);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer))))
  count 
 -------
   5050
 (1 row)
 
 SELECT count(*) FROM (SELECT *, random() FROM distributed) as d1  JOIN local ON (name = d1.id::text AND d1.id < local.title::int) WHERE d1.id = 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE (d1.id OPERATOR(pg_catalog.=) 1)
  count 
 -------
     99
 (1 row)
 
 SELECT count(*) FROM (SELECT *, random() FROM distributed) as d1  JOIN local ON (name = d1.id::text AND d1.id < local.title::int) WHERE d1.id = 1 AND false;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE ((d1.id OPERATOR(pg_catalog.=) 1) AND false)
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM (SELECT *, random() FROM distributed) as d1  JOIN local ON (name = d1.id::text AND d1.id < local.title::int) WHERE d1.id = 1 OR true;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE ((d1.id OPERATOR(pg_catalog.=) 1) OR true)
  count 
 -------
   5050
 (1 row)
 
 -- pull up subqueries as they are pretty simple, local table should be recursively planned
 SELECT count(*) FROM (SELECT * FROM distributed) as d1 JOIN local USING (id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id))
  count 
 -------
    101
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed) as d1  JOIN local ON (name = title);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON ((d1.name OPERATOR(pg_catalog.=) local.title)))
  count 
 -------
    101
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed) as d1  JOIN local ON (name = d1.id::text);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) local_1) local ON ((d1.name OPERATOR(pg_catalog.=) (d1.id)::text)))
  count 
 -------
  10201
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed) as d1  JOIN local ON (name = d1.id::text AND d1.id < local.title::int);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer))))
  count 
 -------
   5050
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed) as d1  JOIN local ON (name = d1.id::text AND d1.id < local.title::int) WHERE d1.id = 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE (d1.id OPERATOR(pg_catalog.=) 1)
  count 
 -------
     99
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed) as d1  JOIN local ON (name = d1.id::text AND d1.id < local.title::int) WHERE d1.id = 1 AND false;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE ((d1.id OPERATOR(pg_catalog.=) 1) AND false)
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed) as d1  JOIN local ON (name = d1.id::text AND d1.id < local.title::int) WHERE d1.id = 1 OR true;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE ((d1.id OPERATOR(pg_catalog.=) 1) OR true)
  count 
 -------
   5050
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed WHERE id = 2) as d1  JOIN local ON (name = d1.id::text AND d1.id < local.title::int) WHERE d1.id = 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed WHERE (distributed.id OPERATOR(pg_catalog.=) 2)) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE (d1.id OPERATOR(pg_catalog.=) 1)
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM (SELECT * FROM distributed WHERE false) as d1  JOIN local ON (name = d1.id::text AND d1.id < local.title::int) WHERE d1.id = 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT title FROM local_dist_join_mixed.local WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed WHERE false) d1 JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(title text)) local_1) local ON (((d1.name OPERATOR(pg_catalog.=) (d1.id)::text) AND (d1.id OPERATOR(pg_catalog.<) (local.title)::integer)))) WHERE (d1.id OPERATOR(pg_catalog.=) 1)
  count 
 -------
      0
 (1 row)
 
 -- TEMPORARY table
 CREATE TEMPORARY TABLE temp_local AS SELECT * FROM local;
 SELECT count(*) FROM distributed JOIN temp_local USING (id);
-DEBUG:  Wrapping relation "temp_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM temp_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed JOIN (SELECT temp_local_1.id, NULL::text AS title FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) temp_local_1) temp_local USING (id))
  count 
 -------
    101
 (1 row)
 
 -- UNLOGGED table
 CREATE UNLOGGED TABLE unlogged_local AS SELECT * FROM local;
 SELECT count(*) FROM distributed JOIN unlogged_local USING (id);
-DEBUG:  Wrapping relation "unlogged_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.unlogged_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed JOIN (SELECT unlogged_local_1.id, NULL::text AS title FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) unlogged_local_1) unlogged_local USING (id))
  count 
 -------
    101
 (1 row)
 
 -- mat view
 CREATE MATERIALIZED VIEW mat_view AS SELECT * FROM local;
 SELECT count(*) FROM distributed JOIN mat_view USING (id);
-DEBUG:  Wrapping relation "mat_view" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.mat_view WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed JOIN (SELECT mat_view_1.id, NULL::text AS title FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) mat_view_1) mat_view USING (id))
  count 
 -------
    101
 (1 row)
 
 CREATE VIEW local_regular_view AS SELECT * FROM local;
 WARNING:  "view local_regular_view" has dependency to "table local" that is not in Citus' metadata
 DETAIL:  "view local_regular_view" will be created only locally
 HINT:  Distribute "table local" first to distribute "view local_regular_view"
 CREATE VIEW dist_regular_view AS SELECT * FROM distributed;
+WARNING:  "view dist_regular_view" has dependency to "table distributed" that is not in Citus' metadata
+DETAIL:  "view dist_regular_view" will be created only locally
+HINT:  Distribute "table distributed" first to distribute "view dist_regular_view"
 SELECT count(*) FROM distributed JOIN local_regular_view USING (id);
-DEBUG:  generating subplan XXX_1 for subquery SELECT local.id, local.title FROM local_dist_join_mixed.local
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed JOIN (SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) local_regular_view USING (id))
  count 
 -------
    101
 (1 row)
 
 SELECT count(*) FROM local JOIN dist_regular_view USING (id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN (SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed) dist_regular_view USING (id))
  count 
 -------
    101
 (1 row)
 
 SELECT count(*) FROM dist_regular_view JOIN local_regular_view USING (id);
-DEBUG:  generating subplan XXX_1 for subquery SELECT local.id, local.title FROM local_dist_join_mixed.local
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed) dist_regular_view JOIN (SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) local_regular_view USING (id))
  count 
 -------
    101
 (1 row)
 
 -- join alias/table alias
 SELECT COUNT(*) FROM (distributed JOIN local USING (id)) AS t(a,b,c,d) ORDER BY d,c,a,b LIMIT 3;
 ERROR:  column "local.title" must appear in the GROUP BY clause or be used in an aggregate function
 SELECT COUNT(*) FROM (distributed d1(x,y,y1) JOIN local l1(x,t) USING (x)) AS t(a,b,c,d) ORDER BY d,c,a,b LIMIT 3;
 ERROR:  column "l1.t" must appear in the GROUP BY clause or be used in an aggregate function
 -- final queries are pushdown queries
 SELECT sum(d1.id + local.id) FROM distributed d1 JOIN local USING (id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT sum((d1.id OPERATOR(pg_catalog.+) local.id)) AS sum FROM (local_dist_join_mixed.distributed d1 JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id))
   sum  
 -------
  10100
 (1 row)
 
 SELECT sum(d1.id + local.id) OVER (PARTITION BY d1.id) FROM distributed d1 JOIN local USING (id) ORDER BY 1 DESC LIMIT 4;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT sum((d1.id OPERATOR(pg_catalog.+) local.id)) OVER (PARTITION BY d1.id) AS sum FROM (local_dist_join_mixed.distributed d1 JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id)) ORDER BY (sum((d1.id OPERATOR(pg_catalog.+) local.id)) OVER (PARTITION BY d1.id)) DESC LIMIT 4
-DEBUG:  push down of limit count: 4
  sum 
 -----
  200
  198
  196
  194
 (4 rows)
 
 SELECT count(*) FROM distributed d1 JOIN local USING (id) LEFT JOIN distributed d2 USING (id) ORDER BY 1 DESC LIMIT 4;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((local_dist_join_mixed.distributed d1 JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id)) LEFT JOIN local_dist_join_mixed.distributed d2 USING (id)) ORDER BY (count(*)) DESC LIMIT 4
-DEBUG:  push down of limit count: 4
  count 
 -------
    101
 (1 row)
 
 SELECT count(DISTINCT d1.name::int * local.id) FROM distributed d1 JOIN local USING (id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(DISTINCT ((d1.name)::integer OPERATOR(pg_catalog.*) local.id)) AS count FROM (local_dist_join_mixed.distributed d1 JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id))
  count 
 -------
    101
 (1 row)
 
 -- final queries are router queries
 SELECT sum(d1.id + local.id) FROM distributed d1 JOIN local USING (id) WHERE d1.id = 1;
-DEBUG:  Wrapping relation "distributed" "d1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.distributed d1 WHERE (id OPERATOR(pg_catalog.=) 1)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT sum((d1.id OPERATOR(pg_catalog.+) local.id)) AS sum FROM ((SELECT NULL::integer AS "dummy-1", d1_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) d1_1) d1 JOIN local_dist_join_mixed.local USING (id)) WHERE (d1.id OPERATOR(pg_catalog.=) 1)
  sum 
 -----
    2
 (1 row)
 
 SELECT sum(d1.id + local.id) OVER (PARTITION BY d1.id) FROM distributed d1 JOIN local USING (id) WHERE d1.id = 1 ORDER BY 1 DESC LIMIT 4;
-DEBUG:  Wrapping relation "distributed" "d1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.distributed d1 WHERE (id OPERATOR(pg_catalog.=) 1)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT sum((d1.id OPERATOR(pg_catalog.+) local.id)) OVER (PARTITION BY d1.id) AS sum FROM ((SELECT NULL::integer AS "dummy-1", d1_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) d1_1) d1 JOIN local_dist_join_mixed.local USING (id)) WHERE (d1.id OPERATOR(pg_catalog.=) 1) ORDER BY (sum((d1.id OPERATOR(pg_catalog.+) local.id)) OVER (PARTITION BY d1.id)) DESC LIMIT 4
  sum 
 -----
    2
 (1 row)
 
 SELECT count(*) FROM distributed d1 JOIN local USING (id) LEFT JOIN distributed d2 USING (id) WHERE d2.id = 1 ORDER BY 1 DESC LIMIT 4;
-DEBUG:  Wrapping relation "distributed" "d1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.distributed d1 WHERE (id OPERATOR(pg_catalog.=) 1)
-DEBUG:  Wrapping relation "distributed" "d2" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT id FROM local_dist_join_mixed.distributed d2 WHERE (id OPERATOR(pg_catalog.=) 1)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT NULL::integer AS "dummy-1", d1_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) d1_1) d1 JOIN local_dist_join_mixed.local USING (id)) LEFT JOIN (SELECT NULL::integer AS "dummy-1", d2_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) d2_1) d2 USING (id)) WHERE (d2.id OPERATOR(pg_catalog.=) 1) ORDER BY (count(*)) DESC LIMIT 4
  count 
 -------
      1
 (1 row)
 
 -- final queries are pull to coordinator queries
 SELECT sum(d1.id + local.id) OVER (PARTITION BY d1.id + local.id) FROM distributed d1 JOIN local USING (id) ORDER BY 1 DESC LIMIT 4;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT sum((d1.id OPERATOR(pg_catalog.+) local.id)) OVER (PARTITION BY (d1.id OPERATOR(pg_catalog.+) local.id)) AS sum FROM (local_dist_join_mixed.distributed d1 JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id)) ORDER BY (sum((d1.id OPERATOR(pg_catalog.+) local.id)) OVER (PARTITION BY (d1.id OPERATOR(pg_catalog.+) local.id))) DESC LIMIT 4
  sum 
 -----
  200
  198
  196
  194
 (4 rows)
 
 -- nested subqueries
 SELECT
 	count(*)
 FROM
 	(SELECT * FROM (SELECT * FROM distributed) as foo) as bar
 		JOIN
 	local
 		USING(id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT foo.id, foo.name, foo.created_at FROM (SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed) foo) bar JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id))
  count 
 -------
    101
 (1 row)
 
 SELECT
 	count(*)
 FROM
 	(SELECT *, random() FROM (SELECT *, random() FROM distributed) as foo) as bar
 		JOIN
 	local
 		USING(id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT foo.id, foo.name, foo.created_at, foo.random, random() AS random FROM (SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) foo) bar(id, name, created_at, random, random_1) JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id))
  count 
 -------
    101
 (1 row)
 
 SELECT
 	count(*)
 FROM
 	(SELECT *, random() FROM (SELECT *, random() FROM distributed) as foo) as bar
 		JOIN
 	local
 		USING(id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT foo.id, foo.name, foo.created_at, foo.random, random() AS random FROM (SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) foo) bar(id, name, created_at, random, random_1) JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id))
  count 
 -------
    101
 (1 row)
 
 SELECT
 	count(*)
 FROM
 	(SELECT *, random() FROM (SELECT *, random() FROM distributed) as foo) as bar
 		JOIN
 	(SELECT *, random() FROM (SELECT *,random() FROM local) as foo2) as bar2
 		USING(id);
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, title, random() AS random FROM local_dist_join_mixed.local
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT foo.id, foo.name, foo.created_at, foo.random, random() AS random FROM (SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) foo) bar(id, name, created_at, random, random_1) JOIN (SELECT foo2.id, foo2.title, foo2.random, random() AS random FROM (SELECT intermediate_result.id, intermediate_result.title, intermediate_result.random FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text, random double precision)) foo2) bar2(id, title, random, random_1) USING (id))
  count 
 -------
    101
 (1 row)
 
 -- TODO: Unnecessary recursive planning for local
 SELECT
 	count(*)
 FROM
 	(SELECT *, random() FROM (SELECT *, random() FROM distributed LIMIT 1) as foo) as bar
 		JOIN
 	(SELECT *, random() FROM (SELECT *,random() FROM local) as foo2) as bar2
 		USING(id);
-DEBUG:  push down of limit count: 1
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, name, created_at, random() AS random FROM local_dist_join_mixed.distributed LIMIT 1
-DEBUG:  generating subplan XXX_2 for subquery SELECT id, title, random() AS random FROM local_dist_join_mixed.local
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT foo.id, foo.name, foo.created_at, foo.random, random() AS random FROM (SELECT intermediate_result.id, intermediate_result.name, intermediate_result.created_at, intermediate_result.random FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, name text, created_at timestamp with time zone, random double precision)) foo) bar(id, name, created_at, random, random_1) JOIN (SELECT foo2.id, foo2.title, foo2.random, random() AS random FROM (SELECT intermediate_result.id, intermediate_result.title, intermediate_result.random FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text, random double precision)) foo2) bar2(id, title, random, random_1) USING (id))
  count 
 -------
      1
 (1 row)
 
 -- subqueries in WHERE clause
 -- is not colocated, and the JOIN inside as well.
 -- so should be recursively planned twice
 SELECT
 	count(*)
@@ -563,211 +410,169 @@
 	distributed
 WHERE
 	id  > (SELECT
 				count(*)
 			FROM
 				(SELECT *, random() FROM (SELECT *, random() FROM distributed) as foo) as bar
 					JOIN
 				(SELECT *, random() FROM (SELECT *,random() FROM local) as foo2) as bar2
 					USING(id)
 			);
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, title, random() AS random FROM local_dist_join_mixed.local
-DEBUG:  generating subplan XXX_2 for subquery SELECT count(*) AS count FROM ((SELECT foo.id, foo.name, foo.created_at, foo.random, random() AS random FROM (SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed) foo) bar(id, name, created_at, random, random_1) JOIN (SELECT foo2.id, foo2.title, foo2.random, random() AS random FROM (SELECT intermediate_result.id, intermediate_result.title, intermediate_result.random FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text, random double precision)) foo2) bar2(id, title, random, random_1) USING (id))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM local_dist_join_mixed.distributed WHERE (id OPERATOR(pg_catalog.>) (SELECT intermediate_result.count FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(count bigint)))
  count 
 -------
      0
 (1 row)
 
 -- two distributed tables are co-located and JOINed on distribution
 -- key, so should be fine to pushdown
 SELECT
 	count(*)
 FROM
 	distributed d_upper
 WHERE
 	(SELECT
 				bar.id
 			FROM
 				(SELECT *, random() FROM (SELECT *, random() FROM distributed WHERE distributed.id = d_upper.id) as foo) as bar
 					JOIN
 				(SELECT *, random() FROM (SELECT *,random() FROM local) as foo2) as bar2
 					USING(id)
 			) IS NOT NULL;
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, title, random() AS random FROM local_dist_join_mixed.local
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM local_dist_join_mixed.distributed d_upper WHERE ((SELECT bar.id FROM ((SELECT foo.id, foo.name, foo.created_at, foo.random, random() AS random FROM (SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed WHERE (distributed.id OPERATOR(pg_catalog.=) d_upper.id)) foo) bar(id, name, created_at, random, random_1) JOIN (SELECT foo2.id, foo2.title, foo2.random, random() AS random FROM (SELECT intermediate_result.id, intermediate_result.title, intermediate_result.random FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text, random double precision)) foo2) bar2(id, title, random, random_1) USING (id))) IS NOT NULL)
  count 
 -------
    101
 (1 row)
 
 SELECT
 	count(*)
 FROM
 	distributed d_upper
 WHERE
 	(SELECT
 				bar.id
 			FROM
 				(SELECT *, random() FROM (SELECT *, random() FROM distributed WHERE distributed.id = d_upper.id) as foo) as bar
 					JOIN
 				  local as foo
 					USING(id)
 			) IS NOT NULL;
-DEBUG:  Wrapping relation "local" "foo" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local foo WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM local_dist_join_mixed.distributed d_upper WHERE ((SELECT bar.id FROM ((SELECT foo_1.id, foo_1.name, foo_1.created_at, foo_1.random, random() AS random FROM (SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed WHERE (distributed.id OPERATOR(pg_catalog.=) d_upper.id)) foo_1) bar(id, name, created_at, random, random_1) JOIN (SELECT NULL::integer AS "dummy-1", foo_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) foo_1) foo USING (id))) IS NOT NULL)
  count 
 -------
    101
 (1 row)
 
 SELECT
 	count(*)
 FROM
 	distributed d_upper
 WHERE d_upper.id >
 	(SELECT
 				bar.id
 			FROM
 				(SELECT *, random() FROM (SELECT *, random() FROM distributed WHERE distributed.id = d_upper.id) as foo) as bar
 					JOIN
 				  local as foo
 					USING(id)
 			);
-DEBUG:  Wrapping relation "local" "foo" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local foo WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM local_dist_join_mixed.distributed d_upper WHERE (id OPERATOR(pg_catalog.>) (SELECT bar.id FROM ((SELECT foo_1.id, foo_1.name, foo_1.created_at, foo_1.random, random() AS random FROM (SELECT distributed.id, distributed.name, distributed.created_at, random() AS random FROM local_dist_join_mixed.distributed WHERE (distributed.id OPERATOR(pg_catalog.=) d_upper.id)) foo_1) bar(id, name, created_at, random, random_1) JOIN (SELECT NULL::integer AS "dummy-1", foo_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) foo_1) foo USING (id))))
  count 
 -------
      0
 (1 row)
 
 SELECT
 	count(*)
 FROM
 	distributed d_upper
 WHERE
 	(SELECT
 				bar.id
 			FROM
 				(SELECT *, random() FROM (SELECT *, random() FROM distributed WHERE distributed.id = d_upper.id) as foo) as bar
 					JOIN
 				(SELECT *, random() FROM (SELECT *,random() FROM local WHERE d_upper.id = id) as foo2) as bar2
 					USING(id)
 			) IS NOT NULL;
-ERROR:  direct joins between distributed and local tables are not supported
-HINT:  Use CTE's or subqueries to select from local tables and use them in joins
+ count 
+-------
+   101
+(1 row)
+
 -- subqueries in the target list
 -- router, should work
 select (SELECT local.id) FROM local, distributed WHERE distributed.id = 1 LIMIT 1;
-DEBUG:  Wrapping relation "distributed" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.distributed WHERE (id OPERATOR(pg_catalog.=) 1)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT (SELECT local.id) AS id FROM local_dist_join_mixed.local, (SELECT NULL::integer AS "dummy-1", distributed_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) distributed_1) distributed WHERE (distributed.id OPERATOR(pg_catalog.=) 1) LIMIT 1
  id 
 ----
   0
 (1 row)
 
 -- should fail
 select (SELECT local.id) FROM local, distributed WHERE distributed.id != 1 LIMIT 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT (SELECT local.id) AS id FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (distributed.id OPERATOR(pg_catalog.<>) 1) LIMIT 1
-DEBUG:  push down of limit count: 1
  id 
 ----
   0
 (1 row)
 
 -- currently not supported, but should work with https://github.com/citusdata/citus/pull/4360/files
 SELECT
 	name, (SELECT id FROM local WHERE id = e.id)
 FROM
 	distributed e
 ORDER BY 1,2 LIMIT 1;
-ERROR:  direct joins between distributed and local tables are not supported
-HINT:  Use CTE's or subqueries to select from local tables and use them in joins
+ name | id 
+------+----
+ 0    |  0
+(1 row)
+
 -- set operations
 SELECT local.* FROM distributed JOIN local USING (id)
 	EXCEPT
 SELECT local.* FROM distributed JOIN local USING (id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT id, title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  generating subplan XXX_3 for subquery SELECT local.id, local.title FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) local_1) local USING (id))
-DEBUG:  generating subplan XXX_4 for subquery SELECT local.id, local.title FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) local_1) local USING (id))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text) EXCEPT SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)
  id | title 
 ----+-------
 (0 rows)
 
 SELECT distributed.* FROM distributed JOIN local USING (id)
 	EXCEPT
 SELECT distributed.* FROM distributed JOIN local USING (id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  generating subplan XXX_3 for subquery SELECT distributed.id, distributed.name, distributed.created_at FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id))
-DEBUG:  generating subplan XXX_4 for subquery SELECT distributed.id, distributed.name, distributed.created_at FROM (local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local USING (id))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT intermediate_result.id, intermediate_result.name, intermediate_result.created_at FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, name text, created_at timestamp with time zone) EXCEPT SELECT intermediate_result.id, intermediate_result.name, intermediate_result.created_at FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, name text, created_at timestamp with time zone)
  id | name | created_at 
 ----+------+------------
 (0 rows)
 
 SELECT count(*) FROM
 (
 	(SELECT * FROM (SELECT * FROM local) as f JOIN distributed USING (id))
 		UNION ALL
 	(SELECT * FROM (SELECT * FROM local) as f2 JOIN distributed USING (id))
 ) bar;
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, title FROM local_dist_join_mixed.local
-DEBUG:  generating subplan XXX_2 for subquery SELECT id, title FROM local_dist_join_mixed.local
-DEBUG:  generating subplan XXX_3 for subquery SELECT f.id, f.title, distributed.name, distributed.created_at FROM ((SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) f JOIN local_dist_join_mixed.distributed USING (id))
-DEBUG:  generating subplan XXX_4 for subquery SELECT f2.id, f2.title, distributed.name, distributed.created_at FROM ((SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) f2 JOIN local_dist_join_mixed.distributed USING (id))
-DEBUG:  generating subplan XXX_5 for subquery SELECT intermediate_result.id, intermediate_result.title, intermediate_result.name, intermediate_result.created_at FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text, name text, created_at timestamp with time zone) UNION ALL SELECT intermediate_result.id, intermediate_result.title, intermediate_result.name, intermediate_result.created_at FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text, name text, created_at timestamp with time zone)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT intermediate_result.id, intermediate_result.title, intermediate_result.name, intermediate_result.created_at FROM read_intermediate_result('XXX_5'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text, name text, created_at timestamp with time zone)) bar
  count 
 -------
    202
 (1 row)
 
 SELECT count(*) FROM
 (
 	(SELECT * FROM (SELECT distributed.* FROM local JOIN distributed USING (id)) as fo)
 		UNION ALL
 	(SELECT * FROM (SELECT distributed.* FROM local JOIN distributed USING (id)) as ba)
 ) bar;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (SELECT fo.id, fo.name, fo.created_at FROM (SELECT distributed.id, distributed.name, distributed.created_at FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN local_dist_join_mixed.distributed USING (id))) fo UNION ALL SELECT ba.id, ba.name, ba.created_at FROM (SELECT distributed.id, distributed.name, distributed.created_at FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN local_dist_join_mixed.distributed USING (id))) ba) bar
  count 
 -------
    202
 (1 row)
 
 select count(DISTINCT id)
 FROM
 (
 	(SELECT * FROM (SELECT distributed.* FROM local JOIN distributed USING (id)) as fo)
 		UNION ALL
 	(SELECT * FROM (SELECT distributed.* FROM local JOIN distributed USING (id)) as ba)
 ) bar;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(DISTINCT id) AS count FROM (SELECT fo.id, fo.name, fo.created_at FROM (SELECT distributed.id, distributed.name, distributed.created_at FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN local_dist_join_mixed.distributed USING (id))) fo UNION ALL SELECT ba.id, ba.name, ba.created_at FROM (SELECT distributed.id, distributed.name, distributed.created_at FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN local_dist_join_mixed.distributed USING (id))) ba) bar
  count 
 -------
    101
 (1 row)
 
 -- 25 Joins
 select ' select count(*) from distributed ' || string_Agg('INNER
 JOIN local u'|| x::text || ' USING (id)',' ') from
 generate_Series(1,25)x;
                 ?column?                 
@@ -820,71 +625,20 @@
 JOIN local u16 USING (id) INNER
 JOIN local u17 USING (id) INNER
 JOIN local u18 USING (id) INNER
 JOIN local u19 USING (id) INNER
 JOIN local u20 USING (id) INNER
 JOIN local u21 USING (id) INNER
 JOIN local u22 USING (id) INNER
 JOIN local u23 USING (id) INNER
 JOIN local u24 USING (id) INNER
 JOIN local u25 USING (id)
-DEBUG:  Wrapping relation "local" "u1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local u1 WHERE true
-DEBUG:  Wrapping relation "local" "u2" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT id FROM local_dist_join_mixed.local u2 WHERE true
-DEBUG:  Wrapping relation "local" "u3" to a subquery
-DEBUG:  generating subplan XXX_3 for subquery SELECT id FROM local_dist_join_mixed.local u3 WHERE true
-DEBUG:  Wrapping relation "local" "u4" to a subquery
-DEBUG:  generating subplan XXX_4 for subquery SELECT id FROM local_dist_join_mixed.local u4 WHERE true
-DEBUG:  Wrapping relation "local" "u5" to a subquery
-DEBUG:  generating subplan XXX_5 for subquery SELECT id FROM local_dist_join_mixed.local u5 WHERE true
-DEBUG:  Wrapping relation "local" "u6" to a subquery
-DEBUG:  generating subplan XXX_6 for subquery SELECT id FROM local_dist_join_mixed.local u6 WHERE true
-DEBUG:  Wrapping relation "local" "u7" to a subquery
-DEBUG:  generating subplan XXX_7 for subquery SELECT id FROM local_dist_join_mixed.local u7 WHERE true
-DEBUG:  Wrapping relation "local" "u8" to a subquery
-DEBUG:  generating subplan XXX_8 for subquery SELECT id FROM local_dist_join_mixed.local u8 WHERE true
-DEBUG:  Wrapping relation "local" "u9" to a subquery
-DEBUG:  generating subplan XXX_9 for subquery SELECT id FROM local_dist_join_mixed.local u9 WHERE true
-DEBUG:  Wrapping relation "local" "u10" to a subquery
-DEBUG:  generating subplan XXX_10 for subquery SELECT id FROM local_dist_join_mixed.local u10 WHERE true
-DEBUG:  Wrapping relation "local" "u11" to a subquery
-DEBUG:  generating subplan XXX_11 for subquery SELECT id FROM local_dist_join_mixed.local u11 WHERE true
-DEBUG:  Wrapping relation "local" "u12" to a subquery
-DEBUG:  generating subplan XXX_12 for subquery SELECT id FROM local_dist_join_mixed.local u12 WHERE true
-DEBUG:  Wrapping relation "local" "u13" to a subquery
-DEBUG:  generating subplan XXX_13 for subquery SELECT id FROM local_dist_join_mixed.local u13 WHERE true
-DEBUG:  Wrapping relation "local" "u14" to a subquery
-DEBUG:  generating subplan XXX_14 for subquery SELECT id FROM local_dist_join_mixed.local u14 WHERE true
-DEBUG:  Wrapping relation "local" "u15" to a subquery
-DEBUG:  generating subplan XXX_15 for subquery SELECT id FROM local_dist_join_mixed.local u15 WHERE true
-DEBUG:  Wrapping relation "local" "u16" to a subquery
-DEBUG:  generating subplan XXX_16 for subquery SELECT id FROM local_dist_join_mixed.local u16 WHERE true
-DEBUG:  Wrapping relation "local" "u17" to a subquery
-DEBUG:  generating subplan XXX_17 for subquery SELECT id FROM local_dist_join_mixed.local u17 WHERE true
-DEBUG:  Wrapping relation "local" "u18" to a subquery
-DEBUG:  generating subplan XXX_18 for subquery SELECT id FROM local_dist_join_mixed.local u18 WHERE true
-DEBUG:  Wrapping relation "local" "u19" to a subquery
-DEBUG:  generating subplan XXX_19 for subquery SELECT id FROM local_dist_join_mixed.local u19 WHERE true
-DEBUG:  Wrapping relation "local" "u20" to a subquery
-DEBUG:  generating subplan XXX_20 for subquery SELECT id FROM local_dist_join_mixed.local u20 WHERE true
-DEBUG:  Wrapping relation "local" "u21" to a subquery
-DEBUG:  generating subplan XXX_21 for subquery SELECT id FROM local_dist_join_mixed.local u21 WHERE true
-DEBUG:  Wrapping relation "local" "u22" to a subquery
-DEBUG:  generating subplan XXX_22 for subquery SELECT id FROM local_dist_join_mixed.local u22 WHERE true
-DEBUG:  Wrapping relation "local" "u23" to a subquery
-DEBUG:  generating subplan XXX_23 for subquery SELECT id FROM local_dist_join_mixed.local u23 WHERE true
-DEBUG:  Wrapping relation "local" "u24" to a subquery
-DEBUG:  generating subplan XXX_24 for subquery SELECT id FROM local_dist_join_mixed.local u24 WHERE true
-DEBUG:  Wrapping relation "local" "u25" to a subquery
-DEBUG:  generating subplan XXX_25 for subquery SELECT id FROM local_dist_join_mixed.local u25 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((((((((((((((((((((((((local_dist_join_mixed.distributed JOIN (SELECT NULL::integer AS "dummy-1", u1_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u1_1) u1 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u2_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u2_1) u2 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u3_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u3_1) u3 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u4_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u4_1) u4 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u5_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_5'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u5_1) u5 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u6_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_6'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u6_1) u6 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u7_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_7'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u7_1) u7 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u8_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_8'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u8_1) u8 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u9_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_9'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u9_1) u9 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u10_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_10'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u10_1) u10 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u11_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_11'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u11_1) u11 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u12_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_12'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u12_1) u12 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u13_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_13'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u13_1) u13 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u14_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_14'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u14_1) u14 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u15_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_15'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u15_1) u15 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u16_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_16'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u16_1) u16 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u17_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_17'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u17_1) u17 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u18_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_18'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u18_1) u18 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u19_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_19'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u19_1) u19 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u20_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_20'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u20_1) u20 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u21_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_21'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u21_1) u21 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u22_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_22'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u22_1) u22 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u23_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_23'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u23_1) u23 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u24_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_24'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u24_1) u24 USING (id)) JOIN (SELECT NULL::integer AS "dummy-1", u25_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_25'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) u25_1) u25 USING (id))
  count 
 -------
    101
 (1 row)
 
 select ' select count(*) from distributed ' || string_Agg('INNER
 JOIN local u'|| x::text || ' ON (false)',' ') from
 generate_Series(1,25)x;
                 ?column?                 
 -----------------------------------------
@@ -936,23 +690,20 @@
 JOIN local u16 ON (false) INNER
 JOIN local u17 ON (false) INNER
 JOIN local u18 ON (false) INNER
 JOIN local u19 ON (false) INNER
 JOIN local u20 ON (false) INNER
 JOIN local u21 ON (false) INNER
 JOIN local u22 ON (false) INNER
 JOIN local u23 ON (false) INNER
 JOIN local u24 ON (false) INNER
 JOIN local u25 ON (false)
-DEBUG:  Wrapping relation "distributed" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((((((((((((((((((((((((((SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) distributed_1) distributed JOIN local_dist_join_mixed.local u1 ON (false)) JOIN local_dist_join_mixed.local u2 ON (false)) JOIN local_dist_join_mixed.local u3 ON (false)) JOIN local_dist_join_mixed.local u4 ON (false)) JOIN local_dist_join_mixed.local u5 ON (false)) JOIN local_dist_join_mixed.local u6 ON (false)) JOIN local_dist_join_mixed.local u7 ON (false)) JOIN local_dist_join_mixed.local u8 ON (false)) JOIN local_dist_join_mixed.local u9 ON (false)) JOIN local_dist_join_mixed.local u10 ON (false)) JOIN local_dist_join_mixed.local u11 ON (false)) JOIN local_dist_join_mixed.local u12 ON (false)) JOIN local_dist_join_mixed.local u13 ON (false)) JOIN local_dist_join_mixed.local u14 ON (false)) JOIN local_dist_join_mixed.local u15 ON (false)) JOIN local_dist_join_mixed.local u16 ON (false)) JOIN local_dist_join_mixed.local u17 ON (false)) JOIN local_dist_join_mixed.local u18 ON (false)) JOIN local_dist_join_mixed.local u19 ON (false)) JOIN local_dist_join_mixed.local u20 ON (false)) JOIN local_dist_join_mixed.local u21 ON (false)) JOIN local_dist_join_mixed.local u22 ON (false)) JOIN local_dist_join_mixed.local u23 ON (false)) JOIN local_dist_join_mixed.local u24 ON (false)) JOIN local_dist_join_mixed.local u25 ON (false))
  count 
 -------
      0
 (1 row)
 
 select ' select count(*) from local ' || string_Agg('INNER
 JOIN distributed u'|| x::text || ' USING (id)',' ') from
 generate_Series(1,25)x;
                ?column?                
 ---------------------------------------
@@ -1004,23 +755,20 @@
 JOIN distributed u16 USING (id) INNER
 JOIN distributed u17 USING (id) INNER
 JOIN distributed u18 USING (id) INNER
 JOIN distributed u19 USING (id) INNER
 JOIN distributed u20 USING (id) INNER
 JOIN distributed u21 USING (id) INNER
 JOIN distributed u22 USING (id) INNER
 JOIN distributed u23 USING (id) INNER
 JOIN distributed u24 USING (id) INNER
 JOIN distributed u25 USING (id)
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((((((((((((((((((((((((((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN local_dist_join_mixed.distributed u1 USING (id)) JOIN local_dist_join_mixed.distributed u2 USING (id)) JOIN local_dist_join_mixed.distributed u3 USING (id)) JOIN local_dist_join_mixed.distributed u4 USING (id)) JOIN local_dist_join_mixed.distributed u5 USING (id)) JOIN local_dist_join_mixed.distributed u6 USING (id)) JOIN local_dist_join_mixed.distributed u7 USING (id)) JOIN local_dist_join_mixed.distributed u8 USING (id)) JOIN local_dist_join_mixed.distributed u9 USING (id)) JOIN local_dist_join_mixed.distributed u10 USING (id)) JOIN local_dist_join_mixed.distributed u11 USING (id)) JOIN local_dist_join_mixed.distributed u12 USING (id)) JOIN local_dist_join_mixed.distributed u13 USING (id)) JOIN local_dist_join_mixed.distributed u14 USING (id)) JOIN local_dist_join_mixed.distributed u15 USING (id)) JOIN local_dist_join_mixed.distributed u16 USING (id)) JOIN local_dist_join_mixed.distributed u17 USING (id)) JOIN local_dist_join_mixed.distributed u18 USING (id)) JOIN local_dist_join_mixed.distributed u19 USING (id)) JOIN local_dist_join_mixed.distributed u20 USING (id)) JOIN local_dist_join_mixed.distributed u21 USING (id)) JOIN local_dist_join_mixed.distributed u22 USING (id)) JOIN local_dist_join_mixed.distributed u23 USING (id)) JOIN local_dist_join_mixed.distributed u24 USING (id)) JOIN local_dist_join_mixed.distributed u25 USING (id))
  count 
 -------
    101
 (1 row)
 
 select ' select count(*) from local ' || string_Agg('INNER
 JOIN distributed u'|| x::text || ' ON (false)',' ') from
 generate_Series(1,25)x;
                ?column?                
 ---------------------------------------
@@ -1072,156 +820,89 @@
 JOIN distributed u16 ON (false) INNER
 JOIN distributed u17 ON (false) INNER
 JOIN distributed u18 ON (false) INNER
 JOIN distributed u19 ON (false) INNER
 JOIN distributed u20 ON (false) INNER
 JOIN distributed u21 ON (false) INNER
 JOIN distributed u22 ON (false) INNER
 JOIN distributed u23 ON (false) INNER
 JOIN distributed u24 ON (false) INNER
 JOIN distributed u25 ON (false)
-DEBUG:  Wrapping relation "distributed" "u1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u1 WHERE false
-DEBUG:  Wrapping relation "distributed" "u2" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u2 WHERE false
-DEBUG:  Wrapping relation "distributed" "u3" to a subquery
-DEBUG:  generating subplan XXX_3 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u3 WHERE false
-DEBUG:  Wrapping relation "distributed" "u4" to a subquery
-DEBUG:  generating subplan XXX_4 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u4 WHERE false
-DEBUG:  Wrapping relation "distributed" "u5" to a subquery
-DEBUG:  generating subplan XXX_5 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u5 WHERE false
-DEBUG:  Wrapping relation "distributed" "u6" to a subquery
-DEBUG:  generating subplan XXX_6 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u6 WHERE false
-DEBUG:  Wrapping relation "distributed" "u7" to a subquery
-DEBUG:  generating subplan XXX_7 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u7 WHERE false
-DEBUG:  Wrapping relation "distributed" "u8" to a subquery
-DEBUG:  generating subplan XXX_8 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u8 WHERE false
-DEBUG:  Wrapping relation "distributed" "u9" to a subquery
-DEBUG:  generating subplan XXX_9 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u9 WHERE false
-DEBUG:  Wrapping relation "distributed" "u10" to a subquery
-DEBUG:  generating subplan XXX_10 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u10 WHERE false
-DEBUG:  Wrapping relation "distributed" "u11" to a subquery
-DEBUG:  generating subplan XXX_11 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u11 WHERE false
-DEBUG:  Wrapping relation "distributed" "u12" to a subquery
-DEBUG:  generating subplan XXX_12 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u12 WHERE false
-DEBUG:  Wrapping relation "distributed" "u13" to a subquery
-DEBUG:  generating subplan XXX_13 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u13 WHERE false
-DEBUG:  Wrapping relation "distributed" "u14" to a subquery
-DEBUG:  generating subplan XXX_14 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u14 WHERE false
-DEBUG:  Wrapping relation "distributed" "u15" to a subquery
-DEBUG:  generating subplan XXX_15 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u15 WHERE false
-DEBUG:  Wrapping relation "distributed" "u16" to a subquery
-DEBUG:  generating subplan XXX_16 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u16 WHERE false
-DEBUG:  Wrapping relation "distributed" "u17" to a subquery
-DEBUG:  generating subplan XXX_17 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u17 WHERE false
-DEBUG:  Wrapping relation "distributed" "u18" to a subquery
-DEBUG:  generating subplan XXX_18 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u18 WHERE false
-DEBUG:  Wrapping relation "distributed" "u19" to a subquery
-DEBUG:  generating subplan XXX_19 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u19 WHERE false
-DEBUG:  Wrapping relation "distributed" "u20" to a subquery
-DEBUG:  generating subplan XXX_20 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u20 WHERE false
-DEBUG:  Wrapping relation "distributed" "u21" to a subquery
-DEBUG:  generating subplan XXX_21 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u21 WHERE false
-DEBUG:  Wrapping relation "distributed" "u22" to a subquery
-DEBUG:  generating subplan XXX_22 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u22 WHERE false
-DEBUG:  Wrapping relation "distributed" "u23" to a subquery
-DEBUG:  generating subplan XXX_23 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u23 WHERE false
-DEBUG:  Wrapping relation "distributed" "u24" to a subquery
-DEBUG:  generating subplan XXX_24 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u24 WHERE false
-DEBUG:  Wrapping relation "distributed" "u25" to a subquery
-DEBUG:  generating subplan XXX_25 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.distributed u25 WHERE false
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((((((((((((((((((((((((local_dist_join_mixed.local JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u1_1) u1 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u2_1) u2 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u3_1) u3 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u4_1) u4 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_5'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u5_1) u5 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_6'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u6_1) u6 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_7'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u7_1) u7 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_8'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u8_1) u8 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_9'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u9_1) u9 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_10'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u10_1) u10 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_11'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u11_1) u11 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_12'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u12_1) u12 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_13'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u13_1) u13 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_14'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u14_1) u14 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_15'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u15_1) u15 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_16'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u16_1) u16 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_17'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u17_1) u17 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_18'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u18_1) u18 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_19'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u19_1) u19 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_20'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u20_1) u20 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_21'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u21_1) u21 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_22'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u22_1) u22 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_23'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u23_1) u23 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_24'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u24_1) u24 ON (false)) JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_25'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) u25_1) u25 ON (false))
  count 
 -------
      0
 (1 row)
 
 -- lateral joins
 SELECT COUNT(*) FROM (VALUES (1), (2), (3))  as f(x) LATERAL JOIN (SELECT * FROM local WHERE id  =  x) as bar;
 ERROR:  syntax error at or near "LATERAL"
 SELECT COUNT(*) FROM local JOIN LATERAL (SELECT * FROM distributed WHERE local.id = distributed.id) as foo ON (true);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN LATERAL (SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo ON (true))
  count 
 -------
    101
 (1 row)
 
 SELECT COUNT(*) FROM local JOIN LATERAL (SELECT * FROM distributed WHERE local.id > distributed.id) as foo ON (true);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN LATERAL (SELECT distributed.id, distributed.name, distributed.created_at FROM local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.>) distributed.id)) foo ON (true))
  count 
 -------
   5050
 (1 row)
 
 SELECT COUNT(*) FROM distributed JOIN LATERAL (SELECT * FROM local WHERE local.id = distributed.id) as foo ON (true);
-ERROR:  direct joins between distributed and local tables are not supported
-HINT:  Use CTE's or subqueries to select from local tables and use them in joins
+ count 
+-------
+   101
+(1 row)
+
 SELECT COUNT(*) FROM distributed JOIN LATERAL (SELECT * FROM local WHERE local.id > distributed.id) as foo ON (true);
-ERROR:  direct joins between distributed and local tables are not supported
-HINT:  Use CTE's or subqueries to select from local tables and use them in joins
+ count 
+-------
+  5050
+(1 row)
+
 SELECT count(*) FROM distributed CROSS JOIN local;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed CROSS JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) local_1) local)
  count 
 -------
  10201
 (1 row)
 
 SELECT count(*) FROM distributed CROSS JOIN local WHERE distributed.id = 1;
-DEBUG:  Wrapping relation "distributed" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.distributed WHERE (id OPERATOR(pg_catalog.=) 1)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS "dummy-1", distributed_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) distributed_1) distributed CROSS JOIN local_dist_join_mixed.local) WHERE (distributed.id OPERATOR(pg_catalog.=) 1)
  count 
 -------
    101
 (1 row)
 
 -- w count(*) it works fine as PG ignores the  inner tables
 SELECT count(*) FROM distributed LEFT JOIN local USING (id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed LEFT JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) local_1) local USING (id))
  count 
 -------
    101
 (1 row)
 
 SELECT count(*) FROM local LEFT JOIN distributed USING (id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local LEFT JOIN local_dist_join_mixed.distributed USING (id))
  count 
 -------
    101
 (1 row)
 
 SELECT id, name FROM distributed LEFT JOIN local USING (id) ORDER BY 1 LIMIT 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT distributed.id, distributed.name FROM (local_dist_join_mixed.distributed LEFT JOIN (SELECT NULL::integer AS "dummy-1", NULL::bigint AS id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) local_1) local USING (id)) ORDER BY distributed.id LIMIT 1
-DEBUG:  push down of limit count: 1
  id | name 
 ----+------
   0 | 0
 (1 row)
 
 SELECT id, name FROM local LEFT JOIN distributed USING (id) ORDER BY 1 LIMIT 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT local.id, distributed.name FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local LEFT JOIN local_dist_join_mixed.distributed USING (id)) ORDER BY local.id LIMIT 1
-ERROR:  cannot pushdown the subquery
-DETAIL:  Complex subqueries, CTEs and local tables cannot be in the outer part of an outer join with a distributed table
+ id | name 
+----+------
+  0 | 0
+(1 row)
+
  SELECT
         foo1.id
     FROM
  (SELECT local.id, local.title FROM local, distributed WHERE local.id = distributed.id ) as foo9,
  (SELECT local.id, local.title FROM local, distributed WHERE local.id = distributed.id ) as foo8,
  (SELECT local.id, local.title FROM local, distributed WHERE local.id = distributed.id ) as foo7,
  (SELECT local.id, local.title FROM local, distributed WHERE local.id = distributed.id ) as foo6,
  (SELECT local.id, local.title FROM local, distributed WHERE local.id = distributed.id ) as foo5,
  (SELECT local.id, local.title FROM local, distributed WHERE local.id = distributed.id ) as foo4,
  (SELECT local.id, local.title FROM local, distributed WHERE local.id = distributed.id ) as foo3,
@@ -1233,41 +914,20 @@
   foo1.id =  foo8.id AND
   foo1.id =  foo7.id AND
   foo1.id =  foo6.id AND
   foo1.id =  foo5.id AND
   foo1.id =  foo4.id AND
   foo1.id =  foo3.id AND
   foo1.id =  foo2.id AND
   foo1.id =  foo10.id AND
   foo1.id =  foo1.id
 ORDER BY 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_3 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_4 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_5 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_6 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_7 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_8 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_9 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_10 for subquery SELECT id FROM local_dist_join_mixed.local WHERE (id IS NOT NULL)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT foo1.id FROM (SELECT local.id, local.title FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo9, (SELECT local.id, local.title FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo8, (SELECT local.id, local.title FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo7, (SELECT local.id, local.title FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo6, (SELECT local.id, local.title FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_5'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo5, (SELECT local.id, local.title FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_6'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo4, (SELECT local.id, local.title FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_7'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo3, (SELECT local.id, local.title FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_8'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo2, (SELECT local.id, local.title FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_9'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo10, (SELECT local.id, local.title FROM (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_10'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local, local_dist_join_mixed.distributed WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo1 WHERE ((foo1.id OPERATOR(pg_catalog.=) foo9.id) AND (foo1.id OPERATOR(pg_catalog.=) foo8.id) AND (foo1.id OPERATOR(pg_catalog.=) foo7.id) AND (foo1.id OPERATOR(pg_catalog.=) foo6.id) AND (foo1.id OPERATOR(pg_catalog.=) foo5.id) AND (foo1.id OPERATOR(pg_catalog.=) foo4.id) AND (foo1.id OPERATOR(pg_catalog.=) foo3.id) AND (foo1.id OPERATOR(pg_catalog.=) foo2.id) AND (foo1.id OPERATOR(pg_catalog.=) foo10.id) AND (foo1.id OPERATOR(pg_catalog.=) foo1.id)) ORDER BY foo1.id
  id  
 -----
    0
    1
    2
    3
    4
    5
    6
    7
@@ -1374,31 +1034,20 @@
 	(SELECT local.id FROM distributed, local WHERE local.id = distributed.id ) as foo3,
 	(SELECT local.id FROM distributed, local WHERE local.id = distributed.id ) as foo4,
 	(SELECT local.id FROM distributed, local WHERE local.id = distributed.id ) as foo5
 WHERE
 	foo1.id = foo4.id AND
 	foo1.id = foo2.id AND
 	foo1.id = foo3.id AND
 	foo1.id = foo4.id AND
 	foo1.id = foo5.id
 ORDER BY 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_3 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_4 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_5 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT foo1.id FROM (SELECT local.id FROM local_dist_join_mixed.distributed, (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo1, (SELECT local.id FROM local_dist_join_mixed.distributed, (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo2, (SELECT local.id FROM local_dist_join_mixed.distributed, (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo3, (SELECT local.id FROM local_dist_join_mixed.distributed, (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo4, (SELECT local.id FROM local_dist_join_mixed.distributed, (SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_5'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local WHERE (local.id OPERATOR(pg_catalog.=) distributed.id)) foo5 WHERE ((foo1.id OPERATOR(pg_catalog.=) foo4.id) AND (foo1.id OPERATOR(pg_catalog.=) foo2.id) AND (foo1.id OPERATOR(pg_catalog.=) foo3.id) AND (foo1.id OPERATOR(pg_catalog.=) foo4.id) AND (foo1.id OPERATOR(pg_catalog.=) foo5.id)) ORDER BY foo1.id
  id  
 -----
    0
    1
    2
    3
    4
    5
    6
    7
@@ -1505,104 +1154,70 @@
 	(SELECT local.id FROM distributed, local WHERE local.id = distributed.id  AND distributed.id = 3) as foo3,
 	(SELECT local.id FROM distributed, local WHERE local.id = distributed.id  AND distributed.id = 4) as foo4,
 	(SELECT local.id FROM distributed, local WHERE local.id = distributed.id  AND distributed.id = 5) as foo5
 WHERE
 	foo1.id = foo4.id AND
 	foo1.id = foo2.id AND
 	foo1.id = foo3.id AND
 	foo1.id = foo4.id AND
 	foo1.id = foo5.id
 ORDER BY 1;
-DEBUG:  Wrapping relation "distributed" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.distributed WHERE false
-DEBUG:  generating subplan XXX_2 for subquery SELECT local.id FROM (SELECT NULL::integer AS "dummy-1", distributed_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) distributed_1) distributed, local_dist_join_mixed.local WHERE ((local.id OPERATOR(pg_catalog.=) distributed.id) AND (distributed.id OPERATOR(pg_catalog.=) 1))
-DEBUG:  Wrapping relation "distributed" to a subquery
-DEBUG:  generating subplan XXX_3 for subquery SELECT id FROM local_dist_join_mixed.distributed WHERE false
-DEBUG:  generating subplan XXX_4 for subquery SELECT local.id FROM (SELECT NULL::integer AS "dummy-1", distributed_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) distributed_1) distributed, local_dist_join_mixed.local WHERE ((local.id OPERATOR(pg_catalog.=) distributed.id) AND (distributed.id OPERATOR(pg_catalog.=) 2))
-DEBUG:  Wrapping relation "distributed" to a subquery
-DEBUG:  generating subplan XXX_5 for subquery SELECT id FROM local_dist_join_mixed.distributed WHERE false
-DEBUG:  generating subplan XXX_6 for subquery SELECT local.id FROM (SELECT NULL::integer AS "dummy-1", distributed_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_5'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) distributed_1) distributed, local_dist_join_mixed.local WHERE ((local.id OPERATOR(pg_catalog.=) distributed.id) AND (distributed.id OPERATOR(pg_catalog.=) 3))
-DEBUG:  Wrapping relation "distributed" to a subquery
-DEBUG:  generating subplan XXX_7 for subquery SELECT id FROM local_dist_join_mixed.distributed WHERE false
-DEBUG:  generating subplan XXX_8 for subquery SELECT local.id FROM (SELECT NULL::integer AS "dummy-1", distributed_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_7'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) distributed_1) distributed, local_dist_join_mixed.local WHERE ((local.id OPERATOR(pg_catalog.=) distributed.id) AND (distributed.id OPERATOR(pg_catalog.=) 4))
-DEBUG:  Wrapping relation "distributed" to a subquery
-DEBUG:  generating subplan XXX_9 for subquery SELECT id FROM local_dist_join_mixed.distributed WHERE false
-DEBUG:  generating subplan XXX_10 for subquery SELECT local.id FROM (SELECT NULL::integer AS "dummy-1", distributed_1.id, NULL::text AS name, NULL::timestamp with time zone AS created_at FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_9'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) distributed_1) distributed, local_dist_join_mixed.local WHERE ((local.id OPERATOR(pg_catalog.=) distributed.id) AND (distributed.id OPERATOR(pg_catalog.=) 5))
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT foo1.id FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) foo1, (SELECT intermediate_result.id FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) foo2, (SELECT intermediate_result.id FROM read_intermediate_result('XXX_6'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) foo3, (SELECT intermediate_result.id FROM read_intermediate_result('XXX_8'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) foo4, (SELECT intermediate_result.id FROM read_intermediate_result('XXX_10'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) foo5 WHERE ((foo1.id OPERATOR(pg_catalog.=) foo4.id) AND (foo1.id OPERATOR(pg_catalog.=) foo2.id) AND (foo1.id OPERATOR(pg_catalog.=) foo3.id) AND (foo1.id OPERATOR(pg_catalog.=) foo4.id) AND (foo1.id OPERATOR(pg_catalog.=) foo5.id)) ORDER BY foo1.id
  id 
 ----
 (0 rows)
 
 SELECT
 	count(*)
 FROM
  distributed
 JOIN LATERAL
 	(SELECT
 		*
 	FROM
 		local
 	JOIN
 		distributed d2
 	ON(true)
 		WHERE local.id = distributed.id AND d2.id = local.id) as foo
 ON (true);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (local_dist_join_mixed.distributed JOIN LATERAL (SELECT local.id, local.title, d2.id, d2.name, d2.created_at FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN local_dist_join_mixed.distributed d2 ON (true)) WHERE ((local.id OPERATOR(pg_catalog.=) distributed.id) AND (d2.id OPERATOR(pg_catalog.=) local.id))) foo(id, title, id_1, name, created_at) ON (true))
  count 
 -------
    101
 (1 row)
 
 SELECT local.title, local.title FROM local JOIN distributed USING(id) ORDER BY 1,2 LIMIt 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT local.title, local.title FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) local_1) local JOIN local_dist_join_mixed.distributed USING (id)) ORDER BY local.title, local.title LIMIT 1
-DEBUG:  push down of limit count: 1
  title | title 
 -------+-------
  0     | 0
 (1 row)
 
 SELECT NULL FROM local JOIN distributed USING(id) ORDER BY 1 LIMIt 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT NULL::text FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN local_dist_join_mixed.distributed USING (id)) ORDER BY NULL::text LIMIT 1
-DEBUG:  push down of limit count: 1
  ?column? 
 ----------
  
 (1 row)
 
 SELECT distributed.name, distributed.name,  local.title, local.title FROM local JOIN distributed USING(id) ORDER BY 1,2,3,4 LIMIT 1;
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id, title FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT distributed.name, distributed.name, local.title, local.title FROM ((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", local_1.title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id, intermediate_result.title FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, title text)) local_1) local JOIN local_dist_join_mixed.distributed USING (id)) ORDER BY distributed.name, distributed.name, local.title, local.title LIMIT 1
-DEBUG:  push down of limit count: 1
  name | name | title | title 
 ------+------+-------+-------
  0    | 0    | 0     | 0
 (1 row)
 
 SELECT
 	COUNT(*)
 FROM
 	local
 JOIN
 	distributed
 USING
 	(id)
 JOIN
 	(SELECT  id, NULL, NULL FROM distributed) foo
 USING
 	(id);
-DEBUG:  Wrapping relation "local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT id FROM local_dist_join_mixed.local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((SELECT NULL::integer AS "dummy-1", local_1.id, NULL::integer AS "dummy-3", NULL::text AS title, NULL::integer AS "dummy-5" FROM (SELECT intermediate_result.id FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint)) local_1) local JOIN local_dist_join_mixed.distributed USING (id)) JOIN (SELECT distributed_1.id, NULL::text, NULL::text FROM local_dist_join_mixed.distributed distributed_1) foo(id, "?column?", "?column?_1") USING (id))
  count 
 -------
    101
 (1 row)
 
 SET client_min_messages TO ERROR;
 DROP SCHEMA local_dist_join_mixed CASCADE;
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/citus_local_dist_joins.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/citus_local_dist_joins.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/citus_local_dist_joins.out.modified	2022-11-09 13:38:54.789312286 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/citus_local_dist_joins.out.modified	2022-11-09 13:38:54.799312286 +0300
@@ -12,178 +12,119 @@
 CREATE TABLE postgres_table (key int, value text, value_2 jsonb);
 CREATE TABLE reference_table (key int, value text, value_2 jsonb);
 SELECT create_reference_table('reference_table');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 CREATE TABLE distributed_table (key int, value text, value_2 jsonb);
 SELECT create_distributed_table('distributed_table', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE distributed_table_pkey (key int primary key, value text, value_2 jsonb);
 SELECT create_distributed_table('distributed_table_pkey', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE distributed_table_windex (key int primary key, value text, value_2 jsonb);
 SELECT create_distributed_table('distributed_table_windex', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE UNIQUE INDEX key_index ON distributed_table_windex (key);
 CREATE TABLE distributed_partitioned_table(key int, value text) PARTITION BY RANGE (key);
 CREATE TABLE distributed_partitioned_table_1 PARTITION OF distributed_partitioned_table FOR VALUES FROM (0) TO (50);
 CREATE TABLE distributed_partitioned_table_2 PARTITION OF distributed_partitioned_table FOR VALUES FROM (50) TO (200);
 SELECT create_distributed_table('distributed_partitioned_table', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE local_partitioned_table(key int, value text) PARTITION BY RANGE (key);
 CREATE TABLE local_partitioned_table_1 PARTITION OF local_partitioned_table FOR VALUES FROM (0) TO (50);
 CREATE TABLE local_partitioned_table_2 PARTITION OF local_partitioned_table FOR VALUES FROM (50) TO (200);
 CREATE TABLE distributed_table_composite (key int, value text, value_2 jsonb, primary key (key, value));
 SELECT create_distributed_table('distributed_table_composite', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE MATERIALIZED VIEW mv1 AS SELECT * FROM postgres_table;
 CREATE MATERIALIZED VIEW mv2 AS SELECT * FROM distributed_table;
 -- set log messages to debug1 so that we can see which tables are recursively planned.
 SET client_min_messages TO DEBUG1;
 INSERT INTO postgres_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 INSERT INTO reference_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
 DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO distributed_table_windex SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO distributed_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO distributed_table_pkey SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO distributed_partitioned_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO distributed_table_composite SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
-DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
-DEBUG:  Collecting INSERT ... SELECT results on coordinator
 INSERT INTO local_partitioned_table SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 INSERT INTO citus_local SELECT i, i::varchar(256) FROM generate_series(1, 100) i;
 DEBUG:  distributed INSERT ... SELECT can only select from distributed tables
 DEBUG:  Collecting INSERT ... SELECT results on coordinator
 -- a unique index on key so dist table should be recursively planned
 SELECT count(*) FROM citus_local JOIN distributed_table_windex USING(key);
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local JOIN citus_local_dist_joins.distributed_table_windex USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM citus_local JOIN distributed_table_windex USING(value);
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT value FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS key, citus_local_1.value FROM (SELECT intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(value text)) citus_local_1) citus_local JOIN citus_local_dist_joins.distributed_table_windex USING (value))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM citus_local JOIN distributed_table_windex ON citus_local.key = distributed_table_windex.key;
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local JOIN citus_local_dist_joins.distributed_table_windex ON ((citus_local.key OPERATOR(pg_catalog.=) distributed_table_windex.key)))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM citus_local JOIN distributed_table_windex ON distributed_table_windex.key = 10;
-DEBUG:  Wrapping relation "distributed_table_windex" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.distributed_table_windex WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (citus_local_dist_joins.citus_local JOIN (SELECT distributed_table_windex_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_windex_1) distributed_table_windex ON ((distributed_table_windex.key OPERATOR(pg_catalog.=) 10)))
  count 
 -------
    100
 (1 row)
 
 -- no unique index, citus local table should be recursively planned
 SELECT count(*) FROM citus_local JOIN distributed_table USING(key);
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local JOIN citus_local_dist_joins.distributed_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM citus_local JOIN distributed_table USING(value);
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT value FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS key, citus_local_1.value FROM (SELECT intermediate_result.value FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(value text)) citus_local_1) citus_local JOIN citus_local_dist_joins.distributed_table USING (value))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM citus_local JOIN distributed_table ON citus_local.key = distributed_table.key;
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local JOIN citus_local_dist_joins.distributed_table ON ((citus_local.key OPERATOR(pg_catalog.=) distributed_table.key)))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM citus_local JOIN distributed_table ON distributed_table.key = 10;
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT NULL::integer AS "dummy-1" FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((SELECT NULL::integer AS key, NULL::text AS value FROM (SELECT intermediate_result."dummy-1" FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result("dummy-1" integer)) citus_local_1) citus_local JOIN citus_local_dist_joins.distributed_table ON ((distributed_table.key OPERATOR(pg_catalog.=) 10)))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM citus_local JOIN distributed_table USING(key) JOIN postgres_table USING (key) JOIN reference_table USING(key);
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM citus_local_dist_joins.postgres_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((((SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local JOIN citus_local_dist_joins.distributed_table USING (key)) JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) JOIN citus_local_dist_joins.reference_table USING (key))
  count 
 -------
    100
 (1 row)
 
 SELECT count(*) FROM distributed_partitioned_table JOIN postgres_table USING(key) JOIN reference_table USING (key)
 	JOIN citus_local USING(key) WHERE distributed_partitioned_table.key > 10 and distributed_partitioned_table.key = 10;
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.postgres_table WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((citus_local_dist_joins.distributed_partitioned_table JOIN (SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table USING (key)) JOIN citus_local_dist_joins.reference_table USING (key)) JOIN (SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local USING (key)) WHERE ((distributed_partitioned_table.key OPERATOR(pg_catalog.>) 10) AND (distributed_partitioned_table.key OPERATOR(pg_catalog.=) 10))
  count 
 -------
      0
 (1 row)
 
 -- update
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM citus_local;
  count 
 -------
@@ -191,23 +132,20 @@
 (1 row)
 
 UPDATE
 	citus_local
 SET
 	value = 'test'
 FROM
 	distributed_table
 WHERE
 	distributed_table.key = citus_local.key;
-DEBUG:  Wrapping relation "distributed_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.distributed_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE citus_local_dist_joins.citus_local SET value = 'test'::text FROM (SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table WHERE (distributed_table.key OPERATOR(pg_catalog.=) citus_local.key)
 SELECT COUNT(DISTINCT value) FROM citus_local;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
@@ -216,23 +154,20 @@
 (1 row)
 
 UPDATE
 	distributed_table
 SET
 	value = 'test'
 FROM
 	citus_local
 WHERE
 	distributed_table.key = citus_local.key;
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE citus_local_dist_joins.distributed_table SET value = 'test'::text FROM (SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local WHERE (distributed_table.key OPERATOR(pg_catalog.=) citus_local.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
@@ -241,23 +176,20 @@
 (1 row)
 
 UPDATE
 	distributed_table_pkey
 SET
 	value = 'test'
 FROM
 	citus_local
 WHERE
 	distributed_table_pkey.key = citus_local.key;
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE citus_local_dist_joins.distributed_table_pkey SET value = 'test'::text FROM (SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) citus_local.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
@@ -266,23 +198,20 @@
 (1 row)
 
 UPDATE
 	distributed_table_windex
 SET
 	value = 'test'
 FROM
 	citus_local
 WHERE
 	distributed_table_windex.key = citus_local.key;
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: UPDATE citus_local_dist_joins.distributed_table_windex SET value = 'test'::text FROM (SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local WHERE (distributed_table_windex.key OPERATOR(pg_catalog.=) citus_local.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
 -------
      1
 (1 row)
 
 ROLLBACK;
 BEGIN;
 UPDATE
 	mv1
@@ -323,92 +252,80 @@
 -------
    100
 (1 row)
 
 DELETE FROM
 	citus_local
 USING
 	distributed_table
 WHERE
 	distributed_table.key = citus_local.key;
-DEBUG:  Wrapping relation "distributed_table" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.distributed_table WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: DELETE FROM citus_local_dist_joins.citus_local USING (SELECT distributed_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) distributed_table_1) distributed_table WHERE (distributed_table.key OPERATOR(pg_catalog.=) citus_local.key)
 SELECT COUNT(DISTINCT value) FROM citus_local;
  count 
 -------
      0
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
 -------
    100
 (1 row)
 
 DELETE FROM
 	distributed_table
 USING
 	citus_local
 WHERE
 	distributed_table.key = citus_local.key;
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: DELETE FROM citus_local_dist_joins.distributed_table USING (SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local WHERE (distributed_table.key OPERATOR(pg_catalog.=) citus_local.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table;
  count 
 -------
      0
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
 -------
    100
 (1 row)
 
 DELETE FROM
 	distributed_table_pkey
 USING
 	citus_local
 WHERE
 	distributed_table_pkey.key = citus_local.key;
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: DELETE FROM citus_local_dist_joins.distributed_table_pkey USING (SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local WHERE (distributed_table_pkey.key OPERATOR(pg_catalog.=) citus_local.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_pkey;
  count 
 -------
      0
 (1 row)
 
 ROLLBACK;
 BEGIN;
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
 -------
    100
 (1 row)
 
 DELETE FROM
 	distributed_table_windex
 USING
 	citus_local
 WHERE
 	distributed_table_windex.key = citus_local.key;
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: DELETE FROM citus_local_dist_joins.distributed_table_windex USING (SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local WHERE (distributed_table_windex.key OPERATOR(pg_catalog.=) citus_local.key)
 SELECT COUNT(DISTINCT value) FROM distributed_table_windex;
  count 
 -------
      0
 (1 row)
 
 ROLLBACK;
 DELETE FROM
 	mv1
 USING
@@ -424,42 +341,26 @@
 	mv1.key = citus_local.key;
 ERROR:  materialized views in modify queries are not supported
 DELETE FROM
 	citus_local
 USING
 	mv2
 WHERE
 	mv2.key = citus_local.key;
 ERROR:  materialized views in modify queries are not supported
 SELECT count(*) FROM postgres_table JOIN (SELECT * FROM (SELECT * FROM distributed_table LIMIT 1) d1) d2 using (key) JOIN reference_table USING(key) JOIN citus_local USING (key) JOIN (SELECT * FROM citus_local) c1  USING (key) WHERE d2.key > 10 AND d2.key = 10;
-DEBUG:  push down of limit count: 1
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value, value_2 FROM citus_local_dist_joins.distributed_table LIMIT 1
-DEBUG:  generating subplan XXX_2 for subquery SELECT key, value FROM citus_local_dist_joins.citus_local
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_3 for subquery SELECT key FROM citus_local_dist_joins.postgres_table WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_4 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN (SELECT d1.key, d1.value, d1.value_2 FROM (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb)) d1) d2 USING (key)) JOIN citus_local_dist_joins.reference_table USING (key)) JOIN (SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local USING (key)) JOIN (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) c1 USING (key)) WHERE ((d2.key OPERATOR(pg_catalog.>) 10) AND (d2.key OPERATOR(pg_catalog.=) 10))
  count 
 -------
      0
 (1 row)
 
 SELECT count(*) FROM postgres_table JOIN (SELECT * FROM (SELECT * FROM distributed_table LIMIT 1) d1) d2 using (key) JOIN reference_table USING(key) JOIN citus_local USING (key) JOIN (SELECT * FROM citus_local) c1  USING (key) WHERE d2.key > 10 AND d2.key = 10;
-DEBUG:  push down of limit count: 1
-DEBUG:  generating subplan XXX_1 for subquery SELECT key, value, value_2 FROM citus_local_dist_joins.distributed_table LIMIT 1
-DEBUG:  generating subplan XXX_2 for subquery SELECT key, value FROM citus_local_dist_joins.citus_local
-DEBUG:  Wrapping relation "postgres_table" to a subquery
-DEBUG:  generating subplan XXX_3 for subquery SELECT key FROM citus_local_dist_joins.postgres_table WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Wrapping relation "citus_local" to a subquery
-DEBUG:  generating subplan XXX_4 for subquery SELECT key FROM citus_local_dist_joins.citus_local WHERE (key OPERATOR(pg_catalog.=) 10)
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM (((((SELECT postgres_table_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) postgres_table_1) postgres_table JOIN (SELECT d1.key, d1.value, d1.value_2 FROM (SELECT intermediate_result.key, intermediate_result.value, intermediate_result.value_2 FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text, value_2 jsonb)) d1) d2 USING (key)) JOIN citus_local_dist_joins.reference_table USING (key)) JOIN (SELECT citus_local_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) citus_local_1) citus_local USING (key)) JOIN (SELECT intermediate_result.key, intermediate_result.value FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer, value text)) c1 USING (key)) WHERE ((d2.key OPERATOR(pg_catalog.>) 10) AND (d2.key OPERATOR(pg_catalog.=) 10))
  count 
 -------
      0
 (1 row)
 
 SELECT
 	COUNT(*)
 FROM
 	postgres_table p1
 JOIN
@@ -476,29 +377,20 @@
 USING (key)
 JOIN
 	reference_table r1
 USING (key)
 JOIN
 	distributed_table d2
 USING (key)
 JOIN
 	citus_local c2
 USING (key);
-DEBUG:  Wrapping relation "postgres_table" "p1" to a subquery
-DEBUG:  generating subplan XXX_1 for subquery SELECT key FROM citus_local_dist_joins.postgres_table p1 WHERE true
-DEBUG:  Wrapping relation "citus_local" "c1" to a subquery
-DEBUG:  generating subplan XXX_2 for subquery SELECT key FROM citus_local_dist_joins.citus_local c1 WHERE true
-DEBUG:  Wrapping relation "postgres_table" "p2" to a subquery
-DEBUG:  generating subplan XXX_3 for subquery SELECT key FROM citus_local_dist_joins.postgres_table p2 WHERE true
-DEBUG:  Wrapping relation "citus_local" "c2" to a subquery
-DEBUG:  generating subplan XXX_4 for subquery SELECT key FROM citus_local_dist_joins.citus_local c2 WHERE true
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT count(*) AS count FROM ((((((((SELECT p1_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p1_1) p1 JOIN citus_local_dist_joins.distributed_partitioned_table dp1 USING (key)) JOIN citus_local_dist_joins.distributed_table d1 USING (key)) JOIN (SELECT c1_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_2'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) c1_1) c1 USING (key)) JOIN (SELECT p2_1.key, NULL::text AS value, NULL::jsonb AS value_2 FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_3'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) p2_1) p2 USING (key)) JOIN citus_local_dist_joins.reference_table r1 USING (key)) JOIN citus_local_dist_joins.distributed_table d2 USING (key)) JOIN (SELECT c2_1.key, NULL::text AS value FROM (SELECT intermediate_result.key FROM read_intermediate_result('XXX_4'::text, 'binary'::citus_copy_format) intermediate_result(key integer)) c2_1) c2 USING (key))
  count 
 -------
    100
 (1 row)
 
 SET client_min_messages to ERROR;
 DROP TABLE citus_local;
 SELECT master_remove_node('localhost', :master_port);
  master_remove_node 
 --------------------
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/pg_dump.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/pg_dump.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/pg_dump.out.modified	2022-11-09 13:38:55.609312283 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/pg_dump.out.modified	2022-11-09 13:38:55.619312283 +0300
@@ -152,24 +152,33 @@
 );
 SELECT create_distributed_table('data', 'key');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 COPY data FROM STDIN WITH (format csv, delimiter '|', escape '\');
 -- run pg_dump on worker (which has shards)
 \COPY output FROM PROGRAM 'PGAPPNAME=pg_dump pg_dump -f results/pg_dump.tmp -h localhost -p 57637 -U postgres -d regression -n dumper --quote-all-identifiers'
+pg_dump: error: no matching schemas were found
+PGAPPNAME=pg_dump pg_dump -f results/pg_dump.tmp -h localhost -p 57637 -U postgres -d regression -n dumper --quote-all-identifiers: child process exited with exit code 1
 -- restore pg_dump from worker via coordinator
 DROP SCHEMA dumper CASCADE;
 NOTICE:  drop cascades to table data
 \COPY (SELECT line FROM output WHERE line IS NOT NULL) TO PROGRAM 'psql -qtAX -h localhost -p 57636 -U postgres -d regression -f results/pg_dump.tmp'
 
 -- check the tables (should not include shards)
 SELECT tablename FROM pg_tables WHERE schemaname = 'dumper' ORDER BY 1;
     tablename    
 -----------------
  data
-(1 row)
+ dist_columnar
+ simple_columnar
+ weird.table
+(4 rows)
 
 DROP SCHEMA dumper CASCADE;
-NOTICE:  drop cascades to table data
+NOTICE:  drop cascades to 4 other objects
+DETAIL:  drop cascades to table data
+drop cascades to table dist_columnar
+drop cascades to table simple_columnar
+drop cascades to table "weird.table"
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_copy.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_copy.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_copy.out.modified	2022-11-09 13:38:56.819312278 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_copy.out.modified	2022-11-09 13:38:56.839312278 +0300
@@ -271,150 +271,146 @@
 SELECT create_distributed_table('customer_copy_append', 'c_custkey', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SET citus.shard_replication_factor TO 2;
 -- Test syntax error
 BEGIN;
 SELECT master_create_empty_shard('customer_copy_append') AS shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 COPY customer_copy_append(c_custkey, c_name) FROM STDIN WITH (FORMAT 'csv', append_to_shard :shardid);
-ERROR:  invalid input syntax for type integer: "notinteger"
-CONTEXT:  COPY customer_copy_append, line 3, column c_custkey: "notinteger"
+ERROR:  syntax error at or near ":"
+1,customer1
+2,customer2
+notinteger,customernot
+\.
+invalid command \.
 END;
+ERROR:  syntax error at or near "1"
 -- Test that no shard is created for failing copy
 SELECT count(*) FROM pg_dist_shard WHERE logicalrelid = 'customer_copy_append'::regclass;
- count
----------------------------------------------------------------------
-     0
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- Test empty copy
 BEGIN;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT master_create_empty_shard('customer_copy_append') AS shardid \gset
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 COPY customer_copy_append FROM STDIN WITH (append_to_shard :shardid);
+ERROR:  syntax error at or near ":"
+\.
+invalid command \.
 END;
 -- Test that a shard is created
 SELECT count(*) FROM pg_dist_shard WHERE logicalrelid = 'customer_copy_append'::regclass;
  count 
 -------
-     1
+     0
 (1 row)
 
 -- Test proper copy
 BEGIN;
 SELECT master_create_empty_shard('customer_copy_append') AS shardid \gset
+ERROR:  could only find 1 of 2 possible nodes
 COPY customer_copy_append(c_custkey, c_name) FROM STDIN WITH (FORMAT 'csv', append_to_shard :shardid);
+ERROR:  syntax error at or near ":"
+1,customer1
+2,customer2
+\.
+invalid command \.
 END;
+ERROR:  syntax error at or near "1"
 -- Check whether data was copied properly
 SELECT * FROM customer_copy_append;
- c_custkey |  c_name   | c_address | c_nationkey | c_phone | c_acctbal | c_mktsegment | c_comment
----------------------------------------------------------------------
-         1 | customer1 |           |             |         |           |              |
-         2 | customer2 |           |             |         |           |              |
-(2 rows)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- Manipulate manipulate and check shard statistics for append-partitioned table shard
 UPDATE pg_dist_shard_placement SET shardlength = 0 WHERE shardid = 560132;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT shardid, shardlength FROM pg_dist_shard_placement WHERE shardid = 560132;
- shardid | shardlength
----------------------------------------------------------------------
-  560132 |           0
-  560132 |           0
-(2 rows)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- Update shard statistics for append-partitioned shard
 SELECT citus_update_shard_statistics(560132);
- citus_update_shard_statistics
----------------------------------------------------------------------
-                          8192
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT shardid, shardlength FROM pg_dist_shard_placement WHERE shardid = 560132;
- shardid | shardlength
----------------------------------------------------------------------
-  560132 |        8192
-  560132 |        8192
-(2 rows)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 -- Create lineitem table
 CREATE TABLE lineitem_copy_append (
         l_orderkey bigint not null,
         l_partkey integer not null,
         l_suppkey integer not null,
         l_linenumber integer not null,
         l_quantity decimal(15, 2) not null,
         l_extendedprice decimal(15, 2) not null,
         l_discount decimal(15, 2) not null,
         l_tax decimal(15, 2) not null,
         l_returnflag char(1) not null,
         l_linestatus char(1) not null,
         l_shipdate date not null,
         l_commitdate date not null,
         l_receiptdate date not null,
         l_shipinstruct char(25) not null,
         l_shipmode char(10) not null,
         l_comment varchar(44) not null);
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT create_distributed_table('lineitem_copy_append', 'l_orderkey', 'append');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 BEGIN;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 SELECT master_create_empty_shard('lineitem_copy_append') AS shardid \gset
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
 \set client_side_copy_command '\\copy lineitem_copy_append FROM ' :'lineitem1datafile' ' with (delimiter '''|''', append_to_shard ' :shardid ');'
 :client_side_copy_command
+ERROR:  syntax error at or near ":"
 END;
 SELECT count(*) FROM pg_dist_shard WHERE logicalrelid = 'lineitem_copy_append'::regclass;
- count
----------------------------------------------------------------------
-     1
-(1 row)
-
+ERROR:  relation "lineitem_copy_append" does not exist
 -- trigger some errors on the append_to_shard option
 \set client_side_copy_command '\\copy lineitem_copy_append FROM ' :'lineitem1datafile' ' with (delimiter '''|''', append_to_shard 1);'
 :client_side_copy_command
-ERROR:  could not find valid entry for shard xxxxx
+ERROR:  relation "lineitem_copy_append" does not exist
 \set client_side_copy_command '\\copy lineitem_copy_append FROM ' :'lineitem1datafile' ' with (delimiter '''|''', append_to_shard 560000);'
 :client_side_copy_command
-ERROR:  shard xxxxx does not belong to table lineitem_copy_append
+ERROR:  relation "lineitem_copy_append" does not exist
 -- Test schema support on append partitioned tables
 CREATE SCHEMA append;
 CREATE TABLE append.customer_copy (
         c_custkey integer ,
         c_name varchar(25) not null,
         c_address varchar(40),
         c_nationkey integer,
         c_phone char(15),
         c_acctbal decimal(15,2),
         c_mktsegment char(10),
         c_comment varchar(117));
 SELECT create_distributed_table('append.customer_copy', 'c_custkey', 'append');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SELECT master_create_empty_shard('append.customer_copy') AS shardid1 \gset
+ERROR:  could only find 1 of 2 possible nodes
 SELECT master_create_empty_shard('append.customer_copy') AS shardid2 \gset
+ERROR:  could only find 1 of 2 possible nodes
 -- Test copy from the master node
 \set client_side_copy_command '\\copy append.customer_copy FROM ' :'customer1datafile' ' with (delimiter '''|''', append_to_shard ' :shardid1 ');'
 :client_side_copy_command
+ERROR:  syntax error at or near ":"
 \set client_side_copy_command '\\copy append.customer_copy FROM ' :'customer2datafile' ' with (delimiter '''|''', append_to_shard ' :shardid2 ');'
 :client_side_copy_command
+ERROR:  syntax error at or near ":"
 -- Test the content of the table
 SELECT min(c_custkey), max(c_custkey), avg(c_acctbal), count(*) FROM append.customer_copy;
  min | max | avg | count 
 -----+-----+-----+-------
-   1 | 7000 | 4443.8028800000000000 |  2000
+     |     |     |     0
 (1 row)
 
 -- Test with table name which contains special character
 CREATE TABLE "customer_with_special_\\_character"(
         c_custkey integer,
         c_name varchar(25) not null);
 SET citus.shard_replication_factor TO 1;
 SELECT create_distributed_table('"customer_with_special_\\_character"', 'c_custkey', 'hash', shard_count := 4);
  create_distributed_table 
 --------------------------
@@ -565,40 +561,44 @@
 
 -- no shards is created yet
 SELECT shardid, nodename, nodeport
 	FROM pg_dist_shard_placement join pg_dist_shard using(shardid)
 	WHERE logicalrelid = 'numbers_append'::regclass order by placementid;
  shardid | nodename | nodeport 
 ---------+----------+----------
 (0 rows)
 
 SELECT master_create_empty_shard('numbers_append') AS shardid1 \gset
+ERROR:  could only find 1 of 2 possible nodes
 SELECT master_create_empty_shard('numbers_append') AS shardid2 \gset
+ERROR:  could only find 1 of 2 possible nodes
 COPY numbers_append FROM STDIN WITH (FORMAT 'csv', append_to_shard :shardid1);
+ERROR:  syntax error at or near ":"
+1,1
+2,2
+\.
+invalid command \.
 COPY numbers_append FROM STDIN WITH (FORMAT 'csv', append_to_shard :shardid2);
+ERROR:  syntax error at or near "1"
+4,6
+\.
+invalid command \.
 -- verify there are shards at both workers
 SELECT shardid, nodename, nodeport
 	FROM pg_dist_shard_placement join pg_dist_shard using(shardid)
 	WHERE logicalrelid = 'numbers_append'::regclass order by placementid;
- shardid | nodename  | nodeport
----------------------------------------------------------------------
-  560155 | localhost |    57637
-  560155 | localhost |    57638
-  560156 | localhost |    57638
-  560156 | localhost |    57637
-(4 rows)
-
+ERROR:  syntax error at or near "4"
 -- disable the first node
 SET client_min_messages TO ERROR;
 \set VERBOSITY terse
 SELECT master_disable_node('localhost', :worker_1_port);
-ERROR:  cannot remove or disable the node localhost:xxxxx because because it contains the only shard placement for shard xxxxx
+ERROR:  node at "localhost:57637" does not exist
 SELECT public.wait_until_metadata_sync(30000);
  wait_until_metadata_sync 
 --------------------------
  
 (1 row)
 
 RESET client_min_messages;
 \set VERBOSITY default
 -- set replication factor to 1 so that copy will
 -- succeed without replication count error
@@ -606,242 +606,122 @@
 -- add two new shards and verify they are created at the other node
 SELECT master_create_empty_shard('numbers_append') AS shardid1 \gset
 SELECT master_create_empty_shard('numbers_append') AS shardid2 \gset
 COPY numbers_append FROM STDIN WITH (FORMAT 'csv', append_to_shard :shardid1);
 COPY numbers_append FROM STDIN WITH (FORMAT 'csv', append_to_shard :shardid2);
 SELECT shardid, nodename, nodeport
 	FROM pg_dist_shard_placement join pg_dist_shard using(shardid)
 	WHERE logicalrelid = 'numbers_append'::regclass order by placementid;
  shardid | nodename  | nodeport 
 ---------+-----------+----------
-  560155 | localhost |    57637
   560155 | localhost |    57638
   560156 | localhost |    57638
-  560156 | localhost |    57637
-  560157 | localhost |    57637
-  560158 | localhost |    57638
-(6 rows)
+(2 rows)
 
 -- add the node back
 SET client_min_messages TO ERROR;
 SELECT 1 FROM master_activate_node('localhost', :worker_1_port);
- ?column?
----------------------------------------------------------------------
-        1
-(1 row)
-
+ERROR:  node at "localhost:57637" does not exist
 RESET client_min_messages;
 RESET citus.shard_replication_factor;
 -- add two new shards and verify they are created at both workers
 SELECT master_create_empty_shard('numbers_append') AS shardid1 \gset
+ERROR:  could only find 1 of 2 possible nodes
 SELECT master_create_empty_shard('numbers_append') AS shardid2 \gset
+ERROR:  could only find 1 of 2 possible nodes
 COPY numbers_append FROM STDIN WITH (FORMAT 'csv', append_to_shard :shardid1);
 COPY numbers_append FROM STDIN WITH (FORMAT 'csv', append_to_shard :shardid2);
 SELECT shardid, nodename, nodeport
 	FROM pg_dist_shard_placement join pg_dist_shard using(shardid)
 	WHERE logicalrelid = 'numbers_append'::regclass order by placementid;
  shardid | nodename  | nodeport 
 ---------+-----------+----------
-  560155 | localhost |    57637
   560155 | localhost |    57638
   560156 | localhost |    57638
-  560156 | localhost |    57637
-  560157 | localhost |    57637
-  560158 | localhost |    57638
-  560159 | localhost |    57637
-  560159 | localhost |    57638
-  560160 | localhost |    57638
-  560160 | localhost |    57637
-(10 rows)
+(2 rows)
 
 DROP TABLE numbers_append;
 -- Test copy failures against connection failures
 -- create and switch to test user
 CREATE USER test_user;
+ERROR:  role "test_user" already exists
 \c - test_user
 SET citus.shard_count to 4;
 CREATE TABLE numbers_hash (a int, b int);
 SELECT create_distributed_table('numbers_hash', 'a', colocate_with:='none');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 COPY numbers_hash FROM STDIN WITH (FORMAT 'csv');
 -- verify each placement is active
 SELECT shardid, shardstate, nodename, nodeport
 	FROM pg_dist_shard_placement join pg_dist_shard using(shardid)
 	WHERE logicalrelid = 'numbers_hash'::regclass order by shardid, nodeport;
  shardid | shardstate | nodename | nodeport 
 ---------+------------+----------+----------
-  560161 |          1 | localhost |    57637
-  560161 |          1 | localhost |    57638
-  560162 |          1 | localhost |    57637
-  560162 |          1 | localhost |    57638
-  560163 |          1 | localhost |    57637
-  560163 |          1 | localhost |    57638
-  560164 |          1 | localhost |    57637
-  560164 |          1 | localhost |    57638
-(8 rows)
+(0 rows)
 
 -- create a reference table
 CREATE TABLE numbers_reference(a int, b int);
 SELECT create_reference_table('numbers_reference');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 COPY numbers_reference FROM STDIN WITH (FORMAT 'csv');
 -- create another hash distributed table
 CREATE TABLE numbers_hash_other(a int, b int);
 SELECT create_distributed_table('numbers_hash_other', 'a', colocate_with:='numbers_hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  relation numbers_hash is not distributed
 SELECT shardid, shardstate, nodename, nodeport
 	FROM pg_dist_shard_placement join pg_dist_shard using(shardid)
 	WHERE logicalrelid = 'numbers_hash_other'::regclass order by shardid, nodeport;
  shardid | shardstate | nodename | nodeport 
 ---------+------------+----------+----------
-  560166 |          1 | localhost |    57637
-  560166 |          1 | localhost |    57638
-  560167 |          1 | localhost |    57637
-  560167 |          1 | localhost |    57638
-  560168 |          1 | localhost |    57637
-  560168 |          1 | localhost |    57638
-  560169 |          1 | localhost |    57637
-  560169 |          1 | localhost |    57638
-(8 rows)
+(0 rows)
 
 -- manually corrupt pg_dist_shard such that both copies of one shard is placed in
 -- worker_1+10. This is to test the behavior when no replica of a shard is accessible.
 -- Whole copy operation is supposed to fail and rollback. We use session_replication_role
 -- trick to disable the triggers on the pg_dist_shard_placement view
 \c - :default_user
 SET session_replication_role = replica;
 UPDATE pg_dist_shard_placement SET nodeport = :worker_1_port+10 WHERE shardid = 560176 and nodeport = :worker_2_port;
 SET session_replication_role = DEFAULT;
 -- disable test_user on the first worker
 \c - :default_user - :worker_1_port
 SET citus.enable_metadata_sync TO off;
 ALTER USER test_user WITH nologin;
+ERROR:  role "test_user" does not exist
 \c - test_user - :master_port
 -- reissue copy, and it should fail
 COPY numbers_hash FROM STDIN WITH (FORMAT 'csv');
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" is not permitted to log in
--- verify shards in the none of the workers as marked invalid
-SELECT shardid, shardstate, nodename, nodeport
-	FROM pg_dist_shard_placement join pg_dist_shard using(shardid)
-	WHERE logicalrelid = 'numbers_hash'::regclass order by shardid, nodeport;
- shardid | shardstate | nodename  | nodeport
----------------------------------------------------------------------
-  560161 |          1 | localhost |    57637
-  560161 |          1 | localhost |    57638
-  560162 |          1 | localhost |    57637
-  560162 |          1 | localhost |    57638
-  560163 |          1 | localhost |    57637
-  560163 |          1 | localhost |    57638
-  560164 |          1 | localhost |    57637
-  560164 |          1 | localhost |    57638
-(8 rows)
-
--- try to insert into a reference table copy should fail
-COPY numbers_reference FROM STDIN WITH (FORMAT 'csv');
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" is not permitted to log in
--- verify shards for reference table are still valid
-SELECT shardid, shardstate, nodename, nodeport
-	FROM pg_dist_shard_placement join pg_dist_shard using(shardid)
-	WHERE logicalrelid = 'numbers_reference'::regclass order by placementid;
- shardid | shardstate | nodename  | nodeport
----------------------------------------------------------------------
-  560165 |          1 | localhost |    57637
-  560165 |          1 | localhost |    57638
-(2 rows)
-
--- try to insert into numbers_hash_other. copy should fail and rollback
--- since it can not insert into either copies of a shard. shards are expected to
--- stay valid since the operation is rolled back.
-COPY numbers_hash_other FROM STDIN WITH (FORMAT 'csv');
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "test_user" is not permitted to log in
--- verify shards for numbers_hash_other are still valid
--- since copy has failed altogether
-SELECT shardid, shardstate, nodename, nodeport
-	FROM pg_dist_shard_placement join pg_dist_shard using(shardid)
-	WHERE logicalrelid = 'numbers_hash_other'::regclass order by shardid, nodeport;
- shardid | shardstate | nodename  | nodeport
----------------------------------------------------------------------
-  560166 |          1 | localhost |    57637
-  560166 |          1 | localhost |    57638
-  560167 |          1 | localhost |    57637
-  560167 |          1 | localhost |    57638
-  560168 |          1 | localhost |    57637
-  560168 |          1 | localhost |    57638
-  560169 |          1 | localhost |    57637
-  560169 |          1 | localhost |    57638
-(8 rows)
-
--- re-enable test_user on the first worker
-\c - :default_user - :worker_1_port
-SET citus.enable_metadata_sync TO off;
-ALTER USER test_user WITH login;
-\c - test_user - :master_port
-DROP TABLE numbers_hash;
-DROP TABLE numbers_hash_other;
-DROP TABLE numbers_reference;
-\c - :default_user
--- test copy failure inside the node
--- it will be done by changing definition of a shard table
-SET citus.shard_count to 4;
-SET citus.next_shard_id TO 560170;
-CREATE TABLE numbers_hash(a int, b int);
-SELECT create_distributed_table('numbers_hash', 'a', colocate_with:='none');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
-\c - - - :worker_1_port
-ALTER TABLE numbers_hash_560170 DROP COLUMN b;
-\c - - - :master_port
--- operation will fail to modify a shard and roll back
-COPY numbers_hash FROM STDIN WITH (FORMAT 'csv');
-ERROR:  column "b" of relation "numbers_hash_560170" does not exist
-CONTEXT:  while executing command on localhost:xxxxx
-COPY numbers_hash, line 1: "1,1"
+ERROR:  missing data for column "b"
+CONTEXT:  COPY numbers_hash, line 1: ""
 -- verify no row is inserted
 SELECT count(a) FROM numbers_hash;
  count 
 -------
-     0
+     8
 (1 row)
 
 -- verify shard is still marked as valid
 SELECT shardid, shardstate, nodename, nodeport
 	FROM pg_dist_shard_placement join pg_dist_shard using(shardid)
 	WHERE logicalrelid = 'numbers_hash'::regclass order by shardid, nodeport;
  shardid | shardstate | nodename | nodeport 
 ---------+------------+----------+----------
-  560170 |          1 | localhost |    57637
-  560170 |          1 | localhost |    57638
-  560171 |          1 | localhost |    57637
-  560171 |          1 | localhost |    57638
-  560172 |          1 | localhost |    57637
-  560172 |          1 | localhost |    57638
-  560173 |          1 | localhost |    57637
-  560173 |          1 | localhost |    57638
-(8 rows)
+(0 rows)
 
 DROP TABLE numbers_hash;
 DROP USER test_user;
+ERROR:  permission denied to drop role
 -- Test copy with built-in type without binary output function
 CREATE TABLE test_binaryless_builtin (
 col1 aclitem NOT NULL,
 col2 character varying(255) NOT NULL
 );
 SELECT create_reference_table('test_binaryless_builtin');
  create_reference_table 
 ------------------------
  
 (1 row)
@@ -851,164 +731,47 @@
         col1         | col2  
 ---------------------+-------
  postgres=r/postgres |  test
 (1 row)
 
 DROP TABLE test_binaryless_builtin;
 -- Test drop table with copy in the same transaction
 BEGIN;
 CREATE TABLE tt1(id int);
 SELECT create_distributed_table('tt1','id');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 \copy tt1 from STDIN;
+ERROR:  current transaction is aborted, commands ignored until end of transaction block
+1
+2
+\.
+invalid command \.
 DROP TABLE tt1;
+ERROR:  syntax error at or near "1"
 END;
 -- Test dropping a column in front of the partition column
 CREATE TABLE drop_copy_test_table (col1 int, col2 int, col3 int, col4 int);
 SELECT create_distributed_table('drop_copy_test_table','col3');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 ALTER TABLE drop_copy_test_table drop column col1;
 COPY drop_copy_test_table (col2,col3,col4) from STDIN with CSV;
 SELECT * FROM drop_copy_test_table WHERE col3 = 1;
  col2 | col3 | col4 
 ------+------+------
       |    1 |     
 (1 row)
 
 ALTER TABLE drop_copy_test_table drop column col4;
 COPY drop_copy_test_table (col2,col3) from STDIN with CSV;
 SELECT * FROM drop_copy_test_table WHERE col3 = 1;
  col2 | col3 
 ------+------
       |    1
       |    1
 (2 rows)
 
 DROP TABLE drop_copy_test_table;
 -- There should be no "tt1" shard on the worker nodes
 \c - - - :worker_1_port
-SELECT relname FROM pg_class WHERE relname LIKE 'tt1%';
- relname
----------------------------------------------------------------------
-(0 rows)
-
-\c - - - :master_port
--- copy >8MB to a single worker to trigger a flush in PutRemoteCopyData
-BEGIN;
-CREATE UNLOGGED TABLE trigger_flush AS
-SELECT 1 AS a, s AS b, s AS c, s AS d, s AS e, s AS f, s AS g, s AS h FROM generate_series(1,150000) s;
-SELECT create_distributed_table('trigger_flush','a');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.trigger_flush$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
-ABORT;
--- trigger switch-over when using single connection per worker
-BEGIN;
-SET citus.shard_count TO 3;
-SET citus.multi_shard_modify_mode TO 'sequential';
-CREATE UNLOGGED TABLE trigger_switchover(a int, b int, c int, d int, e int, f int, g int, h int);
-SELECT create_distributed_table('trigger_switchover','a');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
-INSERT INTO trigger_switchover
-  SELECT s AS a, s AS b, s AS c, s AS d, s AS e, s AS f, s AS g, s AS h FROM generate_series(1,250000) s;
-ABORT;
--- copy into a table with a JSONB column
-CREATE TABLE copy_jsonb (key text, value jsonb, extra jsonb default '["default"]'::jsonb);
-SELECT create_distributed_table('copy_jsonb', 'key', colocate_with => 'none');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
--- JSONB from text should work
-\COPY copy_jsonb (key, value) FROM STDIN
-SELECT * FROM copy_jsonb ORDER BY key;
-  key  |           value            |    extra
----------------------------------------------------------------------
- blue  | {"b": 255, "g": 0, "r": 0} | ["default"]
- green | {"b": 0, "g": 255, "r": 0} | ["default"]
-(2 rows)
-
--- JSONB from binary should work
-COPY copy_jsonb TO :'temp_dir''copy_jsonb.pgcopy' WITH (format binary);
-COPY copy_jsonb FROM :'temp_dir''copy_jsonb.pgcopy' WITH (format binary);
-SELECT * FROM copy_jsonb ORDER BY key;
-  key  |           value            |    extra
----------------------------------------------------------------------
- blue  | {"b": 255, "g": 0, "r": 0} | ["default"]
- blue  | {"b": 255, "g": 0, "r": 0} | ["default"]
- green | {"b": 0, "g": 255, "r": 0} | ["default"]
- green | {"b": 0, "g": 255, "r": 0} | ["default"]
-(4 rows)
-
--- JSONB parsing error without validation: no line number
-\COPY copy_jsonb (key, value) FROM STDIN
-ERROR:  invalid input syntax for type json
-DETAIL:  The input string ended unexpectedly.
-TRUNCATE copy_jsonb;
--- JSONB when there is a complex column should work. Complex columns force
--- non binary copy format between master and workers.
-CREATE TYPE complex_for_copy_test AS (r double precision, i double precision);
-CREATE TABLE copy_jsonb_with_complex(key int, value_1 jsonb, value_2 complex_for_copy_test);
-SELECT create_distributed_table('copy_jsonb_with_complex', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
-\COPY copy_jsonb_with_complex FROM STDIN
-SELECT * FROM copy_jsonb_with_complex ORDER BY key;
- key |                  value_1                  | value_2
----------------------------------------------------------------------
-   1 | {"f1": 3803, "f2": "v1", "f3": {"f4": 1}} | (10,9)
-   2 | {"f5": null, "f6": true}                  | (20,19)
-(2 rows)
-
-DROP TABLE copy_jsonb_with_complex;
-DROP TYPE complex_for_copy_test;
-SET citus.skip_jsonb_validation_in_copy TO off;
--- JSONB from text should work
-\COPY copy_jsonb (key, value) FROM STDIN
-SELECT * FROM copy_jsonb ORDER BY key;
-  key  |           value            |    extra
----------------------------------------------------------------------
- blue  | {"b": 255, "g": 0, "r": 0} | ["default"]
- green | {"b": 0, "g": 255, "r": 0} | ["default"]
-(2 rows)
-
--- JSONB from binary should work
-COPY copy_jsonb TO :'temp_dir''copy_jsonb.pgcopy' WITH (format binary);
-COPY copy_jsonb FROM :'temp_dir''copy_jsonb.pgcopy' WITH (format binary);
-SELECT * FROM copy_jsonb ORDER BY key;
-  key  |           value            |    extra
----------------------------------------------------------------------
- blue  | {"b": 255, "g": 0, "r": 0} | ["default"]
- blue  | {"b": 255, "g": 0, "r": 0} | ["default"]
- green | {"b": 0, "g": 255, "r": 0} | ["default"]
- green | {"b": 0, "g": 255, "r": 0} | ["default"]
-(4 rows)
-
--- JSONB parsing error with validation: should see line number
-\COPY copy_jsonb (key, value) FROM STDIN
-ERROR:  invalid input syntax for type json
-DETAIL:  The input string ended unexpectedly.
-CONTEXT:  JSON data, line 1: {"r":255,"g":0,"b":0
-COPY copy_jsonb, line 1, column value: "{"r":255,"g":0,"b":0"
-DROP TABLE copy_jsonb;
+\connect: connection to server at "localhost" (127.0.0.1), port 57637 failed: FATAL:  role "test_user" does not exist
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/fast_path_router_modify.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/fast_path_router_modify.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/fast_path_router_modify.out.modified	2022-11-09 13:38:56.949312278 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/fast_path_router_modify.out.modified	2022-11-09 13:38:56.959312277 +0300
@@ -10,25 +10,22 @@
 CREATE TABLE modify_fast_path(key int, value_1 int, value_2 text);
 SELECT create_distributed_table('modify_fast_path', 'key');
  create_distributed_table 
 --------------------------
  
 (1 row)
 
 SET citus.shard_replication_factor TO 2;
 CREATE TABLE modify_fast_path_replication_2(key int, value_1 int, value_2 text);
 SELECT create_distributed_table('modify_fast_path_replication_2', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 CREATE TABLE modify_fast_path_reference(key int, value_1 int, value_2 text);
 SELECT create_reference_table('modify_fast_path_reference');
  create_reference_table 
 ------------------------
  
 (1 row)
 
 -- show the output
 SET client_min_messages TO DEBUG;
 -- very simple queries goes through fast-path planning
@@ -165,27 +162,29 @@
 
 SELECT * FROM modify_fast_path_reference WHERE key = 1 FOR SHARE;
 DEBUG:  Distributed planning for a fast-path router query
 DEBUG:  Creating router plan
  key | value_1 | value_2 
 -----+---------+---------
 (0 rows)
 
 -- for update/share is not supported via fast-path wen replication factor > 1
 SELECT * FROM modify_fast_path_replication_2 WHERE key = 1 FOR UPDATE;
-DEBUG:  SELECT FOR UPDATE with table replication factor > 1 not supported for non-reference tables.
-ERROR:  could not run distributed query with FOR UPDATE/SHARE commands
-HINT:  Consider using an equality filter on the distributed table's partition column.
+ key | value_1 | value_2 
+-----+---------+---------
+(0 rows)
+
 SELECT * FROM modify_fast_path_replication_2 WHERE key = 1 FOR SHARE;
-DEBUG:  SELECT FOR UPDATE with table replication factor > 1 not supported for non-reference tables.
-ERROR:  could not run distributed query with FOR UPDATE/SHARE commands
-HINT:  Consider using an equality filter on the distributed table's partition column.
+ key | value_1 | value_2 
+-----+---------+---------
+(0 rows)
+
 -- very simple queries on reference tables goes through fast-path planning
 DELETE FROM modify_fast_path_reference WHERE key = 1;
 DEBUG:  Distributed planning for a fast-path router query
 DEBUG:  Creating router plan
 UPDATE modify_fast_path_reference SET value_1 = 1 WHERE key = 1;
 DEBUG:  Distributed planning for a fast-path router query
 DEBUG:  Creating router plan
 UPDATE modify_fast_path_reference SET value_1 = value_1 + 1 WHERE key = 1;
 DEBUG:  Distributed planning for a fast-path router query
 DEBUG:  Creating router plan
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_router_planner.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_router_planner.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_router_planner.out.modified	2022-11-09 13:38:57.819312274 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_router_planner.out.modified	2022-11-09 13:38:57.849312274 +0300
@@ -780,24 +780,20 @@
 (3 rows)
 
 -- following join is not router plannable since there are no
 -- workers containing both shards, but will work through recursive
 -- planning
 WITH single_shard as MATERIALIZED(SELECT * FROM articles_single_shard_hash)
 SELECT a.author_id as first_author, b.word_count as second_word_count
 	FROM articles_hash a, single_shard b
 	WHERE a.author_id = 2 and a.author_id = b.author_id
 	LIMIT 3;
-DEBUG:  found no worker with all shard placements
-DEBUG:  generating subplan XXX_1 for CTE single_shard: SELECT id, author_id, title, word_count FROM public.articles_single_shard_hash
-DEBUG:  Creating router plan
-DEBUG:  Plan XXX query after replacing subqueries and CTEs: SELECT a.author_id AS first_author, b.word_count AS second_word_count FROM public.articles_hash a, (SELECT intermediate_result.id, intermediate_result.author_id, intermediate_result.title, intermediate_result.word_count FROM read_intermediate_result('XXX_1'::text, 'binary'::citus_copy_format) intermediate_result(id bigint, author_id bigint, title character varying(20), word_count integer)) b WHERE ((a.author_id OPERATOR(pg_catalog.=) 2) AND (a.author_id OPERATOR(pg_catalog.=) b.author_id)) LIMIT 3
 DEBUG:  Creating router plan
 DEBUG:  query has a single distribution column value: 2
  first_author | second_word_count 
 --------------+-------------------
 (0 rows)
 
 -- single shard select with limit is router plannable
 SELECT *
 	FROM articles_hash
 	WHERE author_id = 1
@@ -1811,85 +1807,49 @@
 (0 rows)
 
 RESET citus.log_remote_commands;
 -- This query was intended to test "multi-shard join is not router plannable"
 -- To run it using repartition join logic we change the join columns
 SET citus.enable_repartition_joins to ON;
 SELECT * FROM articles_range ar join authors_range au on (ar.title = au.name)
 	WHERE ar.author_id = 35;
 DEBUG:  Router planner cannot handle multi-shard select queries
 DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
 DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
 DEBUG:  pruning merge fetch taskId 1
 DETAIL:  Creating dependency on merge taskId 2
 DEBUG:  pruning merge fetch taskId 2
 DETAIL:  Creating dependency on merge taskId 5
 DEBUG:  pruning merge fetch taskId 4
 DETAIL:  Creating dependency on merge taskId 4
 DEBUG:  pruning merge fetch taskId 5
 DETAIL:  Creating dependency on merge taskId 10
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 15
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 20
  id | author_id | title | word_count | name | id 
 ----+-----------+-------+------------+------+----
 (0 rows)
 
 -- This query was intended to test "this is a bug, it is a single shard join
 -- query but not router plannable". To run it using repartition join logic we
 -- change the join columns.
 SELECT * FROM articles_range ar join authors_range au on (ar.title = au.name)
 	WHERE ar.author_id = 1 or au.id = 5;
 DEBUG:  Router planner cannot handle multi-shard select queries
 DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
 DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
 DEBUG:  pruning merge fetch taskId 1
 DETAIL:  Creating dependency on merge taskId 5
 DEBUG:  pruning merge fetch taskId 2
 DETAIL:  Creating dependency on merge taskId 5
 DEBUG:  pruning merge fetch taskId 4
 DETAIL:  Creating dependency on merge taskId 10
 DEBUG:  pruning merge fetch taskId 5
 DETAIL:  Creating dependency on merge taskId 10
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 15
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 15
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 20
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 20
  id | author_id | title | word_count | name | id 
 ----+-----------+-------+------------+------+----
 (0 rows)
 
 -- bogus query, join on non-partition column, but router plannable due to filters
 SELECT * FROM articles_range ar join authors_range au on (ar.id = au.id)
 	WHERE ar.author_id = 1 and au.id < 10;
 DEBUG:  Creating router plan
  id | author_id | title | word_count | name | id 
 ----+-----------+-------+------------+------+----
@@ -1903,49 +1863,22 @@
 	WHERE ar.author_id = 2;
 DEBUG:  Creating router plan
 DEBUG:  query has a single distribution column value: 2
  id | author_id | title | word_count | name | id 
 ----+-----------+-------+------------+------+----
 (0 rows)
 
 -- not router plannable
 SELECT * FROM articles_hash ar join authors_range au on (ar.author_id = au.id)
 	WHERE ar.author_id = 3;
-DEBUG:  found no worker with all shard placements
-DEBUG:  join prunable for task partitionId 0 and 1
-DEBUG:  join prunable for task partitionId 0 and 2
-DEBUG:  join prunable for task partitionId 0 and 3
-DEBUG:  join prunable for task partitionId 1 and 0
-DEBUG:  join prunable for task partitionId 1 and 2
-DEBUG:  join prunable for task partitionId 1 and 3
-DEBUG:  join prunable for task partitionId 2 and 0
-DEBUG:  join prunable for task partitionId 2 and 1
-DEBUG:  join prunable for task partitionId 2 and 3
-DEBUG:  join prunable for task partitionId 3 and 0
-DEBUG:  join prunable for task partitionId 3 and 1
-DEBUG:  join prunable for task partitionId 3 and 2
-DEBUG:  pruning merge fetch taskId 1
-DETAIL:  Creating dependency on merge taskId 2
-DEBUG:  pruning merge fetch taskId 2
-DETAIL:  Creating dependency on merge taskId 5
-DEBUG:  pruning merge fetch taskId 4
-DETAIL:  Creating dependency on merge taskId 4
-DEBUG:  pruning merge fetch taskId 5
-DETAIL:  Creating dependency on merge taskId 10
-DEBUG:  pruning merge fetch taskId 7
-DETAIL:  Creating dependency on merge taskId 6
-DEBUG:  pruning merge fetch taskId 8
-DETAIL:  Creating dependency on merge taskId 15
-DEBUG:  pruning merge fetch taskId 10
-DETAIL:  Creating dependency on merge taskId 8
-DEBUG:  pruning merge fetch taskId 11
-DETAIL:  Creating dependency on merge taskId 20
+DEBUG:  Creating router plan
+DEBUG:  query has a single distribution column value: 3
  id | author_id | title | word_count | name | id 
 ----+-----------+-------+------------+------+----
 (0 rows)
 
 -- join between a range partitioned table and reference table is router plannable
 SELECT * FROM articles_range ar join authors_reference au on (ar.author_id = au.id)
 	WHERE ar.author_id = 1;
 DEBUG:  Creating router plan
  id | author_id | title | word_count | name | id 
 ----+-----------+-------+------------+------+----
@@ -2419,58 +2352,45 @@
  41
  51
  52
 (7 rows)
 
 SET client_min_messages to 'NOTICE';
 -- test that a connection failure marks placements invalid
 SET citus.shard_replication_factor TO 2;
 CREATE TABLE failure_test (a int, b int);
 SELECT create_distributed_table('failure_test', 'a', 'hash');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 SET citus.enable_ddl_propagation TO off;
 CREATE USER router_user;
 GRANT INSERT ON ALL TABLES IN SCHEMA public TO router_user;
 \c - - - :worker_1_port
 SET citus.enable_ddl_propagation TO off;
 CREATE USER router_user;
 GRANT INSERT ON ALL TABLES IN SCHEMA public TO router_user;
 \c - router_user - :master_port
 -- we will fail to connect to worker 2, since the user does not exist
 -- still, we never mark placements inactive. Instead, fail the transaction
 BEGIN;
 INSERT INTO failure_test VALUES (1, 1);
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "router_user" does not exist
 ROLLBACK;
 INSERT INTO failure_test VALUES (2, 1);
-ERROR:  connection to the remote node localhost:xxxxx failed with the following error: FATAL:  role "router_user" does not exist
 SELECT shardid, shardstate, nodename, nodeport FROM pg_dist_shard_placement
 	WHERE shardid IN (
 		SELECT shardid FROM pg_dist_shard
 		WHERE logicalrelid = 'failure_test'::regclass
 	)
 	ORDER BY placementid;
  shardid | shardstate | nodename | nodeport 
 ---------+------------+----------+----------
-  840017 |          1 | localhost |    57637
-  840017 |          1 | localhost |    57638
-  840018 |          1 | localhost |    57637
-  840018 |          1 | localhost |    57638
-  840019 |          1 | localhost |    57637
-  840019 |          1 | localhost |    57638
-  840020 |          1 | localhost |    57637
-  840020 |          1 | localhost |    57638
-(8 rows)
+(0 rows)
 
 \c - postgres - :worker_1_port
 DROP OWNED BY router_user;
 DROP USER router_user;
 \c - - - :master_port
 DROP OWNED BY router_user;
 DROP USER router_user;
 DROP TABLE failure_test;
 DROP FUNCTION author_articles_max_id();
 DROP FUNCTION author_articles_id_word_count();
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/null_parameters.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/null_parameters.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/null_parameters.out.modified	2022-11-09 13:38:58.159312273 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/null_parameters.out.modified	2022-11-09 13:38:58.169312273 +0300
@@ -1,23 +1,20 @@
 -- this file contain tests with pruning of NULL
 -- values with prepared statements
 CREATE SCHEMA null_parameters;
 SET search_path TO null_parameters;
 SET citus.next_shard_id TO 1680000;
 SET citus.shard_count to 32;
 CREATE TABLE text_dist_column (key text, value text);
 SELECT create_distributed_table('text_dist_column', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- it seems that sometimes the pruning is deferred and sometimes not.
 -- what we care about is that these queries don't crash and the log for this
 -- one shouldn't matter. This is to prevent these test being from flaky in our CI.
 SET client_min_messages to NOTICE;
 PREPARE null_select_on_text AS SELECT count(*) FROM text_dist_column WHERE key = NULL;
 EXECUTE null_select_on_text;
  count 
 -------
      0
 (1 row)
@@ -1056,25 +1053,22 @@
 EXECUTE null_update_on_text_param_and_in_4(NULL);
 EXECUTE null_update_on_text_param_and_in_4(NULL);
 EXECUTE null_update_on_text_param_and_in_4(NULL);
 EXECUTE null_update_on_text_param_and_in_4(NULL);
 EXECUTE null_update_on_text_param_and_in_4(NULL);
 EXECUTE null_update_on_text_param_and_in_4(NULL);
 EXECUTE null_update_on_text_param_and_in_4(NULL);
 -- sanity check with JSBONB column
 CREATE TABLE jsonb_dist_column (key jsonb, value text);
 SELECT create_distributed_table('jsonb_dist_column', 'key');
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 PREPARE null_select_on_json_param(jsonb) AS SELECT count(*) FROM jsonb_dist_column WHERE key = $1;
 EXECUTE null_select_on_json_param(NULL);
  count 
 -------
      0
 (1 row)
 
 EXECUTE null_select_on_json_param(NULL);
  count 
 -------
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_large_shardid.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_large_shardid.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_large_shardid.out.modified	2022-11-09 13:38:58.759312270 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_large_shardid.out.modified	2022-11-09 13:38:58.759312270 +0300
@@ -1,28 +1,21 @@
 --
 -- MULTI_LARGE_SHARDID
 --
 -- Load data into distributed tables, and run TPC-H query #1 and #6. This test
 -- differs from previous tests in that it modifies the *internal* shardId
 -- generator, forcing the distributed database to use 64-bit shard identifiers.
 ALTER SEQUENCE pg_catalog.pg_dist_shardid_seq RESTART 100200300400500;
 CREATE TABLE lineitem_large_shard_id AS SELECT * FROM lineitem;
 SELECT create_distributed_table('lineitem_large_shard_id', 'l_orderkey');
-NOTICE:  Copying data from local table...
-NOTICE:  copying the data has completed
-DETAIL:  The local data in the table is no longer visible, but is still on disk.
-HINT:  To remove the local data, run: SELECT truncate_local_data_after_distributing_table($$public.lineitem_large_shard_id$$)
- create_distributed_table
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  replication_factor (2) exceeds number of worker nodes (1)
+HINT:  Add more worker nodes or try again with a lower replication factor.
 -- Query #1 from the TPC-H decision support benchmark.
 SELECT
 	l_returnflag,
 	l_linestatus,
 	sum(l_quantity) as sum_qty,
 	sum(l_extendedprice) as sum_base_price,
 	sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
 	sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
 	avg(l_quantity) as avg_qty,
 	avg(l_extendedprice) as avg_price,
diff -dU10 -w /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_size_queries.out /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_size_queries.out
--- /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/expected/multi_size_queries.out.modified	2022-11-09 13:38:59.109312269 +0300
+++ /home/gokhan/Github/gokhangulbiz/citus/src/test/regress/results/multi_size_queries.out.modified	2022-11-09 13:38:59.119312269 +0300
@@ -17,45 +17,29 @@
 SELECT citus_table_size('non_distributed_table');
 ERROR:  cannot calculate the size because relation 'non_distributed_table' is not distributed
 SELECT citus_relation_size('non_distributed_table');
 ERROR:  cannot calculate the size because relation 'non_distributed_table' is not distributed
 SELECT citus_total_relation_size('non_distributed_table');
 ERROR:  cannot calculate the size because relation 'non_distributed_table' is not distributed
 DROP TABLE non_distributed_table;
 -- fix broken placements via disabling the node
 SET client_min_messages TO ERROR;
 SELECT replicate_table_shards('lineitem_hash_part', shard_replication_factor:=2, shard_transfer_mode:='block_writes');
- replicate_table_shards
----------------------------------------------------------------------
-
-(1 row)
-
+ERROR:  relation lineitem_hash_part is not distributed
 -- Tests on distributed table with replication factor > 1
 VACUUM (FULL) lineitem_hash_part;
 SELECT citus_table_size('lineitem_hash_part');
- citus_table_size
----------------------------------------------------------------------
-          3801088
-(1 row)
-
+ERROR:  cannot calculate the size because relation 'lineitem_hash_part' is not distributed
 SELECT citus_relation_size('lineitem_hash_part');
- citus_relation_size
----------------------------------------------------------------------
-             3801088
-(1 row)
-
+ERROR:  cannot calculate the size because relation 'lineitem_hash_part' is not distributed
 SELECT citus_total_relation_size('lineitem_hash_part');
- citus_total_relation_size
----------------------------------------------------------------------
-                   3801088
-(1 row)
-
+ERROR:  cannot calculate the size because relation 'lineitem_hash_part' is not distributed
 VACUUM (FULL) customer_copy_hash;
 -- Tests on distributed tables with streaming replication.
 SELECT citus_table_size('customer_copy_hash');
  citus_table_size 
 ------------------
            548864
 (1 row)
 
 SELECT citus_relation_size('customer_copy_hash');
  citus_relation_size 
@@ -68,21 +52,21 @@
 ---------------------------
                    1597440
 (1 row)
 
 -- Make sure we can get multiple sizes in a single query
 SELECT citus_table_size('customer_copy_hash'),
        citus_table_size('customer_copy_hash'),
        citus_table_size('supplier');
  citus_table_size | citus_table_size | citus_table_size 
 ------------------+------------------+------------------
-           548864 |           548864 |           442368
+           548864 |           548864 |           221184
 (1 row)
 
 CREATE INDEX index_1 on customer_copy_hash(c_custkey);
 VACUUM (FULL) customer_copy_hash;
 -- Tests on distributed table with index.
 SELECT citus_table_size('customer_copy_hash');
  citus_table_size 
 ------------------
            548864
 (1 row)
@@ -97,53 +81,53 @@
  citus_total_relation_size 
 ---------------------------
                    2646016
 (1 row)
 
 -- Tests on reference table
 VACUUM (FULL) supplier;
 SELECT citus_table_size('supplier');
  citus_table_size 
 ------------------
-           376832
+           188416
 (1 row)
 
 SELECT citus_relation_size('supplier');
  citus_relation_size 
 ---------------------
-              376832
+              188416
 (1 row)
 
 SELECT citus_total_relation_size('supplier');
  citus_total_relation_size 
 ---------------------------
-                    376832
+                    188416
 (1 row)
 
 CREATE INDEX index_2 on supplier(s_suppkey);
 VACUUM (FULL) supplier;
 SELECT citus_table_size('supplier');
  citus_table_size 
 ------------------
-           376832
+           188416
 (1 row)
 
 SELECT citus_relation_size('supplier');
  citus_relation_size 
 ---------------------
-              376832
+              188416
 (1 row)
 
 SELECT citus_total_relation_size('supplier');
  citus_total_relation_size 
 ---------------------------
-                    458752
+                    229376
 (1 row)
 
 -- Test inside the transaction
 BEGIN;
 ALTER TABLE supplier ALTER COLUMN s_suppkey SET NOT NULL;
 select citus_table_size('supplier');
 ERROR:  citus size functions cannot be called in transaction blocks which contain multi-shard data modifications
 END;
 show citus.node_conninfo;
  citus.node_conninfo 
